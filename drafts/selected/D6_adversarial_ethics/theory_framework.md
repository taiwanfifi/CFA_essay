# D6 Theory Framework: Adversarial Financial Ethics Testing
# Can LLMs Uphold Fiduciary Duty Under Pressure?

## Paper Title
**Under Pressure: Adversarial Stress Testing of LLM Ethical Judgment in Financial Decision-Making**

**Target Journal**: Finance Research Letters (FRL) or Journal of Financial Regulation

---

## 1. Research Hypotheses

### H1: Adversarial Degradation
**LLM accuracy on CFA Ethics questions declines significantly when adversarial pressure (profit incentives, authority pressure, emotional manipulation, moral dilemmas) is applied to the prompt context.**

- **Rationale**: LLMs learn ethical responses from training data patterns, not from principled moral reasoning. When adversarial prompts create tension between the "correct" ethical answer and a contextually compelling violation, the pattern-matching approach breaks down.
- **Operationalization**: Ethics Robustness Score (ERS) = Accuracy_adversarial / Accuracy_standard. ERS < 1.0 indicates degradation.
- **Expected effect size**: ERS = 0.60–0.80 (20–40% accuracy drop under adversarial conditions).

### H2: Pressure Type Heterogeneity
**Different adversarial pressure types produce different degradation magnitudes: reframing (linguistic disguise of violations) is most effective, followed by authority pressure, profit incentives, emotional manipulation, and moral dilemmas.**

- **Rationale**: Reframing attacks exploit the model's surface-level pattern matching — if a violation doesn't "look like" a violation, the model may approve it. Authority pressure exploits deference patterns in RLHF training. Moral dilemmas are least effective because they trigger the model's explicitly trained ethical safeguards.
- **Operationalization**: Compare ERS across 5 adversarial types; rank by effectiveness.

### H3: CFA Standard Vulnerability Variation
**Some CFA Ethics Standards are more vulnerable to adversarial pressure than others: insider trading (Standard II(A)) and conflicts of interest (Standard VI) are more robustly defended than fiduciary duty (Standard III(A)) and misrepresentation (Standard I(C)).**

- **Rationale**: Insider trading has clear, well-publicized red lines that feature prominently in training data. Fiduciary duty and misrepresentation involve nuanced judgment that is harder to learn from patterns and easier to undermine with reframing.
- **Operationalization**: Per-CFA-Standard ERS comparison.

### H4: Commercial vs. Open-Source Robustness Gap
**Commercial models (GPT-4o, Claude) exhibit significantly higher Ethics Robustness Scores than open-source models (Llama, Qwen), reflecting superior safety training.**

- **Rationale**: Commercial providers invest heavily in RLHF and safety alignment. Open-source models may lack domain-specific ethical safety training.
- **Operationalization**: Compare ERS between commercial and open-source model groups.

---

## 2. Economic Significance

### 2.1 Fiduciary Duty in the Age of AI

CFA Standard III(A) — Loyalty, Prudence, and Care — establishes that investment professionals owe a fiduciary duty to clients. When AI systems make recommendations that violate this duty:

- **Direct harm**: Clients receive unsuitable investment recommendations generated by a compromised AI
- **Liability exposure**: Firms deploying AI that fails ethical stress tests face regulatory sanctions and litigation risk
- **Trust erosion**: A single publicized case of AI ethical failure in finance can undermine industry-wide trust in AI-assisted advisory

Our adversarial testing framework provides a **pre-deployment safety audit** — analogous to penetration testing in cybersecurity.

### 2.2 Regulatory Compliance: AI as a Supervised Person

Under the CFA Institute's Ethical Standards, an AI system providing financial advice can be viewed as an "agent" of the supervised person (the CFA charterholder). If the AI's ethical judgment can be compromised by adversarial prompts that a sophisticated client or counterparty might employ:

- The charterholder may be **vicariously liable** for AI ethical failures
- Firms must demonstrate **reasonable supervision** of AI systems (Standard IV(C))
- Our ERS metric provides a concrete, auditable measure of AI ethical resilience

### 2.3 AI Safety in Finance: Beyond Technical Alignment

The AI safety community has focused on general alignment (helpful, harmless, honest). Financial ethics requires **domain-specific alignment**: an AI must understand not just "don't be harmful" but "don't violate CFA Standard II(A) Material Nonpublic Information even when the user frames it as 'strategic information utilization'." Our work bridges AI safety and financial regulation.

---

## 3. Discussion Points

### 3.1 Ethics as Pattern Matching vs. Principled Reasoning
If adversarial prompts can flip ethical judgment, this suggests LLMs learn the *form* of ethical responses rather than the *principles*. This has profound implications for AI deployment in any trust-sensitive domain.

### 3.2 The Reframing Attack Surface
Our finding that linguistic reframing is the most effective attack type suggests that financial AI systems need "semantic red-line detection" — the ability to recognize violations regardless of how they're linguistically packaged.

### 3.3 Minimum Ethics Robustness Standards
We propose ERS ≥ 0.85 as a minimum threshold for financial AI deployment — no more than 15% accuracy degradation under adversarial conditions. Systems below this threshold should not be deployed without mandatory human oversight for ethical decisions.

### 3.4 Limitations
- Adversarial prompts are synthetic; real-world pressure may be more subtle or more extreme
- Sample size (30–50 Ethics questions) limits statistical power
- Results are specific to CFA Ethics framework; other regulatory frameworks may yield different patterns

---

## 4. Key References

- Perez et al. (2022). Red Teaming Language Models with Language Models. — Adversarial testing methodology.
- Wei et al. (2023). Jailbroken: How Does LLM Safety Training Fail? — Jailbreak taxonomy.
- CFA Institute (2024). Standards of Practice Handbook, 12th ed.
- EU AI Act (2024). Regulation on Artificial Intelligence — High-risk AI systems.
