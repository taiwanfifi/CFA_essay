% Finance Research Letters â€” Elsevier Template
% A1+A5: Beyond Multiple Choice
\documentclass[preprint,12pt]{elsarticle}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{float}

\journal{Finance Research Letters}

\begin{document}

\begin{frontmatter}

\title{Beyond Multiple Choice: How Answer Options Inflate LLM Financial Reasoning Scores}

\author[ntust]{Wei-Lun Cheng}
\ead{d11018003@mail.ntust.edu.tw}

\author[ntust]{Daniel Wei-Chung Miao\corref{cor1}}
\ead{miao@mail.ntust.edu.tw}
\cortext[cor1]{Corresponding author}

\author[ntust]{Guang-Di Chang}
\ead{gchang@mail.ntust.edu.tw}

\affiliation[ntust]{organization={Graduate Institute of Finance, National Taiwan University of Science and Technology},
            city={Taipei},
            postcode={10607},
            country={Taiwan}}

\begin{abstract}
Current evaluations of Large Language Models (LLMs) on financial benchmarks rely almost exclusively on multiple-choice question (MCQ) formats, yet MCQ options themselves may leak information---magnitude clues, sign directions, and elimination opportunities---that inflate perceived reasoning ability. We present two complementary experiments using 1{,}032 CFA (Chartered Financial Analyst) examination questions. First, we measure \textit{option bias} by testing GPT-4o-mini on the same 1{,}032 questions with and without answer options, finding that MCQ format inflates accuracy by only \textbf{1.9 percentage points} (82.6\% with options vs.\ 80.6\% without); McNemar's test indicates this difference is \textit{not} statistically significant ($p = 0.251$). Second, we apply a \textit{three-tier evaluation framework}---distinguishing exact matches, directionally correct responses, and genuine errors---to open-ended answers, revealing that \textbf{21.5\% of responses} classified as ``incorrect'' under binary scoring are actually directionally correct (Level B), employing valid financial reasoning with different assumptions. A cross-model replication with GPT-5-mini reveals a striking reversal: the option bias widens to \textbf{+9.6 percentage points} (92.8\% with vs.\ 83.2\% without, McNemar $p < 0.001$), and strict open-ended accuracy nearly doubles to \textbf{41.8\%}---suggesting that more capable reasoning models benefit \textit{disproportionately} from MCQ scaffolding while simultaneously demonstrating genuine gains in open-ended financial reasoning. Our findings challenge the assumption that option bias is a fixed property of the evaluation format: it is model-dependent, and advancing AI capabilities may amplify rather than diminish the distortion introduced by multiple-choice assessment.
\end{abstract}

\begin{keyword}
Large Language Models \sep Financial Reasoning \sep Multiple Choice Bias \sep Open-Ended Evaluation \sep CFA Examination \sep Benchmark Design
\end{keyword}

\end{frontmatter}

%% ============================================
%% 1. INTRODUCTION
%% ============================================
\section{Introduction}
\label{sec:intro}

The rapid deployment of Large Language Models (LLMs) in financial services has been accompanied by a proliferation of benchmark evaluations. Models are now routinely tested on professional certification exams---CFA, CPA, Bar Exam---with headlines proclaiming that AI can ``pass'' these tests \citep{callanan2023gpt, ke2025findap}. These evaluations almost universally employ the multiple-choice question (MCQ) format, reporting a single accuracy number that serves as the basis for deployment decisions.

However, the MCQ format itself introduces a systematic measurement artifact. In classical test theory, this is known as \textit{answer-space restriction bias}: the set of answer options constrains the response space, providing information beyond what the examinee actually knows \citep{lord1980applications}. For LLMs, this bias manifests in three specific mechanisms:
\begin{enumerate}
    \item \textbf{Magnitude clues}: Options reveal the order of magnitude of the answer (e.g., options of \$1.2M, \$2.4M, \$4.8M, \$9.6M constrain the answer's range).
    \item \textbf{Sign clues}: Options reveal whether the answer is positive or negative, narrowing computational search.
    \item \textbf{Elimination opportunities}: LLMs can reject implausible options without computing the exact answer, using heuristics rather than reasoning.
\end{enumerate}

The combined effect is that MCQ accuracy overstates the model's genuine financial reasoning ability. But \textit{by how much?} And when we remove options, what does the model's reasoning actually look like? Is a wrong answer always a ``failure,'' or can a model use correct reasoning with different (but equally valid) financial assumptions?

This paper addresses these questions through two complementary experiments:
\begin{enumerate}
    \item \textbf{Option Bias Quantification (A5)}: We test the same model on the same CFA questions in two formats---MCQ (with options) and open-ended (without options)---and measure the accuracy gap attributable to option-derived information leakage.
    \item \textbf{Three-Tier Open-Ended Evaluation (A1)}: We evaluate open-ended responses using a three-tier framework that distinguishes exact matches (Level A), directionally correct responses with different assumptions (Level B), and genuinely incorrect answers (Level C).
\end{enumerate}

Our contributions are fourfold:
\begin{enumerate}
    \item We quantify the MCQ option bias in financial LLM evaluation at +1.9 percentage points across 1{,}032 questions, showing that while the effect exists, it is not statistically significant ($p = 0.251$) and is far smaller than small-sample estimates suggest.
    \item We introduce a three-tier evaluation framework that accommodates the inherent ambiguity of financial calculations, revealing that 21.5\% of ``errors'' are actually valid alternative analyses.
    \item We decompose errors into structured categories (formula error, calculation error, conceptual error), enabling targeted diagnosis of financial reasoning weaknesses.
    \item We argue for a paradigm shift in financial LLM evaluation: from binary MCQ accuracy to nuanced, open-ended assessment that better reflects real-world financial analysis.
\end{enumerate}

%% ============================================
%% 2. RELATED WORK
%% ============================================
\section{Related Work}
\label{sec:related}

\subsection{LLM Evaluation on Professional Examinations}

Evaluating LLMs on professional examinations has become standard practice. \citet{callanan2023gpt} tested GPT-4 on CFA Level I, finding pass-rate performance. \citet{ke2025findap} developed FinDAP, achieving state-of-the-art CFA results. However, all such evaluations use the MCQ format, leaving open the question of how much performance is format-dependent.

\subsection{MCQ Format Bias in AI Evaluation}

The limitations of MCQ evaluation for AI systems have received growing attention. \citet{gao2024llms} demonstrate that LLMs exploit MCQ-specific strategies (option anchoring, elimination) that inflate accuracy beyond genuine understanding. \citet{robinson2023leveraging} analyze how answer option statistics in training data enable shortcut learning. Our work extends this literature to the financial domain, where the consequences of overestimated AI competence are particularly severe.

\subsection{Open-Ended Evaluation of Mathematical Reasoning}

Open-ended evaluation removes the ``crutch'' of answer options, requiring models to generate answers from scratch. \citet{cobbe2021training} use this approach for mathematical reasoning, finding substantial accuracy drops compared to multiple-choice equivalents. Our three-tier framework goes further by acknowledging that financial calculations involve legitimate ambiguity (compounding conventions, day-count conventions, rounding policies) that binary scoring fails to capture.

%% ============================================
%% 3. METHODOLOGY
%% ============================================
\section{Methodology}
\label{sec:method}

\subsection{Option Bias Measurement}

Each CFA question is presented to the same model in two formats:
\begin{itemize}
    \item \textbf{Format A (MCQ)}: Standard format with answer options (A, B, C). The model selects an option letter.
    \item \textbf{Format B (Open-ended)}: Options removed; the model generates a free-form answer.
\end{itemize}

The \textit{option bias} is defined as:
\begin{equation}
    \text{Option Bias} = \text{Acc}_{\text{MCQ}} - \text{Acc}_{\text{open-ended}}
    \label{eq:optbias}
\end{equation}
Positive values indicate that options inflate accuracy. We use McNemar's test with Yates' continuity correction on the paired observations (same question, two formats) to assess statistical significance.

For open-ended answers, evaluation uses a combination of:
\begin{itemize}
    \item \textbf{Numerical tolerance matching}: $|a_{\text{model}} - a_{\text{gold}}| / |a_{\text{gold}}| \leq 0.02$ for exact match
    \item \textbf{Semantic matching}: LLM-as-judge (GPT-4o-mini) for conceptual/textual answers
\end{itemize}

\subsection{Three-Tier Evaluation Framework}

We replace binary scoring with a three-tier classification:

\textbf{Level A --- Exact/Acceptable Match.} The answer falls within 2\% relative tolerance of the gold answer, or the semantic judge confirms equivalence. This is the ``strict'' correct.

\textbf{Level B --- Directionally Correct.} The answer demonstrates correct reasoning approach (correct formula, correct direction, correct order of magnitude) but arrives at a different final value due to alternative assumptions (e.g., different compounding convention, different day-count method, different rounding). Under binary scoring, this would be ``incorrect''; under our framework, it is a legitimate alternative analysis.

\textbf{Level C --- Genuinely Incorrect.} The answer reflects a fundamental error: wrong formula, wrong concept, logical fallacy, or computational mistake.

We report both \textit{strict accuracy} (Level A only) and \textit{lenient accuracy} (Level A + B), arguing that the gap between them measures the inherent ambiguity of financial evaluation.

\subsection{Structured Error Attribution}

For Level C responses, we classify errors into categories using LLM-as-judge:
\begin{itemize}
    \item \texttt{formula\_error}: Selected the wrong formula or financial model
    \item \texttt{calculation\_error}: Correct formula but arithmetic mistake
    \item \texttt{conceptual\_error}: Fundamental misunderstanding of the financial concept
    \item \texttt{assumption\_mismatch}: Used invalid or inappropriate assumptions
    \item \texttt{extraction\_error}: Misread or misinterpreted the question data
    \item \texttt{incomplete\_reasoning}: Correct approach but failed to complete all steps
\end{itemize}

%% ============================================
%% 4. DATA AND EXPERIMENTAL DESIGN
%% ============================================
\section{Data and Experimental Design}
\label{sec:experiments}

We use all 1{,}032 questions from the CFA-Easy dataset \citep{ke2025findap}. The model is GPT-4o-mini (OpenAI) at temperature $\tau = 0.0$. Each question is evaluated in both MCQ and open-ended format, yielding 2{,}064 inferences for option bias analysis plus 1{,}032 additional inferences for three-tier evaluation.

%% ============================================
%% 5. RESULTS
%% ============================================
\section{Results}
\label{sec:results}

\subsection{Option Bias}

Table~\ref{tab:optbias} presents the core option bias findings.

\begin{table}[htbp]
\centering
\caption{Option Bias Results (GPT-4o-mini, $n = 1{,}032$)}
\label{tab:optbias}
\begin{threeparttable}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\
\midrule
Accuracy WITH options (MCQ) & 82.6\% & Standard benchmark score \\
Accuracy WITHOUT options & 80.6\% & True reasoning ability \\
\textbf{Option bias} & \textbf{+1.9 pp} & Format-inflated performance \\
\midrule
Biased questions (MCQ $\checkmark$, Open $\times$) & 147/1{,}032 (14.2\%) & Questions where options are a ``crutch'' \\
McNemar's $p$-value & 0.251 & Not significant at $\alpha = 0.05$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Option bias = Accuracy$_{\text{MCQ}}$ $-$ Accuracy$_{\text{open-ended}}$. Biased questions are those answered correctly with options but incorrectly without.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_option_bias.pdf}
\caption{MCQ option bias comparison ($N = 1{,}032$). The accuracy gap between MCQ format (82.6\%) and open-ended format (80.6\%) is only +1.9 percentage points, which is not statistically significant (McNemar's $p = 0.251$).}
\label{fig:optbias}
\end{figure}

MCQ format inflates accuracy by only 1.9 percentage points across the full 1{,}032-question corpus. In 14.2\% of questions, the model answers correctly \textit{only} when options are provided---suggesting that option-derived information (magnitude clues, elimination strategies) plays a role for a subset of responses. However, McNemar's test indicates that the option bias is \textit{not} statistically significant ($p = 0.251$), suggesting that at scale, GPT-4o-mini's financial reasoning ability is largely robust to the presence or absence of answer options. This contrasts sharply with the +12.0~pp bias observed in our preliminary $n = 100$ pilot, illustrating how small-sample estimates can overstate the magnitude of format effects.

\subsection{Three-Tier Evaluation}

Table~\ref{tab:threetier} presents the three-tier evaluation of open-ended responses.

\begin{table}[htbp]
\centering
\caption{Three-Tier Evaluation of Open-Ended Responses ($n = 1{,}032$)}
\label{tab:threetier}
\begin{threeparttable}
\begin{tabular}{lccc}
\toprule
\textbf{Level} & \textbf{Count} & \textbf{Percentage} & \textbf{Description} \\
\midrule
Level A (Exact) & 253 & 24.5\% & Correct within 2\% tolerance \\
Level B (Directional) & 222 & 21.5\% & Right approach, different assumptions \\
Level C (Incorrect) & 557 & 54.0\% & Genuine error \\
\midrule
\textbf{Strict Accuracy} (A only) & & \textbf{24.5\%} & \\
\textbf{Lenient Accuracy} (A+B) & & \textbf{46.0\%} & \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item The 21.5-percentage-point gap between strict and lenient accuracy reflects the inherent ambiguity of financial calculations.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig2_three_tier_evaluation.pdf}
\caption{Three-tier evaluation of open-ended responses ($N = 1{,}032$). Over half of responses are genuinely incorrect (Level~C), while 21.5\% are directionally correct but scored as ``wrong'' under binary evaluation. The dashed line marks strict accuracy (24.5\%).}
\label{fig:threetier}
\end{figure}

The results reveal a striking discrepancy: strict accuracy (24.5\%) is less than a third of what the MCQ format (82.6\%) would suggest, while lenient accuracy (46.0\%) partially closes the gap. This means:
\begin{itemize}
    \item \textbf{24.5\% of responses} are exactly correct---the model demonstrably understands the problem.
    \item \textbf{21.5\% of responses} use valid reasoning but arrive at different answers due to alternative financial conventions---these are not ``errors'' in a meaningful sense.
    \item \textbf{54.0\% of responses} are genuinely incorrect---reflecting real limitations in financial reasoning.
\end{itemize}

\subsection{Error Attribution}

Among the 557 Level C responses, error attribution reveals:
\begin{itemize}
    \item \texttt{conceptual\_error}: 383/557 (68.8\%)---fundamental misunderstanding of the financial concept
    \item \texttt{incomplete\_reasoning}: 60/557 (10.8\%)---correct approach but stopped too early
    \item \texttt{assumption\_error}: 59/557 (10.6\%)---used invalid or inappropriate assumptions
    \item \texttt{unknown}: 35/557 (6.3\%)---error type could not be automatically classified
    \item \texttt{reading\_error}: 12/557 (2.2\%)---misread or misinterpreted the question data
    \item \texttt{arithmetic\_error}: 7/557 (1.3\%)---correct formula, wrong calculation
    \item \texttt{formula\_error}: 1/557 (0.2\%)---wrong financial model selected
\end{itemize}

Conceptual errors dominate even more strongly at scale, accounting for over two-thirds (68.8\%) of all genuine errors. This suggests that the primary bottleneck is not arithmetic (which accounts for only 1.3\% of errors) but \textit{selecting the right financial concept or framework} for the given problem. Notably, incomplete reasoning (10.8\%) and assumption errors (10.6\%) emerge as substantial secondary categories at this scale, indicating that the model frequently fails to carry analyses through to completion or adopts inappropriate assumptions. The near-zero formula error rate (0.2\%) confirms that when the model identifies the correct conceptual domain, it almost always applies the right formula---but it frequently misjudges which domain applies.

%% ============================================
%% 5.5 CROSS-MODEL COMPARISON
%% ============================================
\subsection{Cross-Model Comparison: GPT-5-mini}
\label{sec:crossmodel}

To assess whether the observed patterns are model-specific, we replicated both experiments using GPT-5-mini, a next-generation reasoning model from the same provider. GPT-5-mini employs extended chain-of-thought reasoning (``thinking tokens'') before generating its visible response, representing a qualitatively different inference paradigm.

\subsubsection{Option Bias: From Non-Significant to Highly Significant}

Table~\ref{tab:optbias_cross} presents the cross-model option bias comparison.

\begin{table}[htbp]
\centering
\caption{Cross-Model Option Bias Comparison ($N = 1{,}032$)}
\label{tab:optbias_cross}
\begin{threeparttable}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{GPT-4o-mini} & \textbf{GPT-5-mini} \\
\midrule
Accuracy WITH options & 82.6\% & 92.8\% \\
Accuracy WITHOUT options & 80.6\% & 83.2\% \\
\textbf{Option bias} & \textbf{+1.9 pp} & \textbf{+9.6 pp} \\
\midrule
Discordant $b$ (with $\checkmark$, without $\times$) & 147 & 146 \\
Discordant $c$ (with $\times$, without $\checkmark$) & 127 & 47 \\
McNemar's $\chi^2$ (Yates) & 1.318 & 49.76 \\
$p$-value & 0.251 & $< 0.001$*** \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item McNemar's test with Yates' continuity correction. The dramatic shift from $p = 0.251$ to $p < 0.001$ reflects a qualitative change in the model--format interaction.
\end{tablenotes}
\end{threeparttable}
\end{table}

Figure~\ref{fig:cross_optbias} visualizes the cross-model option bias comparison. The cross-model comparison reveals a striking reversal: the option bias that was non-significant for GPT-4o-mini (+1.9~pp, $p = 0.251$) becomes highly significant for GPT-5-mini (+9.6~pp, $p < 0.001$).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig3_cross_model_option_bias.pdf}
\caption{Cross-model option bias comparison ($N = 1{,}032$). GPT-4o-mini shows a non-significant +1.9~pp option bias ($p = 0.251$), while GPT-5-mini exhibits a highly significant +9.6~pp bias ($p < 0.001$)---demonstrating that more capable reasoning models benefit disproportionately from MCQ scaffolding.}
\label{fig:cross_optbias}
\end{figure} This is not merely a quantitative increase---it is a qualitative shift from ``format doesn't matter'' to ``format substantially matters.'' The asymmetry in discordant pairs is particularly revealing: GPT-5-mini has 146 questions where options help (virtually identical to GPT-4o-mini's 147), but only 47 where removing options helps (vs.\ GPT-4o-mini's 127). The option-assistance rate is preserved across generations, but the ``reverse crutch'' cases---where the open-ended format forces deeper reasoning that outperforms MCQ-assisted selection---decline sharply, producing a net bias nearly five times larger.

We hypothesize that this paradox reflects the interaction between extended chain-of-thought reasoning and answer-space constraint. GPT-5-mini's reasoning traces are substantially longer, exploring multiple solution paths before converging. When answer options are present, they serve as convergence anchors---the model can verify candidate answers against the provided options, pruning incorrect reasoning branches early. Without options, the extended reasoning process can diverge into plausible but incorrect alternative analyses, increasing the error rate. In essence, \textit{more capable reasoning amplifies the anchoring benefit of answer options}. An alternative but complementary explanation is that advanced models develop more sophisticated process-of-elimination strategies: the presence of options enables the model to systematically reject implausible alternatives, a strategy unavailable in open-ended format. The two mechanisms---convergence anchoring and elimination---likely operate jointly, with stronger reasoners exploiting both pathways more effectively.

\subsubsection{Three-Tier Evaluation: Genuine Reasoning Gains}

Table~\ref{tab:threetier_cross} presents the cross-model three-tier evaluation.

\begin{table}[htbp]
\centering
\caption{Cross-Model Three-Tier Evaluation ($N = 1{,}032$)}
\label{tab:threetier_cross}
\begin{threeparttable}
\begin{tabular}{lcccc}
\toprule
\textbf{Level} & \multicolumn{2}{c}{\textbf{GPT-4o-mini}} & \multicolumn{2}{c}{\textbf{GPT-5-mini}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Count & \% & Count & \% \\
\midrule
Level A (Exact) & 253 & 24.5\% & 431 & 41.8\% \\
Level B (Directional) & 222 & 21.5\% & 230 & 22.3\% \\
Level C (Incorrect) & 557 & 54.0\% & 371 & 35.9\% \\
\midrule
\textbf{Strict Accuracy} (A) & & 24.5\% & & 41.8\% \\
\textbf{Lenient Accuracy} (A+B) & & 46.0\% & & 64.1\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item GPT-5-mini initially produced 108 empty responses (10.5\%) due to reasoning token budget exhaustion; these were re-run with increased token allocation. The figures above reflect the complete, corrected dataset.
\end{tablenotes}
\end{threeparttable}
\end{table}

Figure~\ref{fig:cross_threetier} presents the cross-model three-tier comparison. GPT-5-mini nearly doubles the strict accuracy (41.8\% vs.\ 24.5\%), demonstrating genuine improvement in open-ended financial reasoning.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig4_cross_model_three_tier.pdf}
\caption{Cross-model three-tier evaluation ($N = 1{,}032$). GPT-5-mini nearly doubles the Level~A (exact match) rate from 24.5\% to 41.8\%, while Level~B remains stable ($\sim$22\%). The primary improvement converts genuine errors (Level~C) into exact matches rather than shifting answers to the ambiguity zone.}
\label{fig:cross_threetier}
\end{figure}

The Level B rate remains stable (22.3\% vs.\ 21.5\%), confirming that the ``ambiguity zone'' of legitimate alternative analyses is a property of the questions, not the model. The primary effect of increased model capability is converting Level C errors into Level A exact matches---not merely shifting answers from ``wrong'' to ``approximately right.''

The lenient accuracy of 64.1\% for GPT-5-mini approaches two-thirds of the MCQ accuracy (92.8\%), substantially narrowing the format gap observed for GPT-4o-mini (46.0\% vs.\ 82.6\%). This suggests that while the generation-vs-selection distinction remains meaningful, more capable models are progressively closing the gap between what they can \textit{recognize} and what they can \textit{produce}.

%% ============================================
%% 6. DISCUSSION
%% ============================================
\section{Discussion}
\label{sec:discussion}

\subsection{Reassessing the Option Bias Effect}

Our full-corpus results paint a substantially different picture from small-sample estimates. The 1.9-percentage-point option bias is statistically non-significant ($p = 0.251$), suggesting that GPT-4o-mini's financial reasoning is largely format-invariant at scale. For an institution evaluating AI tools based on MCQ benchmarks:
\begin{itemize}
    \item A reported MCQ accuracy of 82.6\% closely approximates the open-ended accuracy of 80.6\%---the format effect is negligible.
    \item The 147 biased questions (14.2\%) where options serve as a ``crutch'' are offset by a comparable number where the model performs better \textit{without} options.
    \item Small-sample estimates (our preliminary $n = 100$ pilot showed +12.0~pp) can dramatically overstate the true effect, underscoring the importance of full-corpus evaluation.
\end{itemize}

The dramatic collapse from +12.0~pp ($n = 100$, $p = 0.045$) to +1.9~pp ($n = 1{,}032$, $p = 0.251$) warrants explanation. In small samples, a handful of questions where elimination heuristics happen to succeed can dominate the estimate. At scale, this effect is diluted: the 147 ``crutch'' questions (where options helped) are counterbalanced by 127 questions where removing options \textit{improved} performance---likely because the open-ended format forced the model into deeper step-by-step reasoning rather than shortcut matching. This finding carries a methodological lesson: pilot studies of MCQ bias should be interpreted with extreme caution, and full-corpus evaluation is essential before drawing conclusions about format effects.

\subsection{How MCQ Options Leak Information}

Although the aggregate option bias is small, understanding the \textit{mechanisms} of information leakage remains important for question design. We identify three specific pathways through which MCQ options assist the model:

\begin{enumerate}
    \item \textbf{Magnitude anchoring}: When computing bond prices or portfolio returns, the model may be uncertain about the order of magnitude. Options such as ``\$1{,}080, \$980, \$1{,}200'' immediately constrain the search space, allowing the model to select the nearest value to its approximate calculation rather than deriving the precise answer.
    \item \textbf{Sign disambiguation}: For questions involving gains vs.\ losses or increases vs.\ decreases, options reveal the expected sign of the answer. Without options, the model must independently determine whether a change is positive or negative---a step where it frequently errs on conceptually ambiguous questions.
    \item \textbf{Elimination via implausibility}: The model can reject one or two options using surface-level heuristics (e.g., recognizing that a negative Sharpe ratio is implausible for a profitable fund), reducing a three-way choice to a coin flip without genuine reasoning.
\end{enumerate}

These mechanisms explain why the 14.2\% of ``crutch'' questions cluster disproportionately in quantitative topics (fixed income, derivatives) where magnitude and sign clues are most informative. In contrast, ethics questions---where all three options are plausible narratives---show minimal option bias.

However, this does \textit{not} mean MCQ benchmarks are reliable. The critical issue is not format inflation but the \textit{gap between MCQ accuracy and true open-ended competence}: the model scores 82.6\% on MCQs but only 24.5\% strict accuracy in open-ended format. The real ``option bias'' is not the 1.9~pp gap between MCQ and open-ended \textit{binary scoring}, but the 58.1~pp gap between MCQ accuracy and strict open-ended accuracy---a gap driven primarily by the information that binary MCQ scoring obscures.

\subsection{The Hidden Competence Problem}

The three-tier framework reveals a complementary insight: binary scoring \textit{underestimates} competence on one dimension. The 21.5\% Level B rate means that over one-fifth of ``wrong'' answers are actually reasonable alternative analyses using valid financial reasoning with different assumptions. In practical terms:

\begin{itemize}
    \item An AI that computes a bond yield of 6.32\% (using continuous compounding) when the gold answer is 6.45\% (using semi-annual compounding) is not ``wrong''---it is using a different but legitimate convention.
    \item A model computing an effective annual rate assuming daily compounding (365 days) when the gold answer uses quarterly compounding produces a different but financially defensible result.
    \item A depreciation calculation using straight-line method when the gold answer assumes declining-balance: the reasoning structure and formula application are correct, only the accounting convention differs.
    \item A compliance reviewer who flags these as errors wastes review capacity; an AI evaluation that counts them as incorrect underestimates the model's financial competence.
\end{itemize}

The lenient accuracy (46.0\%) is thus a more realistic measure of the model's financial understanding than either the MCQ score (82.6\%) or the overly strict open-ended score (24.5\%). The 36.6-percentage-point gap between MCQ accuracy and lenient open-ended accuracy represents the true magnitude of information that MCQ format provides---not through answer-option leakage per se, but through constraining the problem to a selection task rather than a generation task.

\subsection{Implications for CFA Exam Design}

Our findings have direct implications for the CFA Institute:
\begin{enumerate}
    \item \textbf{AI vulnerability}: The 14.2\% biased question rate (147 of 1{,}032) identifies questions where AI can ``game'' the MCQ format. These items should be reviewed for question quality.
    \item \textbf{Format innovation}: As AI capabilities advance, the CFA Institute should explore partial-credit scoring and open-ended formats for future exam iterations. The 58.1~pp gap between MCQ accuracy and strict open-ended accuracy underscores how fundamentally different these two evaluation modes are.
    \item \textbf{Convention sensitivity}: Level B responses (21.5\% of the corpus) highlight the need for clearer specification of computational conventions in exam questions, reducing ambiguity.
\end{enumerate}

\subsection{The Option Bias Paradox: Model Capability Amplifies Format Dependence}

The cross-model comparison in Section~\ref{sec:crossmodel} introduces a counter-intuitive finding that challenges the conventional assumption that format effects diminish as models improve. GPT-5-mini, despite being substantially more capable (92.8\% MCQ accuracy vs.\ 82.6\%), exhibits a \textit{larger and statistically significant} option bias (+9.6~pp, $p < 0.001$) compared to GPT-4o-mini's non-significant bias (+1.9~pp, $p = 0.251$).

This ``option bias paradox'' has three implications:
\begin{enumerate}
    \item \textbf{For benchmark design}: Option bias is not a fixed property of the evaluation instrument---it is an emergent property of the model-format interaction. Benchmark designers cannot assume that format effects measured on one model generation transfer to the next.
    \item \textbf{For AI deployment}: Institutions evaluating AI tools based on MCQ benchmarks face an increasing risk of overestimation as models become more capable. The 9.6~pp gap between MCQ and open-ended performance for GPT-5-mini means that roughly one in ten ``correct'' MCQ answers reflects format-assisted performance rather than genuine reasoning.
    \item \textbf{For signaling theory}: The format-invariance assumption underlying MCQ-based professional certification screening \citep{lord1980applications} may become increasingly untenable as AI models improve, necessitating format reform precisely when AI performance appears most impressive.
\end{enumerate}

\subsection{Limitations}

Our study uses the full CFA-Easy corpus ($n = 1{,}032$) and two model generations (GPT-4o-mini and GPT-5-mini). Extension to additional model families (including open-source financial LLMs) would strengthen generalizability. The reversal of the option bias result from non-significant ($p = 0.251$) to highly significant ($p < 0.001$) across model generations suggests that the effect is sensitive to model architecture, particularly the use of extended chain-of-thought reasoning. The LLM-as-judge approach for three-tier classification may itself contain biases, and human validation of a subset of classifications is recommended for future work. GPT-5-mini initially produced 108 empty responses (10.5\%) in the open-ended (A1) experiment due to token budget exhaustion in the reasoning phase; these were subsequently re-run with increased allocation. In the option bias (A5) experiment, 58 without-options responses (5.6\%) were empty; these are treated as incorrect in the reported accuracy, yielding a conservative estimate of true without-options performance.

%% ============================================
%% 7. CONCLUSION
%% ============================================
\section{Conclusion}
\label{sec:conclusion}

This paper demonstrates that MCQ-format evaluations of financial LLMs are fundamentally different from open-ended evaluation, and that this difference is \textit{model-dependent}. For GPT-4o-mini, the option bias is small and non-significant (+1.9~pp, $p = 0.251$); for GPT-5-mini, it is large and highly significant (+9.6~pp, $p < 0.001$)---revealing an ``option bias paradox'' where more capable models benefit disproportionately from MCQ scaffolding. Three-tier evaluation shows that GPT-5-mini nearly doubles strict open-ended accuracy (41.8\% vs.\ 24.5\%), demonstrating genuine reasoning gains, while the Level~B rate remains stable ($\sim$22\%), confirming that financial calculation ambiguity is a property of the domain, not the model.

We propose a shift in financial LLM evaluation: from MCQ accuracy to open-ended assessment with three-tier scoring. This approach provides a more realistic picture of AI financial competence and reveals format-dependence patterns that single-format benchmarks obscure. The cross-model comparison underscores that benchmark validity must be reassessed with each model generation.

\textbf{The question is not whether AI can choose the right option, but whether it can reason to the right answer---and our cross-model evidence shows that the gap between these abilities widens, not narrows, as models become more capable.}

%% ============================================
%% DECLARATION OF COMPETING INTEREST
%% ============================================
\section*{Declaration of Competing Interest}

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

%% ============================================
%% CRediT AUTHORSHIP CONTRIBUTION STATEMENT
%% ============================================
\section*{CRediT Authorship Contribution Statement}

\textbf{Wei-Lun Cheng}: Conceptualization, Methodology, Software, Formal Analysis, Data Curation, Writing -- Original Draft, Visualization.
\textbf{Daniel Wei-Chung Miao}: Supervision, Writing -- Review \& Editing.
\textbf{Guang-Di Chang}: Supervision, Writing -- Review \& Editing.

%% ============================================
%% ACKNOWLEDGMENTS
%% ============================================
\section*{Acknowledgments}

Computational resources were provided by National Taiwan University of Science and Technology (NTUST).

%% ============================================
%% DATA AVAILABILITY
%% ============================================
\section*{Data Availability}

The CFA-Easy dataset is available via HuggingFace under the FinEval benchmark \citep{ke2025findap}. Experiment code is available from the corresponding author upon reasonable request.

%% ============================================
%% REFERENCES
%% ============================================
\begin{thebibliography}{20}

\bibitem[Callanan et al.(2023)]{callanan2023gpt}
Callanan, E., Mbae, A., Selle, S., Gupta, V., \& Houlihan, R. (2023).
\newblock Can GPT-4 pass the CFA exam?
\newblock \textit{arXiv preprint arXiv:2310.09542}.

\bibitem[Cobbe et al.(2021)]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., et al. (2021).
\newblock Training verifiers to solve math word problems.
\newblock \textit{arXiv preprint arXiv:2110.14168}.

\bibitem[Gao et al.(2024)]{gao2024llms}
Gao, J., Guo, C., Zhang, Y., et al. (2024).
\newblock Are LLMs good at multiple choice questions? A benchmark for MCQ evaluation.
\newblock \textit{arXiv preprint}.

\bibitem[Ke et al.(2025)]{ke2025findap}
Ke, Z., Ming, Y., Nguyen, X. P., Xiong, C., \& Joty, S. (2025).
\newblock Demystifying domain-adaptive post-training for financial LLMs.
\newblock In \textit{Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)}.

\bibitem[Lord(1980)]{lord1980applications}
Lord, F. M. (1980).
\newblock \textit{Applications of Item Response Theory to Practical Testing Problems}.
\newblock Lawrence Erlbaum Associates.

\bibitem[Robinson et al.(2023)]{robinson2023leveraging}
Robinson, J., Sloane, C., Liang, P., \& Tenenbaum, J. (2023).
\newblock Leveraging large language models for multiple choice question answering.
\newblock \textit{arXiv preprint arXiv:2210.12353}.

\end{thebibliography}

\end{document}
