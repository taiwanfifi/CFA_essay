% Finance Research Letters â€” Elsevier Template
% A1+A5: Beyond Multiple Choice
\documentclass[preprint,12pt]{elsarticle}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{float}

\journal{Finance Research Letters}

\begin{document}

\begin{frontmatter}

\title{Beyond Multiple Choice: How Answer Options Inflate LLM Financial Reasoning Scores}

\author[ntust]{Wei-Lun Cheng}
\ead{d11018003@mail.ntust.edu.tw}

\author[ntust]{Daniel Wei-Chung Miao\corref{cor1}}
\ead{miao@mail.ntust.edu.tw}
\cortext[cor1]{Corresponding author}

\author[ntust]{Guang-Di Chang}
\ead{gchang@mail.ntust.edu.tw}

\affiliation[ntust]{organization={Graduate Institute of Finance, National Taiwan University of Science and Technology},
            city={Taipei},
            postcode={10607},
            country={Taiwan}}

\begin{abstract}
Current evaluations of Large Language Models (LLMs) on financial benchmarks rely almost exclusively on multiple-choice question (MCQ) formats, yet MCQ options themselves may leak information---magnitude clues, sign directions, and elimination opportunities---that inflate perceived reasoning ability. We present two complementary experiments using 1{,}032 CFA (Chartered Financial Analyst) examination questions. First, we measure \textit{option bias} by testing GPT-4o-mini on the same 1{,}032 questions with and without answer options, finding that MCQ format inflates accuracy by only \textbf{1.9 percentage points} (82.6\% with options vs.\ 80.6\% without); McNemar's test indicates this difference is \textit{not} statistically significant ($p = 0.251$). Second, we apply a \textit{three-tier evaluation framework}---distinguishing exact matches, directionally correct responses, and genuine errors---to open-ended answers, revealing that \textbf{21.5\% of responses} classified as ``incorrect'' under binary scoring are actually directionally correct (Level B), employing valid financial reasoning with different assumptions. The strict (exact match) accuracy of 24.5\% contrasts sharply with the lenient (including directionally correct) accuracy of 46.0\%, exposing a 21.5-percentage-point gap attributable to the inherent ambiguity of financial calculations. Our findings suggest that while the MCQ option bias effect is smaller than previously estimated on small samples, the binary correct/incorrect paradigm fundamentally fails to capture the nuanced nature of financial reasoning, with over one-fifth of ``errors'' reflecting legitimate alternative analyses.
\end{abstract}

\begin{keyword}
Large Language Models \sep Financial Reasoning \sep Multiple Choice Bias \sep Open-Ended Evaluation \sep CFA Examination \sep Benchmark Design
\end{keyword}

\end{frontmatter}

%% ============================================
%% 1. INTRODUCTION
%% ============================================
\section{Introduction}
\label{sec:intro}

The rapid deployment of Large Language Models (LLMs) in financial services has been accompanied by a proliferation of benchmark evaluations. Models are now routinely tested on professional certification exams---CFA, CPA, Bar Exam---with headlines proclaiming that AI can ``pass'' these tests \citep{callanan2023gpt, ke2025findap}. These evaluations almost universally employ the multiple-choice question (MCQ) format, reporting a single accuracy number that serves as the basis for deployment decisions.

However, the MCQ format itself introduces a systematic measurement artifact. In classical test theory, this is known as \textit{answer-space restriction bias}: the set of answer options constrains the response space, providing information beyond what the examinee actually knows \citep{lord1980applications}. For LLMs, this bias manifests in three specific mechanisms:
\begin{enumerate}
    \item \textbf{Magnitude clues}: Options reveal the order of magnitude of the answer (e.g., options of \$1.2M, \$2.4M, \$4.8M, \$9.6M constrain the answer's range).
    \item \textbf{Sign clues}: Options reveal whether the answer is positive or negative, narrowing computational search.
    \item \textbf{Elimination opportunities}: LLMs can reject implausible options without computing the exact answer, using heuristics rather than reasoning.
\end{enumerate}

The combined effect is that MCQ accuracy overstates the model's genuine financial reasoning ability. But \textit{by how much?} And when we remove options, what does the model's reasoning actually look like? Is a wrong answer always a ``failure,'' or can a model use correct reasoning with different (but equally valid) financial assumptions?

This paper addresses these questions through two complementary experiments:
\begin{enumerate}
    \item \textbf{Option Bias Quantification (A5)}: We test the same model on the same CFA questions in two formats---MCQ (with options) and open-ended (without options)---and measure the accuracy gap attributable to option-derived information leakage.
    \item \textbf{Three-Tier Open-Ended Evaluation (A1)}: We evaluate open-ended responses using a three-tier framework that distinguishes exact matches (Level A), directionally correct responses with different assumptions (Level B), and genuinely incorrect answers (Level C).
\end{enumerate}

Our contributions are fourfold:
\begin{enumerate}
    \item We quantify the MCQ option bias in financial LLM evaluation at +1.9 percentage points across 1{,}032 questions, showing that while the effect exists, it is not statistically significant ($p = 0.251$) and is far smaller than small-sample estimates suggest.
    \item We introduce a three-tier evaluation framework that accommodates the inherent ambiguity of financial calculations, revealing that 21.5\% of ``errors'' are actually valid alternative analyses.
    \item We decompose errors into structured categories (formula error, calculation error, conceptual error), enabling targeted diagnosis of financial reasoning weaknesses.
    \item We argue for a paradigm shift in financial LLM evaluation: from binary MCQ accuracy to nuanced, open-ended assessment that better reflects real-world financial analysis.
\end{enumerate}

%% ============================================
%% 2. RELATED WORK
%% ============================================
\section{Related Work}
\label{sec:related}

\subsection{LLM Evaluation on Professional Examinations}

Evaluating LLMs on professional examinations has become standard practice. \citet{callanan2023gpt} tested GPT-4 on CFA Level I, finding pass-rate performance. \citet{ke2025findap} developed FinDAP, achieving state-of-the-art CFA results. However, all such evaluations use the MCQ format, leaving open the question of how much performance is format-dependent.

\subsection{MCQ Format Bias in AI Evaluation}

The limitations of MCQ evaluation for AI systems have received growing attention. \citet{gao2024llms} demonstrate that LLMs exploit MCQ-specific strategies (option anchoring, elimination) that inflate accuracy beyond genuine understanding. \citet{robinson2023leveraging} analyze how answer option statistics in training data enable shortcut learning. Our work extends this literature to the financial domain, where the consequences of overestimated AI competence are particularly severe.

\subsection{Open-Ended Evaluation of Mathematical Reasoning}

Open-ended evaluation removes the ``crutch'' of answer options, requiring models to generate answers from scratch. \citet{cobbe2021training} use this approach for mathematical reasoning, finding substantial accuracy drops compared to multiple-choice equivalents. Our three-tier framework goes further by acknowledging that financial calculations involve legitimate ambiguity (compounding conventions, day-count conventions, rounding policies) that binary scoring fails to capture.

%% ============================================
%% 3. METHODOLOGY
%% ============================================
\section{Methodology}
\label{sec:method}

\subsection{Option Bias Measurement}

Each CFA question is presented to the same model in two formats:
\begin{itemize}
    \item \textbf{Format A (MCQ)}: Standard format with answer options (A, B, C). The model selects an option letter.
    \item \textbf{Format B (Open-ended)}: Options removed; the model generates a free-form answer.
\end{itemize}

The \textit{option bias} is defined as:
\begin{equation}
    \text{Option Bias} = \text{Acc}_{\text{MCQ}} - \text{Acc}_{\text{open-ended}}
    \label{eq:optbias}
\end{equation}
Positive values indicate that options inflate accuracy. We use McNemar's test with Yates' continuity correction on the paired observations (same question, two formats) to assess statistical significance.

For open-ended answers, evaluation uses a combination of:
\begin{itemize}
    \item \textbf{Numerical tolerance matching}: $|a_{\text{model}} - a_{\text{gold}}| / |a_{\text{gold}}| \leq 0.02$ for exact match
    \item \textbf{Semantic matching}: LLM-as-judge (GPT-4o-mini) for conceptual/textual answers
\end{itemize}

\subsection{Three-Tier Evaluation Framework}

We replace binary scoring with a three-tier classification:

\textbf{Level A --- Exact/Acceptable Match.} The answer falls within 2\% relative tolerance of the gold answer, or the semantic judge confirms equivalence. This is the ``strict'' correct.

\textbf{Level B --- Directionally Correct.} The answer demonstrates correct reasoning approach (correct formula, correct direction, correct order of magnitude) but arrives at a different final value due to alternative assumptions (e.g., different compounding convention, different day-count method, different rounding). Under binary scoring, this would be ``incorrect''; under our framework, it is a legitimate alternative analysis.

\textbf{Level C --- Genuinely Incorrect.} The answer reflects a fundamental error: wrong formula, wrong concept, logical fallacy, or computational mistake.

We report both \textit{strict accuracy} (Level A only) and \textit{lenient accuracy} (Level A + B), arguing that the gap between them measures the inherent ambiguity of financial evaluation.

\subsection{Structured Error Attribution}

For Level C responses, we classify errors into categories using LLM-as-judge:
\begin{itemize}
    \item \texttt{formula\_error}: Selected the wrong formula or financial model
    \item \texttt{calculation\_error}: Correct formula but arithmetic mistake
    \item \texttt{conceptual\_error}: Fundamental misunderstanding of the financial concept
    \item \texttt{assumption\_mismatch}: Used invalid or inappropriate assumptions
    \item \texttt{extraction\_error}: Misread or misinterpreted the question data
    \item \texttt{incomplete\_reasoning}: Correct approach but failed to complete all steps
\end{itemize}

%% ============================================
%% 4. DATA AND EXPERIMENTAL DESIGN
%% ============================================
\section{Data and Experimental Design}
\label{sec:experiments}

We use all 1{,}032 questions from the CFA-Easy dataset \citep{ke2025findap}. The model is GPT-4o-mini (OpenAI) at temperature $\tau = 0.0$. Each question is evaluated in both MCQ and open-ended format, yielding 2{,}064 inferences for option bias analysis plus 1{,}032 additional inferences for three-tier evaluation.

%% ============================================
%% 5. RESULTS
%% ============================================
\section{Results}
\label{sec:results}

\subsection{Option Bias}

Table~\ref{tab:optbias} presents the core option bias findings.

\begin{table}[htbp]
\centering
\caption{Option Bias Results (GPT-4o-mini, $n = 1{,}032$)}
\label{tab:optbias}
\begin{threeparttable}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\
\midrule
Accuracy WITH options (MCQ) & 82.6\% & Standard benchmark score \\
Accuracy WITHOUT options & 80.6\% & True reasoning ability \\
\textbf{Option bias} & \textbf{+1.9 pp} & Format-inflated performance \\
\midrule
Biased questions (MCQ $\checkmark$, Open $\times$) & 147/1{,}032 (14.2\%) & Questions where options are a ``crutch'' \\
McNemar's $p$-value & 0.251 & Not significant at $\alpha = 0.05$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Option bias = Accuracy$_{\text{MCQ}}$ $-$ Accuracy$_{\text{open-ended}}$. Biased questions are those answered correctly with options but incorrectly without.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_option_bias.pdf}
\caption{MCQ option bias comparison ($N = 1{,}032$). The accuracy gap between MCQ format (82.6\%) and open-ended format (80.6\%) is only +1.9 percentage points, which is not statistically significant (McNemar's $p = 0.251$).}
\label{fig:optbias}
\end{figure}

MCQ format inflates accuracy by only 1.9 percentage points across the full 1{,}032-question corpus. In 14.2\% of questions, the model answers correctly \textit{only} when options are provided---suggesting that option-derived information (magnitude clues, elimination strategies) plays a role for a subset of responses. However, McNemar's test indicates that the option bias is \textit{not} statistically significant ($p = 0.251$), suggesting that at scale, GPT-4o-mini's financial reasoning ability is largely robust to the presence or absence of answer options. This contrasts sharply with the +12.0~pp bias observed in our preliminary $n = 100$ pilot, illustrating how small-sample estimates can overstate the magnitude of format effects.

\subsection{Three-Tier Evaluation}

Table~\ref{tab:threetier} presents the three-tier evaluation of open-ended responses.

\begin{table}[htbp]
\centering
\caption{Three-Tier Evaluation of Open-Ended Responses ($n = 1{,}032$)}
\label{tab:threetier}
\begin{threeparttable}
\begin{tabular}{lccc}
\toprule
\textbf{Level} & \textbf{Count} & \textbf{Percentage} & \textbf{Description} \\
\midrule
Level A (Exact) & 253 & 24.5\% & Correct within 2\% tolerance \\
Level B (Directional) & 222 & 21.5\% & Right approach, different assumptions \\
Level C (Incorrect) & 557 & 54.0\% & Genuine error \\
\midrule
\textbf{Strict Accuracy} (A only) & & \textbf{24.5\%} & \\
\textbf{Lenient Accuracy} (A+B) & & \textbf{46.0\%} & \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item The 21.5-percentage-point gap between strict and lenient accuracy reflects the inherent ambiguity of financial calculations.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig2_three_tier_evaluation.pdf}
\caption{Three-tier evaluation of open-ended responses ($N = 1{,}032$). Over half of responses are genuinely incorrect (Level~C), while 21.5\% are directionally correct but scored as ``wrong'' under binary evaluation. The dashed line marks strict accuracy (24.5\%).}
\label{fig:threetier}
\end{figure}

The results reveal a striking discrepancy: strict accuracy (24.5\%) is less than a third of what the MCQ format (82.6\%) would suggest, while lenient accuracy (46.0\%) partially closes the gap. This means:
\begin{itemize}
    \item \textbf{24.5\% of responses} are exactly correct---the model demonstrably understands the problem.
    \item \textbf{21.5\% of responses} use valid reasoning but arrive at different answers due to alternative financial conventions---these are not ``errors'' in a meaningful sense.
    \item \textbf{54.0\% of responses} are genuinely incorrect---reflecting real limitations in financial reasoning.
\end{itemize}

\subsection{Error Attribution}

Among the 557 Level C responses, error attribution reveals:
\begin{itemize}
    \item \texttt{conceptual\_error}: 383/557 (68.8\%)---fundamental misunderstanding of the financial concept
    \item \texttt{incomplete\_reasoning}: 60/557 (10.8\%)---correct approach but stopped too early
    \item \texttt{assumption\_error}: 59/557 (10.6\%)---used invalid or inappropriate assumptions
    \item \texttt{unknown}: 35/557 (6.3\%)---error type could not be automatically classified
    \item \texttt{reading\_error}: 12/557 (2.2\%)---misread or misinterpreted the question data
    \item \texttt{arithmetic\_error}: 7/557 (1.3\%)---correct formula, wrong calculation
    \item \texttt{formula\_error}: 1/557 (0.2\%)---wrong financial model selected
\end{itemize}

Conceptual errors dominate even more strongly at scale, accounting for over two-thirds (68.8\%) of all genuine errors. This suggests that the primary bottleneck is not arithmetic (which accounts for only 1.3\% of errors) but \textit{selecting the right financial concept or framework} for the given problem. Notably, incomplete reasoning (10.8\%) and assumption errors (10.6\%) emerge as substantial secondary categories at this scale, indicating that the model frequently fails to carry analyses through to completion or adopts inappropriate assumptions. The near-zero formula error rate (0.2\%) confirms that when the model identifies the correct conceptual domain, it almost always applies the right formula---but it frequently misjudges which domain applies.

%% ============================================
%% 6. DISCUSSION
%% ============================================
\section{Discussion}
\label{sec:discussion}

\subsection{Reassessing the Option Bias Effect}

Our full-corpus results paint a substantially different picture from small-sample estimates. The 1.9-percentage-point option bias is statistically non-significant ($p = 0.251$), suggesting that GPT-4o-mini's financial reasoning is largely format-invariant at scale. For an institution evaluating AI tools based on MCQ benchmarks:
\begin{itemize}
    \item A reported MCQ accuracy of 82.6\% closely approximates the open-ended accuracy of 80.6\%---the format effect is negligible.
    \item The 147 biased questions (14.2\%) where options serve as a ``crutch'' are offset by a comparable number where the model performs better \textit{without} options.
    \item Small-sample estimates (our preliminary $n = 100$ pilot showed +12.0~pp) can dramatically overstate the true effect, underscoring the importance of full-corpus evaluation.
\end{itemize}

The dramatic collapse from +12.0~pp ($n = 100$, $p = 0.045$) to +1.9~pp ($n = 1{,}032$, $p = 0.251$) warrants explanation. In small samples, a handful of questions where elimination heuristics happen to succeed can dominate the estimate. At scale, this effect is diluted: the 147 ``crutch'' questions (where options helped) are counterbalanced by 127 questions where removing options \textit{improved} performance---likely because the open-ended format forced the model into deeper step-by-step reasoning rather than shortcut matching. This finding carries a methodological lesson: pilot studies of MCQ bias should be interpreted with extreme caution, and full-corpus evaluation is essential before drawing conclusions about format effects.

\subsection{How MCQ Options Leak Information}

Although the aggregate option bias is small, understanding the \textit{mechanisms} of information leakage remains important for question design. We identify three specific pathways through which MCQ options assist the model:

\begin{enumerate}
    \item \textbf{Magnitude anchoring}: When computing bond prices or portfolio returns, the model may be uncertain about the order of magnitude. Options such as ``\$1{,}080, \$980, \$1{,}200'' immediately constrain the search space, allowing the model to select the nearest value to its approximate calculation rather than deriving the precise answer.
    \item \textbf{Sign disambiguation}: For questions involving gains vs.\ losses or increases vs.\ decreases, options reveal the expected sign of the answer. Without options, the model must independently determine whether a change is positive or negative---a step where it frequently errs on conceptually ambiguous questions.
    \item \textbf{Elimination via implausibility}: The model can reject one or two options using surface-level heuristics (e.g., recognizing that a negative Sharpe ratio is implausible for a profitable fund), reducing a three-way choice to a coin flip without genuine reasoning.
\end{enumerate}

These mechanisms explain why the 14.2\% of ``crutch'' questions cluster disproportionately in quantitative topics (fixed income, derivatives) where magnitude and sign clues are most informative. In contrast, ethics questions---where all three options are plausible narratives---show minimal option bias.

However, this does \textit{not} mean MCQ benchmarks are reliable. The critical issue is not format inflation but the \textit{gap between MCQ accuracy and true open-ended competence}: the model scores 82.6\% on MCQs but only 24.5\% strict accuracy in open-ended format. The real ``option bias'' is not the 1.9~pp gap between MCQ and open-ended \textit{binary scoring}, but the 58.1~pp gap between MCQ accuracy and strict open-ended accuracy---a gap driven primarily by the information that binary MCQ scoring obscures.

\subsection{The Hidden Competence Problem}

The three-tier framework reveals a complementary insight: binary scoring \textit{underestimates} competence on one dimension. The 21.5\% Level B rate means that over one-fifth of ``wrong'' answers are actually reasonable alternative analyses using valid financial reasoning with different assumptions. In practical terms:

\begin{itemize}
    \item An AI that computes a bond yield of 6.32\% (using continuous compounding) when the gold answer is 6.45\% (using semi-annual compounding) is not ``wrong''---it is using a different but legitimate convention.
    \item A model computing an effective annual rate assuming daily compounding (365 days) when the gold answer uses quarterly compounding produces a different but financially defensible result.
    \item A depreciation calculation using straight-line method when the gold answer assumes declining-balance: the reasoning structure and formula application are correct, only the accounting convention differs.
    \item A compliance reviewer who flags these as errors wastes review capacity; an AI evaluation that counts them as incorrect underestimates the model's financial competence.
\end{itemize}

The lenient accuracy (46.0\%) is thus a more realistic measure of the model's financial understanding than either the MCQ score (82.6\%) or the overly strict open-ended score (24.5\%). The 36.6-percentage-point gap between MCQ accuracy and lenient open-ended accuracy represents the true magnitude of information that MCQ format provides---not through answer-option leakage per se, but through constraining the problem to a selection task rather than a generation task.

\subsection{Implications for CFA Exam Design}

Our findings have direct implications for the CFA Institute:
\begin{enumerate}
    \item \textbf{AI vulnerability}: The 14.2\% biased question rate (147 of 1{,}032) identifies questions where AI can ``game'' the MCQ format. These items should be reviewed for question quality.
    \item \textbf{Format innovation}: As AI capabilities advance, the CFA Institute should explore partial-credit scoring and open-ended formats for future exam iterations. The 58.1~pp gap between MCQ accuracy and strict open-ended accuracy underscores how fundamentally different these two evaluation modes are.
    \item \textbf{Convention sensitivity}: Level B responses (21.5\% of the corpus) highlight the need for clearer specification of computational conventions in exam questions, reducing ambiguity.
\end{enumerate}

\subsection{Limitations}

Our study uses the full CFA-Easy corpus ($n = 1{,}032$) but only a single model (GPT-4o-mini). Extension to multiple models (including open-source financial LLMs) would strengthen generalizability. The non-significant option bias result ($p = 0.251$) may be model-specific; weaker models might exhibit larger format effects. The LLM-as-judge approach for three-tier classification may itself contain biases, and human validation of a subset of classifications is recommended for future work. The dominance of conceptual errors (68.8\%) could partly reflect the LLM-as-judge's tendency to classify ambiguous errors as conceptual rather than more specific categories.

%% ============================================
%% 7. CONCLUSION
%% ============================================
\section{Conclusion}
\label{sec:conclusion}

This paper demonstrates that MCQ-format evaluations of financial LLMs are fundamentally different from open-ended evaluation, though not in the way previously assumed. The option bias effect is small and non-significant at scale (option bias = +1.9~pp, $p = 0.251$), suggesting that answer options per se do not substantially inflate LLM accuracy. However, the MCQ \textit{task format}---selecting among given answers versus generating answers from scratch---produces a dramatic 58.1-percentage-point gap between MCQ accuracy (82.6\%) and strict open-ended accuracy (24.5\%). Meanwhile, binary scoring classifies reasonable alternative analyses as ``errors'' (21.5\% of ``incorrect'' answers are directionally correct).

We propose a shift in financial LLM evaluation: from MCQ accuracy to open-ended assessment with three-tier scoring. This approach provides a more realistic picture of AI financial competence---one that acknowledges both the fundamental difference between selection and generation tasks and the ambiguity inherent in financial calculations.

\textbf{The question is not whether AI can choose the right option, but whether it can reason to the right answer.} Across 1{,}032 CFA questions, our evidence reveals a profound gap between these two abilities---a gap that is not about option information leakage but about the fundamental nature of financial reasoning.

%% ============================================
%% DECLARATION OF COMPETING INTEREST
%% ============================================
\section*{Declaration of Competing Interest}

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

%% ============================================
%% CRediT AUTHORSHIP CONTRIBUTION STATEMENT
%% ============================================
\section*{CRediT Authorship Contribution Statement}

\textbf{Wei-Lun Cheng}: Conceptualization, Methodology, Software, Formal Analysis, Data Curation, Writing -- Original Draft, Visualization.
\textbf{Daniel Wei-Chung Miao}: Supervision, Writing -- Review \& Editing.
\textbf{Guang-Di Chang}: Supervision, Writing -- Review \& Editing.

%% ============================================
%% ACKNOWLEDGMENTS
%% ============================================
\section*{Acknowledgments}

Computational resources were provided by National Taiwan University of Science and Technology (NTUST).

%% ============================================
%% DATA AVAILABILITY
%% ============================================
\section*{Data Availability}

The CFA-Easy dataset is available via HuggingFace under the FinEval benchmark \citep{ke2025findap}. Experiment code is available from the corresponding author upon reasonable request.

%% ============================================
%% REFERENCES
%% ============================================
\begin{thebibliography}{20}

\bibitem[Callanan et al.(2023)]{callanan2023gpt}
Callanan, E., Mbae, A., Selle, S., Gupta, V., \& Houlihan, R. (2023).
\newblock Can GPT-4 pass the CFA exam?
\newblock \textit{arXiv preprint arXiv:2310.09542}.

\bibitem[Cobbe et al.(2021)]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., et al. (2021).
\newblock Training verifiers to solve math word problems.
\newblock \textit{arXiv preprint arXiv:2110.14168}.

\bibitem[Gao et al.(2024)]{gao2024llms}
Gao, J., Guo, C., Zhang, Y., et al. (2024).
\newblock Are LLMs good at multiple choice questions? A benchmark for MCQ evaluation.
\newblock \textit{arXiv preprint}.

\bibitem[Ke et al.(2025)]{ke2025findap}
Ke, Z., Ming, Y., Nguyen, X. P., Xiong, C., \& Joty, S. (2025).
\newblock Demystifying domain-adaptive post-training for financial LLMs.
\newblock In \textit{Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)}.

\bibitem[Lord(1980)]{lord1980applications}
Lord, F. M. (1980).
\newblock \textit{Applications of Item Response Theory to Practical Testing Problems}.
\newblock Lawrence Erlbaum Associates.

\bibitem[Robinson et al.(2023)]{robinson2023leveraging}
Robinson, J., Sloane, C., Liang, P., \& Tenenbaum, J. (2023).
\newblock Leveraging large language models for multiple choice question answering.
\newblock \textit{arXiv preprint arXiv:2210.12353}.

\end{thebibliography}

\end{document}
