# Independent Peer Review: Three-Paper LLM Financial Reasoning Research Program

**Review Date:** 2026-02-12
**Reviewer:** Independent AI Reviewer (following REF_reviewer.md protocol)
**Scope:** `drafts/edition_260211/` — Three merged papers (A, B, C)

---

## 1. High-Level Assessment

This research program presents a systematic, multi-dimensional evaluation of Large Language Model capabilities in financial reasoning, using CFA examination questions as a standardized testbed. The three papers address complementary questions: *how fragile is LLM financial competence?* (Paper A), *what does AI exam-passing mean for professional certification economics?* (Paper B), and *do LLMs inherit human cognitive biases and ethical vulnerabilities?* (Paper C). The overall research program is well-conceived, addressing a timely and practically important question. The approach—moving beyond headline accuracy to probe robustness, error structure, calibration, signaling value, behavioral biases, and ethical resilience—represents a meaningful contribution to the nascent field of AI financial evaluation. The three-paper structure is logical, with minimal redundancy and clear thematic separation suitable for different target journals (IRFA, FAJ, FRL). However, the work is constrained by its reliance on a narrow model ecosystem (primarily GPT-4o-mini with GPT-5-mini cross-validation, both from OpenAI), moderate sample sizes for some experiments, and an empirical base that—while extensive in total inferences—draws from a single question bank (FinEval CFA-Easy/Challenge).

---

## 2. Novelty Check

**Literature search scope:** 42 peer-reviewed and preprint papers identified across 8 subcategories (LLM financial evaluation, robustness/memorization testing, calibration, behavioral biases, adversarial ethics, signaling theory, error analysis, AI in finance).

### Paper A: "The Illusion of Financial Competence" — Novelty: MEDIUM-HIGH

**Closest prior works:**

| # | Paper | Venue | Relationship |
|---|-------|-------|-------------|
| 1 | GSM-Symbolic (Mirzadeh et al., 2024) | ICLR 2025 | Direct methodological ancestor; counterfactual perturbation on math benchmarks |
| 2 | CFA-Based Benchmark Study (Chen et al., 2025) | arXiv 2509.04468 | 1,560 CFA questions with error analysis; most directly overlapping in domain |
| 3 | Mind the Confidence Gap (Chhikara et al., 2025) | arXiv 2502.11028 | ECE/overconfidence analysis across 9 LLMs |
| 4 | ErrorMap and ErrorAtlas (2026) | arXiv 2601.15812 | General LLM error taxonomy from 35 datasets — **naming collision** |
| 5 | Advanced Financial Reasoning at Scale (2025) | arXiv 2507.02954 | 23 LLMs on CFA Level III |

**Delta:** No prior work combines counterfactual stress testing + error taxonomy + calibration analysis in a unified financial evaluation framework. The memorization paradox (GPT-5-mini's gap nearly doubling GPT-4o-mini's) is a genuine empirical contribution. The GCI experiment is novel. However, individual dimensions are domain transfers of existing methods (GSM-Symbolic → finance, general calibration → CFA).

**Concern:** The "Error Atlas" naming directly collides with a concurrent paper (arXiv 2601.15812). Recommend renaming to avoid reviewer confusion.

### Paper B: "When Machines Pass the Test" — Novelty: HIGH

**Closest prior works:**

| # | Paper | Venue | Relationship |
|---|-------|-------|-------------|
| 1 | AI and Sustainability of Signaling Roles of Higher Education (2024) | Sustainability | Qualitative analysis of AI's impact on credential signaling |
| 2 | Training for Obsolescence (Peterson, 2025) | arXiv | Formal economic model of education/AI substitution |
| 3 | Making Talk Cheap (Galdin & Silbert, 2025) | Princeton WP | Spence signaling applied to freelance labor markets |
| 4 | Can GPT-4 Pass the CFA Exam? (Callanan et al., 2023) | Working paper | Establishes empirical fact that LLMs pass CFA |
| 5 | Reasoning Models Ace CFA (Patel et al., 2025) | arXiv | Latest AI-CFA performance benchmarks |

**Delta:** This is the most novel of the three papers. No existing work formalizes a Modified Spence Signaling Model with AI replication costs, derives the Partial Signaling Collapse Theorem, or computes a quantitative Signaling Retention Ratio for any professional certification. The six-dimensional ability taxonomy mapping CFA curriculum to Autor (2003) task categories is original. The empirical support (option bias experiment) is relatively thin but the theoretical contribution stands on its own.

### Paper C: "Inherited Irrationality and Ethical Fragility" — Novelty: MEDIUM

**Closest prior works:**

| # | Paper | Venue | Relationship |
|---|-------|-------|-------------|
| 1 | LLM Economicus (Ross, Kim, Lo, 2024) | COLM 2024 | Utility-theory framework measuring LLM biases — **most direct competitor** |
| 2 | BiasBuster (EMNLP Findings 2024) | EMNLP 2024 | 13,465 prompts testing cognitive biases with debiasing |
| 3 | Comprehensive Evaluation of Cognitive Biases (NLP4DH 2025) | COLING 2025 | 45 LLMs, 8 biases, 2.8M responses |
| 4 | Risk Concealment in Financial LLMs (2025) | arXiv 2509.10546 | Multi-turn adversarial attacks on financial LLMs, 93.18% ASR |
| 5 | Self-Adaptive Cognitive Debiasing (Lyu et al., 2025) | 2025 | CoT amplifies biases; self-reflection debiasing |

**Delta:** The finance-domain grounding (CFA-level investment scenarios, disposition effect) and three-tier debiasing hierarchy are genuine contributions. The adversarial ethics component targeting CFA Standards with the ERS metric and rationalization taxonomy is novel in its domain specificity. However, the behavioral biases component has substantial prior art at much larger scales (BiasBuster: 13K prompts; comprehensive study: 2.8M responses vs. Paper C's 60+80 scenarios). The ethical fragility component is more novel but statistically limited (N=47).

---

## 3. Experimental Validity

### Data Verification Results

All key numerical claims were verified against raw experiment JSON files. Results:

| Paper | Component | N Claims Checked | Verified | Discrepancies |
|-------|-----------|-----------------|----------|---------------|
| A | I1 Stress Testing | 12 | 12 | 0 |
| A | I3 Noise Sensitivity | 14 | 14 | 0 |
| A | E1 Error Taxonomy | 16 | 16 | 0 |
| A | D1 CFA-Easy Calibration | 7 | 7 | 0 |
| A | **D1 CFA-Challenge Calibration** | **9** | **2** | **7** |
| B | A5 Option Bias (GPT-4o-mini) | 7 | 7 | 0 |
| B | A5 Option Bias (GPT-5-mini corrected) | 7 | 7 | 0 |
| C | I2 Behavioral Biases | 14 | 12 | 2 (minor) |
| C | D6 Adversarial Ethics | 12 | 12 | 0 |

### Detailed Discrepancy Analysis

**ISSUE 1 (Paper A, Table 5 — CFA-Challenge Calibration): MODERATE SEVERITY**

The paper reports N=257 total observations (90 + 95 + 72 across three model-method combinations), but the tracked result file `run_20260202_034237/results.json` contains only N=250 (90 + 90 + 70). The paper claims 95 GPT-4o-mini verbalized observations and 72 Qwen3-32B observations, but the data shows 90 and 70 respectively. This discrepancy propagates to all derived metrics in Table 5:

| Metric | Paper | Recomputed from Data | Gap |
|--------|-------|---------------------|-----|
| GPT-4o-mini Verb. ECE | 0.315 | 0.318 | -0.003 |
| GPT-4o-mini Verb. AUC | 0.586 | 0.600 | -0.014 |
| Qwen3-32B ECE | 0.247 | 0.241 | +0.006 |
| Qwen3-32B AUC | 0.787 | 0.760 | +0.027 |
| Overconfident errors | 77 (30.0%) | 74 (29.6%) | +3 (+0.4pp) |

**Assessment:** The discrepancies are small and do not change any qualitative conclusions (all configurations remain substantially overconfident). However, the missing 7 observations cannot be traced to any tracked result file, which is a reproducibility concern. The paper should either (a) reconcile the numbers with the tracked data or (b) note that a supplementary run produced the additional observations.

**ISSUE 2 (Paper C, I2 Wilcoxon test): LOW SEVERITY**

The paper reports Wilcoxon W=14.0, p=0.023, r=0.284. Verification confirms W=14.0, but:
- Exact Wilcoxon test yields p=0.027 (not 0.023)
- Approximate test yields p=0.013 (not 0.023)
- Effect size r=0.286 (not 0.284)

**Assessment:** The discrepancy is minor. In all cases p < 0.05, so the conclusion (significant debiasing effect) is unchanged. Likely a rounding or software version difference.

### Validity Checklist

| Check | Paper A | Paper B | Paper C |
|-------|---------|---------|---------|
| **Data leakage** | LOW RISK. CFA-Easy from FinEval (public), but perturbations are novel. However, using GPT-4o-mini to *generate* perturbations and then *evaluate* GPT-4o-mini on them creates a circular dependency risk. | LOW RISK. Same questions in with/without format. | LOW RISK. Custom scenarios generated independently. |
| **Baseline fairness** | CONCERN. No human baseline for comparison. The paper frames 82.4% as "impressive" but we don't know typical CFA candidate performance on these exact questions. | OK. Within-model paired comparison eliminates confounds. | CONCERN. No human behavioral bias baseline for comparison (how biased are human financial advisors on the same scenarios?). |
| **Statistical significance** | GOOD for I1/I3 (N=1,032). WEAK for D1 CFA-Challenge (N=90). Topic-level analyses appropriately flagged as exploratory. | GOOD. McNemar's test appropriate for paired nominal data. N=1,032 provides excellent power. | WEAK for I2 (N=60, 10 per bias type). WEAK for D6 (N=47, ~9 per attack type). Supplementary synthetic experiments partially address this. |
| **Metric gaming** | LOW RISK. Multiple metrics capture different aspects. CaR is novel and informative. | LOW RISK. McNemar's is the standard test for this design. | MODERATE RISK. LLM-as-judge scoring (0/0.5/1.0) is coarse and could introduce systematic bias. |
| **Reproducibility** | GOOD. Hyperparameters specified, code available on request. Temperature τ=0.0 ensures determinism. **But** D1 CFA-Challenge numbers don't match tracked data. | GOOD. Fully specified. | GOOD for D6. MODERATE for I2 (synthetic scenario generation process not fully specified). |
| **Ablation completeness** | PARTIAL. No ablation of perturbation methods (e.g., what if you perturb text rather than numbers?). No ablation of noise types in combination. | N/A (theoretical paper with single experiment). | PARTIAL. No ablation of prompt structure for bias measurement. No test of attack combinations. |

---

## 4. Methodology Critique

### Paper A

**Strengths:**
- Three-dimensional framework is genuinely complementary (robustness × error structure × calibration)
- Population-level coverage (N=1,032) for stress testing is excellent
- GCI experiment provides actionable insight (knowledge vs. reasoning gaps)
- Cross-model memorization paradox is a striking and policy-relevant finding

**Weaknesses:**
1. **Circular perturbation generation.** Using GPT-4o-mini to generate perturbations that are then tested on GPT-4o-mini creates a confound: the model may fail on perturbations precisely because they were generated by a system with similar failure modes. The 68% valid perturbation rate suggests many generated perturbations are flawed. A stronger design would use human-verified perturbations or a different model for generation.

2. **GCI knowledge vs. attention confound.** The paper acknowledges this (line 369) but it undermines the headline claim that 82.4% of errors are "knowledge gaps fixable via RAG." They could equally be attention gaps where the model already "knows" the concept but fails to retrieve it without prompting.

3. **Fixed confidence register interpretation.** The CFA-Easy calibration finding (ECE=0.073) could alternatively be interpreted as the model having learned appropriate calibration for easy questions, rather than having a "fixed register." The paper's interpretation (no metacognitive awareness) is one of two plausible readings.

4. **No human baseline.** Without knowing human performance on identical perturbations, we cannot claim the memorization gap is uniquely an AI problem. Humans also show performance degradation on unfamiliar variants of familiar problem types.

### Paper B

**Strengths:**
- Most theoretically rigorous of the three papers
- Partial Signaling Collapse Theorem is a clean, well-proven result
- Integration of Spence, Autor, and Becker frameworks is elegant
- Policy implications are concrete and actionable

**Weaknesses:**
1. **Calibrated assumptions without estimation.** The AI replicability values (ρk) in Table 3 are "calibrated" (assumed) rather than empirically estimated. The entire quantitative conclusion (R=28.8%) rests on these assumed values. The sensitivity analysis (R ∈ [0.19, 0.39] under ±0.10 perturbation) is helpful but the core inputs remain subjective.

2. **Thin empirical component.** The option bias experiment is the sole empirical evidence. It demonstrates format invariance/dependence but does not directly test the signaling model's predictions (e.g., differential wage premium erosion, employer behavioral shifts). FAJ reviewers may expect more empirical grounding.

3. **Curriculum weight assumptions.** The weights (w_k) in Table 4 are described as "estimated" but the estimation methodology is not specified. Are they from CFA Institute publications? Expert survey? Author judgment? This matters because R is directly computed from these weights.

4. **Static model in a dynamic world.** The model treats ρk as moving exogenously. In reality, certification bodies adapt (the CFA Institute has already announced curriculum changes emphasizing AI). The model does not capture this strategic response, which would shift the equilibrium.

### Paper C

**Strengths:**
- Dual-threat framing (biases + ethics) is compelling
- Three-tier debiasing hierarchy is practically useful
- Rationalization taxonomy (utilitarian override, authority deference, semantic repackaging) is insightful
- Synthetic replications partially address small sample concerns

**Weaknesses:**
1. **Small sample sizes.** N=60 (10 per bias type) for behavioral biases and N=47 for adversarial ethics are the smallest in the program. Ten scenarios per bias type is insufficient for reliable per-type conclusions. The paper's claims about individual bias types (e.g., "disposition effect is entirely resistant") rest on 10 observations each.

2. **LLM-as-judge bias scoring.** Using GPT-4o-mini to judge GPT-4o-mini's behavioral biases introduces circularity. If the model has systematic biases, the judge (same model) may share them and fail to detect them. The coarse 3-point scale (0, 0.5, 1.0) further limits measurement precision.

3. **Single-model limitation.** The behavioral biases study tests only GPT-4o-mini. Without cross-model validation (abandoned due to GPT-5-mini empty responses), we cannot distinguish model-specific from general LLM behavior. This is appropriately noted in limitations but significantly constrains generalizability.

4. **Adversarial attack sophistication.** Single-turn prepended prompts are the weakest form of adversarial attack. Prior work (FITD, Risk Concealment) shows multi-turn attacks achieve dramatically higher success rates. The 14 flips from Paper C's attacks may substantially underestimate actual vulnerability.

5. **Causal language.** Despite revisions, the paper still uses "inherited" in the title. The mechanism by which biases arise (training data absorption vs. architectural emergence vs. prompt-response mapping artifacts) is not established. "Exhibit patterns consistent with" is used in the body but the title implies a causal mechanism.

---

## 5. Writing & Presentation

### Paper A

| Aspect | Assessment |
|--------|-----------|
| Clarity | EXCELLENT. The three-dimensional framework is clearly motivated and well-structured. Each dimension builds logically on the previous. |
| Evidence support | STRONG. Claims are generally well-supported by data, with appropriate hedging. The integrated Table 8 is particularly effective. |
| Figures/Tables | GOOD. Tables are informative and well-formatted. The paper could benefit from a visual summary figure (e.g., radar chart of the three dimensions). |
| Limitations | GOOD. Section 6.6 is thorough, covering perturbation validity, classifier bias, sample size, and GCI interpretation. |
| Word count | 3,675 words — appropriate for IRFA. |

### Paper B

| Aspect | Assessment |
|--------|-----------|
| Clarity | VERY GOOD. Mathematical exposition is clean and accessible. The progression from setup → theorem → application → empirics is well-structured. |
| Evidence support | MODERATE. Theoretical claims are well-proven. Empirical claims rest on a single experiment with results shared across papers. |
| Figures/Tables | GOOD. Radar chart (Fig. 1) and signal erosion curve (Fig. 2) are informative. Table 3 (ability taxonomy) is the paper's centerpiece and is well-designed. |
| Limitations | GOOD. Section 8.2 covers key weaknesses including ρk calibration, binary partition simplification, and lack of employer behavior evidence. |
| Word count | 4,430 words — appropriate for FAJ. |

### Paper C

| Aspect | Assessment |
|--------|-----------|
| Clarity | GOOD. The dual-threat framing is clear. The three-tier hierarchy is effectively communicated. |
| Evidence support | MODERATE. Individual per-type claims rest on N=10. Synthetic replications help but use a different question generation process. |
| Figures/Tables | ADEQUATE. Tables communicate results clearly. No figures in the paper — adding a visual for the three-tier hierarchy and/or the adversarial degradation pattern would strengthen presentation. |
| Limitations | GOOD. Section 5.5 is honest about sample sizes, LLM-as-judge limitations, and single-model constraints. |
| Word count | 2,421 words — tight for FRL limit (2,500). Leaves almost no room for revision. |

### Cross-Paper Concerns

1. **Data sharing across papers.** The A5 option bias data appears in both Paper A (implicitly, as CFA-Easy accuracy) and Paper B (explicitly, as the empirical evidence). The I1 memorization gap from Paper A is cross-referenced in Paper C's discussion of GPT-5-mini zero flips. This is appropriate for a multi-paper program but reviewers at different journals may flag apparent overlap.

2. **Single question bank.** All three papers draw from the same FinEval CFA-Easy (N=1,032) dataset. While each paper uses the questions differently, a hostile reviewer could argue this is "one experiment reported three ways." The authors should clearly differentiate the experimental designs across papers.

3. **GPT-5-mini empty response problem.** This issue affects data quality across the program (I2 abandoned, A5 corrected). The papers handle it transparently, but it raises questions about the reliability of cross-model comparisons more broadly.

---

## 6. Scoring

### Paper A: "The Illusion of Financial Competence"

| Dimension | Score | Weight | Weighted | Justification |
|-----------|:-----:|:------:|:--------:|---------------|
| Novelty | 72 | 25% | 18.0 | Three-dimensional integration is novel; individual dimensions are domain transfers. Memorization paradox is a genuine finding. |
| Experimental rigor | 78 | 25% | 19.5 | Population-level coverage excellent. D1 CFA-Challenge data discrepancy is concerning. Circular perturbation generation is a methodological weakness. |
| Technical correctness | 82 | 20% | 16.4 | All numbers verified except D1 Challenge (small discrepancies). Statistical methods appropriate. |
| Writing quality | 85 | 15% | 12.75 | Clear, well-structured, appropriate length. Limitations well-discussed. |
| Impact potential | 80 | 15% | 12.0 | Directly relevant to financial regulators and AI governance. CaR metric is practically useful. |
| **Weighted total** | | | **78.7** | **Competitive — weak accept / borderline for IRFA** |

### Paper B: "When Machines Pass the Test"

| Dimension | Score | Weight | Weighted | Justification |
|-----------|:-----:|:------:|:--------:|---------------|
| Novelty | 88 | 25% | 22.0 | Most original contribution. No prior formal signaling model for AI-era certification. |
| Experimental rigor | 62 | 25% | 15.5 | Single experiment (option bias), calibrated rather than estimated parameters. Thin empirical base for a journal that values both theory and evidence. |
| Technical correctness | 85 | 20% | 17.0 | Mathematical proofs are correct. Option bias statistics fully verified. Sensitivity analysis provided. |
| Writing quality | 88 | 15% | 13.2 | Excellent theoretical exposition. Clean mathematical writing. Well-structured progression. |
| Impact potential | 90 | 15% | 13.5 | Directly targets FAJ readership (CFA charterholders). Policy implications are concrete and actionable. |
| **Weighted total** | | | **81.2** | **Competitive — weak accept for FAJ** |

### Paper C: "Inherited Irrationality and Ethical Fragility"

| Dimension | Score | Weight | Weighted | Justification |
|-----------|:-----:|:------:|:--------:|---------------|
| Novelty | 60 | 25% | 15.0 | Behavioral biases component has substantial prior art. Finance-domain specificity and debiasing hierarchy add value but incrementally. |
| Experimental rigor | 58 | 25% | 14.5 | Small sample sizes (N=60, N=47). Single model. LLM-as-judge circularity. Synthetic replications partially compensate. |
| Technical correctness | 75 | 20% | 15.0 | Numbers verified. Minor Wilcoxon p-value discrepancy. Appropriate statistical tests given design. |
| Writing quality | 78 | 15% | 11.7 | Clear dual-threat framing. Tight word count leaves little room for detail. No figures. |
| Impact potential | 72 | 15% | 10.8 | Practically relevant (robo-advisory, compliance). Debiasing hierarchy is actionable. |
| **Weighted total** | | | **67.0** | **Significant issues — major revision for FRL** |

### Overall Program Score

| Paper | Target Journal | Score | Verdict |
|-------|---------------|:-----:|---------|
| A | IRFA (IF 9.8) | 78.7 | Weak Accept — competitive with revisions |
| B | FAJ (CFA Institute) | 81.2 | Weak Accept — strong theoretical contribution |
| C | FRL (IF 6.9) | 67.0 | Major Revision — needs larger samples or scope reduction |
| **Program average** | | **75.6** | **Competitive overall** |

---

## 7. Actionable Recommendations

### Must Fix (Blocking Issues)

1. **[Paper A] Reconcile D1 CFA-Challenge data.** Table 5 reports N=257 but tracked data contains N=250. Either identify the source of the 7 additional observations and add them to the repository, or correct the table to match tracked data. This is a reproducibility blocker.

2. **[Paper A] Rename "CFA Error Atlas."** A concurrent paper "ErrorMap and ErrorAtlas" (arXiv 2601.15812, Jan 2026) uses the same terminology for a general-purpose LLM error taxonomy. Rename to "CFA Failure Taxonomy" or "Financial Error Cartography" to avoid confusion and accusations of parallel claiming.

3. **[Paper C] Address scale limitations explicitly in abstract/introduction.** The abstract does not flag that behavioral bias conclusions rest on N=10 per type. Reviewers will catch this immediately. Either (a) increase to 20-30 hand-crafted scenarios per type, or (b) lead with the synthetic replication (N=80) as the primary result and position the original 60 as the pilot. Currently the paper presents N=60 as primary and N=80 as supplementary, which inverts the statistical strength.

4. **[Paper C] Fix Wilcoxon p-value.** Paper reports p=0.023 but exact test yields p=0.027. Small difference but easily caught by a reviewer who re-runs the test. Use the exact p-value.

### Should Fix (Significant Improvements)

5. **[Paper A] Add human baseline discussion.** Even if human data is unavailable, discuss expected human memorization gap (citing cognitive psychology literature on transfer learning). This contextualizes the AI findings.

6. **[Paper A] Address perturbation circularity.** Add a sentence acknowledging that using the same model family for perturbation generation and evaluation could inflate the memorization gap. Note that the GPT-5-mini cross-validation (different model) partially addresses this.

7. **[Paper B] Specify curriculum weight methodology.** How were the w_k values in Table 4 derived? If author judgment, say so explicitly and expand the sensitivity analysis. If based on CFA Institute publications, cite them.

8. **[Paper B] Strengthen empirical section.** Consider adding a brief survey of CFA charterholders' perceptions of AI impact on certification value, even informal. FAJ reviewers are practitioners who want real-world grounding beyond option bias statistics.

9. **[Paper C] Add at least one figure.** The paper has zero figures, which is unusual for an empirical paper. A bar chart of the three-tier hierarchy or a heatmap of adversarial degradation would improve readability and reviewer impression.

10. **[Paper C] Differentiate more explicitly from LLM Economicus.** The Related Work section (lines 79-85) devotes only 2 sentences to this. Expand to a full paragraph explaining: (a) they use abstract gambles, you use CFA investment scenarios; (b) they don't test disposition effect; (c) you provide a debiasing hierarchy they don't. Make the delta unmistakable.

11. **[All Papers] Cite Risk Concealment paper.** arXiv 2509.10546 (2025) is a directly relevant adversarial attack paper on financial LLMs that is not cited in any of the three papers. Reviewers in this space will know it.

### Nice to Have (Minor Suggestions)

12. **[Paper A] Consider AUROC confidence intervals.** The paper reports point estimates for AUROC (0.586, 0.671) without intervals. Bootstrap CIs would strengthen the claim that "AUROC remains mediocre."

13. **[Paper B] Add a dynamic extension.** Even a brief appendix showing how R evolves as a function of time (with ρk increasing at assumed rates) would add predictive value to the model.

14. **[Paper C] Report inter-rater reliability for rationalization taxonomy.** The three rationalization strategies (utilitarian override, authority deference, semantic repackaging) are qualitatively derived. Having a second coder classify the 14 flips would strengthen the taxonomy.

15. **[Cross-paper] Prepare a clear differentiation matrix** for each cover letter explaining how the three papers differ despite sharing the same question bank. Anticipate the "salami slicing" objection.

---

## 8. Potential Research Directions

### Gaps Exposed by Current Work

1. **Human-AI comparative memorization study.** Paper A's memorization paradox begs the question: do human CFA candidates show similar perturbation sensitivity? A study administering original + perturbed questions to human candidates would establish whether the "memorization gap" is AI-specific or a general property of exam preparation.

2. **Employer signaling response.** Paper B's testable predictions (differential wage premium erosion, supplementary screening adoption) are untested. A survey or natural experiment studying how financial firms are adjusting hiring practices in response to AI would transform the theoretical contribution into an empirical one.

3. **Multi-turn adversarial escalation on CFA Ethics.** Paper C's single-turn attacks found 14 flips; the FITD literature suggests multi-turn attacks would find substantially more. A progressive escalation study (1-turn → 3-turn → 5-turn) would map the vulnerability surface and test GPT-5-mini's apparent robustness more rigorously.

### Natural Extensions

4. **Cross-provider model comparison.** Extending from OpenAI-only to Claude, Gemini, Llama, and Qwen families would dramatically strengthen generalizability claims across all three papers. The calibration analysis (Paper A) already includes Qwen3-32B as a proof of concept.

5. **Debiasing intervention experiments.** Paper C's three-tier hierarchy predicts that Tier 1 biases respond to prompting, Tier 2 to architecture, Tier 3 to training. Actually implementing and testing these interventions (system prompts for Tier 1, RAG for Tier 2, RLHF for Tier 3) would be a high-impact follow-up.

6. **Temporal memorization tracking.** Paper A's memorization paradox suggests an important dynamic: as models improve, memorization may increase. Tracking the memorization gap across model releases (GPT-4o → 4o-mini → 5-mini → future models) would establish whether this is a persistent trend or a generational anomaly.

### Unexplored Angles

7. **Regulatory sandbox application.** Paper A's tiered deployment thresholds (ECE < 0.15, Memorization Gap < 10%) could be piloted with financial regulators. A policy paper or white paper translating these thresholds into actionable regulatory guidance would have high practical impact.

8. **Adversarial robustness as a training signal.** Paper C's finding that GPT-5-mini achieves zero adversarial flips suggests alignment improvements work. Investigating *what changed* between model generations (RLHF, constitutional AI, instruction tuning) could inform best practices for financial AI safety.

9. **CFA Institute collaboration.** Paper B's theoretical framework would be greatly strengthened by access to actual CFA Institute curriculum weight data, pass rate distributions by topic, and employer hiring data. A partnership with CFA Institute Research Foundation could transform this from an academic exercise into an institutional impact study.

---

## Appendix: Data Verification Summary

| Paper | Experiment | Claims Checked | Verified | Discrepancies | Severity |
|-------|-----------|:--------------:|:--------:|:-------------:|----------|
| A | I1 Stress Testing | 12 | 12 | 0 | — |
| A | I3 Noise Sensitivity | 14 | 14 | 0 | — |
| A | E1 Error Taxonomy | 16 | 16 | 0 | — |
| A | D1 CFA-Easy | 7 | 7 | 0 | — |
| A | D1 CFA-Challenge | 9 | 2 | 7 | MODERATE |
| B | A5 GPT-4o-mini | 7 | 7 | 0 | — |
| B | A5 GPT-5-mini | 7 | 7 | 0 | — |
| C | I2 Biases | 14 | 12 | 2 | LOW |
| C | D6 Ethics (4o-mini) | 6 | 6 | 0 | — |
| C | D6 Ethics (5-mini) | 1 | 1 | 0 | — |
| C | D6 Synthetic | 4 | 4 | 0 | — |
| **Total** | | **97** | **88** | **9** | |

**Overall data integrity: 90.7% of claims verified exactly. All discrepancies are traceable to a single root cause (D1 CFA-Challenge sample size mismatch) or minor rounding (I2 Wilcoxon p-value). No evidence of fabrication or systematic error.**
