# Independent Peer Review — Edition 260211
**Date:** 2026-02-22
**Scope:** Three papers targeting IRFA (Paper A), FAJ (Paper B), FRL (Paper C)
**Reviewer protocol:** REF_reviewer.md v1
**Prior reviews:** NOT consulted (per protocol)

---

## 1. High-Level Assessment

This project presents a systematic, multi-dimensional evaluation of LLM financial reasoning through three complementary papers. Paper A ("The Illusion of Financial Competence") combines stress testing, error taxonomy, and calibration analysis to argue that headline benchmark accuracy overstates genuine LLM competence. Paper B ("When Machines Pass the Test") develops a Modified Spence Signaling Model to analyze how AI replication erodes professional certification value, with empirical support from an option bias experiment. Paper C ("Inherited Irrationality and Ethical Fragility") examines behavioral biases and adversarial ethical vulnerability in financial LLMs. The research questions are well-defined, the experimental framework is coherent, and the three papers form a logically integrated research program. The approach of probing beyond headline accuracy—asking *why* models succeed, *how* they fail, and *whether they know when they're wrong*—is timely and practically relevant. The main risk is that the experimental infrastructure relies heavily on GPT-4o-mini both as subject and evaluator, creating circularity concerns that are only partially addressed.

---

## 2. Novelty Check

### Literature Search (45 papers examined)

Papers surveyed span 8 categories: (1) LLM evaluation on financial/CFA exams (Callanan 2023, Shah 2025, Ke 2025/FinDAP, Wu 2023/BloombergGPT, Xie 2023/PIXIU, Xie 2024/FinBen); (2) LLM robustness/memorization (Mirzadeh 2024/GSM-Symbolic, Li 2024/GSM-Plus, Yang 2025/GSM-DC, Shi 2023, Ishikawa 2025, Jiang 2024); (3) LLM calibration (Xiong 2024, Geng 2024, Kadavath 2022, Chhikara 2025, Tian 2023, Chen 2025/UQ survey); (4) LLM behavioral biases (Ross 2024/LLM Economicus, Koo 2024/CoBBLEr, Echterhoff 2024, Lyu 2025/SACD, Jones & Steinhardt 2022, Pezeshkpour 2024); (5) adversarial ethics/jailbreaking (Zou 2023, Perez 2022, Ganguli 2022, Zhuo 2023, Souly 2025/StrongREJECT, Wang 2025); (6) AI impact on certification/signaling (Spence 1973, Connelly 2025, Acemoglu & Restrepo 2019, Autor et al. 2003, Autor & Thompson 2025, Nori 2023); (7) LLM error analysis (Chen 2025/error classification, Song 2025, Zhong 2025, Li 2025/knowledge-prediction gap); (8) AI finance benchmarks (Mateega 2025/FinanceQA, Yang 2023/FinGPT, Drydakis 2024, Katz 2024).

### Closest Prior Work and Delta

| Paper | Novelty Rating | Closest Prior | Key Delta |
|-------|:---:|---|---|
| A (IRFA) | **MEDIUM-HIGH** | GSM-Symbolic (Mirzadeh 2024) + Xiong 2024 calibration | 3D integration framework, CaR metric, GCI methodology, CFA-specific error taxonomy; no prior work combines all three in finance |
| B (FAJ) | **HIGH** | Connelly 2025 (signaling review) + Autor & Thompson 2025 (expertise) | Modified Spence model with AI replication cost is genuinely novel; no prior work formally models AI-induced signaling collapse in professional certification |
| C (FRL) | **MEDIUM** | Ross et al. 2024 (LLM Economicus) + Zhuo 2023 (red teaming) | Domain application to CFA-level financial scenarios and ethics; ERS metric and debiasing hierarchy are incremental |

### Has the exact experiment been done before?

No, for all three papers. Counterfactual perturbation of CFA questions, CFA-specific error taxonomy with GCI, and the CaR metric (Paper A); formal Spence model extension with AI replication cost (Paper B); and joint behavioral bias + adversarial ethics testing on CFA content (Paper C) are all novel.

### Venue fit assessment

- Paper A → IRFA: Good fit. The three-dimensional framework and regulatory implications align with IRFA's scope.
- Paper B → FAJ: Strong fit. Theoretical contribution with practitioner implications suits FAJ's audience.
- Paper C → FRL: Adequate fit for a short paper format. FRL accepts brief empirical contributions.

---

## 3. Experimental Validity

### Data Verification

Every numerical claim in all three papers was verified against raw experiment JSON files. Results:

| Paper | Claims Verified | Matches | Discrepancies |
|-------|:---:|:---:|:---:|
| A | ~80+ | 79+ | **1** |
| B | 16 | 16 | 0 |
| C | 39+ | 39+ | 0 (1 minor note) |

**Paper A Discrepancy (MODERATE):** The paper states "Even at maximum self-consistency confidence (1.0), the error rate remains 41.7%." The actual error rate at confidence = 1.0 is **32.4%** (12/37). The 41.7% figure is the bin-level error rate for conf ≥ 0.9 (20/48), which includes observations at 0.9, not just 1.0. The broader conclusion (CaR is undefined) is unaffected since 32.4% >> 5%, but the specific number is misattributed.

**Paper C Minor Note:** Rationalization taxonomy (utilitarian 6 + authority 3 + semantic 3 = 12) accounts for only 12 of 14 flips. Two flips are unclassified.

**JSON Bug Found:** The stored AUROC for GPT-4o-mini self-consistency in the results JSON (0.630) differs from the correct recomputed value (0.639). The paper correctly reports 0.639—this is a bug in the metric computation code, not the paper.

### Systematic Checks

| Check | Paper A | Paper B | Paper C |
|-------|---------|---------|---------|
| **Data leakage** | CFA-Easy questions are in GPT-4o-mini's training data (the memorization gap confirms this). The perturbation approach explicitly measures and accounts for contamination. | N/A (theoretical + A5 data) | Synthetic scenarios reduce contamination risk; CFA-Easy ethics questions likely contaminated |
| **Baseline fairness** | Cross-model comparisons use identical question sets on same dataset. ✓ | Both models tested on same N=1,032 questions. ✓ | Same questions, same prompts across attack types. ✓ |
| **Statistical significance** | McNemar tests with Yates correction. ECE/Brier computed. N=1,032 for stress testing, N=257 for calibration (modest). ✓ | McNemar: GPT-4o-mini p=0.251, GPT-5-mini p<0.001. ✓ | Wilcoxon exact test. N=60 pilot (underpowered per type), N=80 replication (12-14/type, still modest). ⚠ |
| **Metric gaming** | CaR is well-motivated. Robust Accuracy is meaningful. ✓ | R=28.8% depends on assumed ρ_k values that are calibrated but not estimated. ⚠ | Bias score {0, 0.5, 1.0} is coarse. ERS is a simple ratio. ⚠ |
| **Reproducibility** | No random seeds for API calls. Temperature=0.0 provides determinism for single-call experiments. Self-consistency uses τ=0.7 but no seed. ⚠ | Deterministic (τ=0.0). ✓ | Deterministic (τ=0.0). ✓ |
| **Ablation completeness** | Three dimensions tested individually and integrated. GCI with/without. Cross-model. ✓ | Sensitivity analysis in limitations (ρ_k ± 0.10 → R ∈ [0.19, 0.39]). ✓ | Pilot + replication. Cross-model for ethics (but not biases). ⚠ |

### Critical Methodological Concerns

**1. LLM-as-Judge Circularity (HIGH)**
GPT-4o-mini is simultaneously the test subject and the judge (for A1 open-ended evaluation, I2 bias scoring, error attribution, and semantic matching). This creates a systematic concern: the model may judge its own errors more leniently. The 55% correct-scoring rate for empty responses (documented in MEMORY.md) is direct evidence of this problem. While the papers acknowledge this in limitations, the magnitude of the effect is underexplored.

**2. Perturbation Generation Circularity (MEDIUM)**
GPT-4o-mini generates the perturbations that are then tested on GPT-4o-mini. The 68% valid perturbation rate suggests 32% of perturbations may be invalid, which would inflate the memorization gap. The `validate_perturbation()` function exists in the codebase but is never called in the experimental pipeline.

**3. Small Per-Cell Sample Sizes (MEDIUM)**
- Paper A: Topic-level claims (Ethics 87.1% reasoning, Derivatives 37.5% calculation) are based on unspecified per-topic N. These are labeled "exploratory" but presented with false precision.
- Paper C: 10 scenarios per bias type in the pilot, 12-14 in the replication. The Wilcoxon test operates on N=60 with 47/60 zero differences.
- Paper A calibration: N=90 per configuration on CFA-Challenge.

---

## 4. Methodology Critique

### Paper A: Fundamental Design

The three-dimensional framework is well-conceived, but two assumptions deserve scrutiny:

1. **Perturbation validity assumption.** The paper assumes that questions failing perturbation reflect memorization. An alternative hypothesis: the model has learned fragile heuristics that break under *any* numerical change, even for genuinely understood concepts. The paper partially addresses this with the GCI experiment, but the two experiments measure different populations (GCI operates on Level C errors, while perturbation operates on all questions).

2. **CaR as a regulatory metric.** The paper proposes CaR by analogy to VaR, but the analogy is imperfect. VaR is estimated from a distribution of outcomes; CaR is computed from a finite set of observations where confidence values are heavily clustered (94% in the 0.85-0.95 range for CFA-Easy). The metric is conceptually sound but may be statistically unstable with small per-bin counts.

### Paper B: Theoretical Rigor

The formal model is the strongest contribution across all three papers. Two concerns:

1. **Assumed vs. estimated ρ_k.** The entire quantitative analysis (R=28.8%) rests on assumed replicability values (ρ_k ∈ {0.95, 0.92, 0.70, 0.45, 0.30, 0.15}). The sensitivity analysis (R ∈ [0.19, 0.39]) is appreciated but insufficient—the *ordering* of dimensions could also change. A systematic method for estimating ρ_k from experimental data would substantially strengthen the paper.

2. **Empirical evidence scope.** The A5 option bias experiment tests format invariance (MCQ vs. open-ended), which is one testable prediction of the theory. The connection from option bias results to signaling erosion requires accepting several intermediate theoretical steps.

### Paper C: Combined Design

1. **Bias score granularity.** The {0.0, 0.5, 1.0} scoring scale limits discriminating power. With only 10 scenarios per type and a 3-point scale, per-type estimates have very wide implicit confidence intervals.

2. **"Inherited" language.** The term "inherited irrationality" implies a causal mechanism (training data → model biases) that is not empirically established. The biases could arise from architecture, RLHF, or prompt-response dynamics rather than training data per se. The paper appropriately softens to "exhibit patterns consistent with" in places but retains "inherited" in the title.

### Cross-Paper Confounding Variables

- **Model version dependency.** All primary results use GPT-4o-mini at a specific snapshot. API model versions are periodically updated; results may not replicate on future checkpoints.
- **Temperature=0.0 is not truly deterministic** for some API backends (the code has a bug where `temperature=0.0` is falsy in Python and may not be sent to the Anthropic backend, though this doesn't affect the OpenAI experiments used here).

---

## 5. Writing & Presentation

### Paper A (IRFA)
- **Strengths:** Clear three-dimensional structure. Tables are well-formatted with appropriate footnotes. The integrated assessment table (Table 8) effectively synthesizes findings. The "Economic Significance" section connects to financial practice.
- **Weaknesses:** The abstract is too long (~280 words; IRFA suggests ~150-200). The "fixed confidence register" claim in the CFA-Easy robustness check is provocative but based on a single comparison (two datasets). The "89% confidence" figure in the Discussion (Standard I(C) paragraph) does not appear in any table and its provenance is unclear.
- **Limitations section:** Thorough and honest. Seven limitations acknowledged including the GCI attention/knowledge gap distinction.

### Paper B (FAJ)
- **Strengths:** Excellent theoretical exposition. The formal model is clearly presented with proper definitions, assumptions, and proofs. Figures (ability taxonomy radar chart, signal erosion curve) are informative. The Limitations section is exemplary in its candor about ρ_k assumptions.
- **Weaknesses:** The transition from theoretical model (Sections 3-4) to empirical evidence (Section 6) feels abrupt—the A5 experiment tests a specific prediction but doesn't directly estimate any model parameter. The "format reform vs. content reform" framing oversimplifies a continuous space.
- **FAJ style:** The paper reads more like an economics journal article than a typical FAJ piece. FAJ readership expects more practitioner-facing language, though the policy recommendations section helps.

### Paper C (FRL)
- **Strengths:** Concise and focused, appropriate for FRL format. The three-tier hierarchy is a clear, memorable contribution. The rationalization taxonomy is qualitatively insightful.
- **Weaknesses:** The scaled replication (Section 4.1.3) is presented in a single paragraph without its own table, making it hard to evaluate the full results. The "double jeopardy" framing is catchy but the two studies are more juxtaposed than integrated—the paper doesn't show that the same questions/scenarios exhibit both behavioral bias and ethical fragility.
- **Claims supported by evidence:** Generally yes, with the caveat that the Wilcoxon test on 60 observations with 47 zero differences is unusual and may be underpowered.

---

## 6. Scoring

| Dimension | Score | Weight | Justification |
|-----------|:---:|:---:|---|
| **Novelty** | 72 | 25% | Paper B is genuinely novel (HIGH). Paper A offers a useful integration framework with the novel CaR metric (MEDIUM-HIGH). Paper C is primarily a domain application (MEDIUM). Average across three papers. |
| **Experimental rigor** | 68 | 25% | Impressive scale (>9,700 inferences for Paper A alone). All numbers verified against raw data. However, LLM-as-judge circularity, small per-cell N in several analyses, and the perturbation generation circularity are significant concerns. |
| **Technical correctness** | 82 | 20% | Only 1 numerical discrepancy found across ~135+ verified claims (the CaR 41.7% vs. 32.4%). Stored AUROC JSON bug exists but paper uses correct value. Minor rounding issues (R=0.288 exact 0.2875) are acceptable. |
| **Writing quality** | 78 | 15% | All three papers are clearly written with logical structure. Paper B's theoretical exposition is particularly strong. Paper A's abstract is too long. Paper C's scaled replication needs a table. |
| **Impact potential** | 74 | 15% | Paper B addresses a genuinely important question (AI erosion of certification signaling) with broad applicability. Paper A's regulatory metric proposals are timely. Paper C's findings, while interesting, have more limited practical novelty. |
| **Weighted total** | **74** | 100% | 72×0.25 + 68×0.25 + 82×0.20 + 78×0.15 + 74×0.15 = 18.0 + 17.0 + 16.4 + 11.7 + 11.1 = **74.2** |

**Calibration:** 74/100 — Competitive but with significant issues requiring revision (weak accept / borderline). Paper B alone would score higher (~80); Paper C alone lower (~65).

---

## 7. Actionable Recommendations

### Must Fix (blocking issues)

1. **Paper A: Correct the CaR error rate claim.** "Even at maximum self-consistency confidence (1.0), the error rate remains 41.7%" should read "32.4%" (at confidence = 1.0) or "41.7%" (at confidence ≥ 0.9). The current wording conflates these. *Location: Section 5.3.4, line ~411.*

2. **Paper A: Clarify the "89% confidence" figure.** The Discussion (Section 6.4, Standard I(C)) states "An AI expressing 89% confidence on answers wrong 30% of the time." The 89% figure does not appear in any results table. Specify its source or replace with a documented value (e.g., "84-86% confidence").

3. **Paper C: Add a table for the scaled replication results.** The N=80 synthetic experiment results are currently in a single prose paragraph (Section 4.1.3). A table showing per-type bias scores, neutral scores, and deltas would allow readers to evaluate the replication properly.

### Should Fix (significant improvements)

4. **All papers: Address LLM-as-judge circularity more explicitly.** Currently mentioned in limitations. Consider adding a brief subsection or paragraph quantifying the potential impact: "What if the judge over-scores empty/wrong responses by X%?" The known 55% correct-scoring rate for empty responses provides a concrete bound.

5. **Paper A: Validate perturbation quality.** The codebase contains `validate_perturbation()` but it was never called. Report the validation rate on a sample (e.g., manual check of 50 perturbations) to bound the perturbation error rate and its impact on the memorization gap.

6. **Paper B: Provide empirical anchoring for ρ_k values.** Even rough estimates (e.g., mapping CFA topic pass rates to AI accuracy rates) would ground the assumed values. Currently the model's quantitative conclusions (R=28.8%) rest entirely on expert judgment.

7. **Paper A: Report topic-level N.** The claims about Ethics (87.1% reasoning) and Derivatives (37.5% calculation) are unverifiable without knowing per-topic sample sizes. Add these N values to the text or a footnote.

8. **Paper C: Report Wilcoxon test details.** Note that 47/60 differences are zero (tied), making the effective sample size for the Wilcoxon test much smaller than N=60. This unusual distribution should be explicitly flagged.

### Nice to Have (minor suggestions)

9. **Paper A: Shorten the abstract.** Current length (~280 words) exceeds typical IRFA guidelines. Target 150-200 words.

10. **Paper B: Add a table mapping empirical results to theoretical predictions.** A 3-column table (Prediction | Test | Result) would sharpen the theory-evidence connection.

11. **Paper C: Consider softening "inherited" in the title.** "Exhibited" or "manifested" would be more precise given that the causal mechanism is unestablished.

12. **All papers: Add a reproducibility statement.** Specify that temperature=0.0 was used for deterministic outputs. Note that API model versions may change over time.

13. **Paper A: The 12/14 unclassified flips in the rationalization taxonomy (from Paper C) is a minor data completeness issue.** Account for the 2 unclassified flips.

---

## 8. Potential Research Directions

### Gaps Exposed by Current Work

1. **Independent judge validation.** Run a human evaluation study (even N=100) comparing human expert scores with GPT-4o-mini judge scores on a stratified sample. This would quantify the judge bias that currently shadows all results.

2. **Perturbation validation study.** Use the existing `validate_perturbation()` function (or manual annotation) to establish ground-truth validity rates. Correct the memorization gap for invalid perturbations: true gap = observed gap × (valid_rate / 100).

3. **Topic-level calibration with sufficient N.** The topic-level claims (Ethics, Derivatives) are currently exploratory. A targeted experiment with N≥50 per topic would enable per-topic statistical testing.

### Natural Extensions

4. **Multi-model calibration comparison.** Extend Paper A's calibration analysis to Claude, Gemini, and open-weight models. The "fixed confidence register" finding (model maintains ~85% confidence regardless of accuracy) is provocative and deserves cross-model validation.

5. **ρ_k estimation from experimental data.** Develop a methodology to estimate AI replicability values from the existing experimental suite (e.g., ρ_k = accuracy_without_options for each CFA topic). This would make Paper B's framework fully data-driven rather than assumption-based.

6. **Longitudinal memorization tracking.** Re-run the counterfactual perturbation experiment on future model checkpoints to test Paper A's prediction that memorization gaps widen with capability.

7. **Multi-turn adversarial attacks for Paper C.** The paper acknowledges (via Chen 2025/FITD) that multi-turn attacks may be more effective. A systematic comparison of 1-turn vs. 3-turn vs. 5-turn escalation would strengthen the ethical fragility findings.

### Unexplored Angles

8. **Financial impact simulation.** The "phantom competence" concept (Paper A) could be quantified via a portfolio simulation: What is the dollar cost of an AI advisor operating at robust accuracy (63.5%) vs. headline accuracy (82.4%) on a model portfolio?

9. **Employer response study.** Paper B's testable prediction P2 (employer behavioral shifts) could be tested via a conjoint experiment: present hiring managers with candidate profiles with/without CFA designation and with/without AI tool access.

10. **GCI control condition.** Paper A identifies the knowledge-gap vs. attention-gap ambiguity. A control experiment injecting *irrelevant* financial concepts would distinguish "the model didn't know" from "the model wasn't paying attention."

---

## Code Review Findings (Supplementary)

Six cross-cutting issues were identified in the experiment codebase:

| # | Issue | Severity | Description |
|---|-------|----------|-------------|
| 1 | LLM-as-Judge circularity | HIGH | GPT-4o-mini judges its own outputs; 55% of empty responses scored correct |
| 2 | Empty response handling | HIGH | No proactive empty-response checks before scoring |
| 3 | Reproducibility gaps | MEDIUM | No random seeds for API calls; self-consistency sampling not seeded |
| 4 | Anthropic temperature bug | LOW-MEDIUM | `temperature=0.0` is falsy in Python, may not be sent (affects only Anthropic backend, not used in these experiments) |
| 5 | Perturbation validation unused | MEDIUM | `validate_perturbation()` exists but never called in pipeline |
| 6 | Layer-5 extraction bias | LOW | Regex fallback matches English article "A", slightly biasing toward answer choice A |

None of these invalidate the paper results, but issues #1 and #2 represent systematic threats that should be discussed. Issue #5 is an easy fix that would strengthen Paper A's claims.

---

*Review completed 2026-02-22. All findings derived from primary sources (LaTeX files, experiment JSON data, Python source code). No prior reviews consulted.*
