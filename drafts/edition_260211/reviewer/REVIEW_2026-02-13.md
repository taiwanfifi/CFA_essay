# Independent Review: LLM Financial Reasoning — Three-Paper Research Program

**Reviewer:** Claude Opus 4.6 (automated independent review)
**Date:** 2026-02-13
**Scope:** `edition_260211/` — Papers A, B, C + experiment code + data
**Protocol:** `REF_reviewer.md` v1

---

## 1. High-Level Assessment

This project presents a three-paper research program systematically evaluating LLM financial reasoning using CFA examination questions, targeting three different journals (IRFA, FAJ, FRL). The core thesis is compelling: headline accuracy on financial benchmarks is a dangerously incomplete measure of AI competence, and the project attacks this from three angles — robustness/error/calibration (Paper A), signaling theory (Paper B), and behavioral/ethical risk (Paper C). The research questions are well-defined and practically relevant. The approach — using the same CFA dataset as a unifying substrate across complementary dimensions — is methodologically sound and creates a coherent research narrative. The three-paper split is strategically intelligent: each paper stands alone with a distinct theoretical contribution while forming a larger story when read together. However, the project has structural vulnerabilities around sample size, reliance on a single primary model (GPT-4o-mini), and the use of LLM-as-judge evaluation that deserve careful scrutiny.

---

## 2. Novelty Check

### Literature Search (47 papers examined)

I searched across the following domains: LLM financial reasoning, CFA exam AI evaluation, LLM calibration, LLM behavioral biases, adversarial AI ethics, professional certification signaling, and data contamination detection.

#### Most Dangerous Competitors (per paper)

| Paper | Most Dangerous Competitor(s) | Why Dangerous | Your Defense |
|-------|------------------------------|---------------|--------------|
| **Paper A** | Apple (2024) GSM-Symbolic; Chen et al. (2025) CFA-Benchmark; MATH-Perturb (ICML 2025); KalshiBench (2025) | GSM-Symbolic established perturbation methodology; Chen et al. do CFA error classification; MATH-Perturb does harder perturbations; KalshiBench is more rigorous on calibration | Three-dimensional integration is unique; financial domain transfer is non-trivial; CaR metric is new; memorization paradox is a novel finding |
| **Paper B** | **Galdin & Silbert (2025, Princeton)** — formal Spence model + AI replication cost on Freelancer.com; structural estimation shows 19% less meritocratic market | Nearly identical theoretical framework; stronger empirical evidence (real labor market data vs. option bias experiment) | Domain shift: formal certification vs. informal portfolios; multi-dimensional ability decomposition (K=6); Partial Signaling Collapse Theorem with selective erosion; R=28.8% metric |
| **Paper C** | **Bini, Cong, Huang & Jin (NBER/NeurIPS 2025)** — "most comprehensive set of experiments to date" on LLM behavioral biases; **TRIDENT (Hui et al., ACL 2025)** — 2,600+ adversarial prompts grounded in CFA ethics standards | Bini et al. is the 800-pound gorilla on LLM biases (larger scale, AFA+NeurIPS); TRIDENT directly tests CFA ethics compliance | CFA-level financial scenarios (not abstract gambles); disposition effect (finance-specific, untested elsewhere); dual-threat framing (bias + ethics); rationalization taxonomy; three-tier debiasing hierarchy |

#### Closest Prior Work (detailed)

| Paper | Closest Competitor(s) | Delta |
|-------|----------------------|-------|
| **Paper A** | Apple (2024) GSM-Symbolic; Li et al. (2024) GSM-Plus; MATH-Perturb (ICML 2025); Lopez-Lira (2025) financial memorization; Chen et al. (2025) CFA-Benchmark; KalshiBench (2025) calibration | First to combine stress testing + error taxonomy + calibration in finance; GCI experiment is novel; CaR metric is new; memorization paradox (better model = larger gap) is unique |
| **Paper B** | Galdin & Silbert (2025) signaling in freelance markets; Katsaros (2024) AI + higher education signaling; Patel et al. (2025) reasoning models on CFA; Zheng et al. (2024, ICLR) MCQ selection bias | First formal Spence model with AI replication cost for professional certification; Partial Signaling Collapse Theorem with selective erosion is original; six-dimensional ability taxonomy; option bias cross-generational reversal is unique |
| **Paper C** | **Bini et al. (NBER/NeurIPS 2025)** behavioral economics of AI; Ross et al. (2024) "LLM Economicus"; Echterhoff et al. (2024) BiasBuster (EMNLP Findings); Capraro et al. (PNAS 2025) amplified cognitive biases; **TRIDENT (Hui et al., ACL 2025)** CFA ethics safety benchmark; Chen et al. (2025) FITD multi-turn attacks | CFA-level financial scenarios (not abstract gambles), first LLM disposition effect test, three-tier debiasing hierarchy, combined bias + ethics dual-threat framework, rationalization taxonomy |

#### Key Papers Found in Search

**LLM on financial exams:**
1. Callanan et al. (2023) — GPT-4 on CFA Level I/II
2. Patel et al. (2025) — o1/o3 pass all CFA levels, 90th percentile
3. Ke et al. (2025) — FinDAP, domain-adaptive post-training
4. Wu et al. (2023) — BloombergGPT
5. Chen et al. (2025) — CFA-Benchmark with error categorization

**Memorization, data contamination, and perturbation:**
6. Apple (2024) — GSM-Symbolic: LLMs cannot do genuine math reasoning (ICLR 2025)
7. Li et al. (2024) — GSM-Plus: 8 perturbation dimensions (ACL 2024)
8. MATH-Perturb (2025) — Hard perturbations that change problem nature (ICML 2025)
9. Lopez-Lira (2025) — Financial LLM memorization: models recall S&P 500 prices verbatim
10. Shi et al. (2023) — Detecting pretraining data
11. DCR (EMNLP 2025) — Quantifying data contamination

**LLM calibration:**
12. Guo et al. (2017) — Modern neural network calibration
13. Kadavath et al. (2022) — "Language models mostly know what they know"
14. Band et al. (2025) — QA-Calibration
15. Chhikara et al. (2025) — Confidence gap across domains
16. Liu et al. (2025) — KalshiBench prediction market calibration (ECE=0.120 for best model)
17. LLM Confidence Estimation Survey (KDD 2025) — Comprehensive ECE survey

**LLM behavioral biases:**
18. Kahneman & Tversky (1979) — Prospect theory
19. Ross et al. (2024) — LLM Economicus: GPT-4 violates EU axioms (COLM 2024)
20. Suri et al. (2024) — LLM loss aversion in economic decisions
21. **Bini, Cong, Huang & Jin (NBER/NeurIPS 2025) — "Behavioral Economics of AI": most comprehensive LLM bias experiments to date; prompting corrections; AFA submission**
22. Capraro et al. (PNAS 2025) — Amplified cognitive biases in moral decision-making; alignment-induced omission bias
23. Echterhoff et al. (2024) — BiasBuster: 13,465 prompts, cognitive bias in LLM decisions (EMNLP Findings)
24. Malberg et al. (2025) — Bias measurement methodology variation
25. Leng (2024) — "Folk Economics in the Machine": mental accounting biases from training text
26. Knipper et al. (2025) — Comprehensive evaluation: susceptibility rates 17.8%-57.3% (NLP4DH)
27. Shefrin & Statman (1985) — Disposition effect
28. Tversky & Kahneman (1974) — Anchoring

**Adversarial AI safety:**
29. Chen et al. (2025) — FITD foot-in-the-door attacks
30. **Hui et al. (ACL 2025) — TRIDENT: 2,600+ adversarial prompts grounded in CFA ethics standards, 19 models**
31. Mazeika et al. (ICML 2024) — HarmBench: 510 behaviors, 18 attack methods
32. Andriushchenko et al. (2025) — Simple adaptive jailbreaks
33. CNFinBench (2025) — Chinese financial safety: 13K+ instances, multi-turn adversarial
34. Anthropic (2024) — Alignment faking: models fake compliance when monitored

**Signaling and labor economics:**
35. Spence (1973) — Job market signaling
36. Becker (1964) — Human capital
37. Autor et al. (2003) — Task-based framework
38. Acemoglu & Restrepo (2019) — Automation and new tasks
39. **Galdin & Silbert (2025, Princeton) — AI signaling erosion in freelance markets: formal Spence model, structural estimation, 19% less meritocratic**
40. Katsaros (2024) — AI erodes both signaling and human capital roles of education
41. Zheng et al. (2024, ICLR Spotlight) — LLMs are not robust MCQ selectors; PriDe debiasing

**RAG, error analysis, and financial benchmarks:**
42. Asai et al. (2024) — Self-RAG (ICLR 2024)
43. Chen et al. (2025) — CFA-Benchmark with error categorization
44. FinAuditing (Xie et al., 2025) — Financial taxonomy-structured benchmark
45. Advanced Financial Reasoning at Scale (2025) — 23 LLMs on CFA Level III

**Additional relevant:**
46. CFA Institute (2023) — Official exam statistics
47. Horton (2023) — "Homo Silicus": LLMs as simulated economic agents (EC 2024)

#### Novelty Assessment

**Paper A (Novelty: HIGH as integrated framework):** The individual components each have close predecessors: GSM-Symbolic/GSM-Plus/MATH-Perturb for perturbation, Chen et al. (2025) for CFA error classification, KalshiBench for calibration. However, no prior paper integrates all three in a unified financial assessment framework. The "Memorization Paradox" (GPT-5-mini higher accuracy but nearly doubled memorization gap) is a genuinely novel finding not reported elsewhere. The "fixed confidence register" (model outputs ~85% confidence regardless of whether accuracy is 53% or 82%) is a striking observation. CaR as a risk metric borrowing VaR terminology is not found in prior literature. The paper must be framed as a unified framework contribution — any single dimension faces a strong predecessor.

**Paper B (Novelty: MODERATE-HIGH):** This is the most theoretically original of the three, but faces a significant challenge from **Galdin & Silbert (2025, Princeton)**. They apply essentially the same theoretical move (Spence model + AI replication cost) to freelance labor markets, with stronger empirical evidence (real Freelancer.com hiring data, structural estimation showing 19% less meritocratic outcomes). Paper B's defense: (a) formal professional certification vs. informal portfolios — the institutional stakes are fundamentally different; (b) the multi-dimensional ability taxonomy (K=6) with selective collapse — Galdin & Silbert don't decompose which skills survive; (c) the Signaling Retention Ratio R=28.8% is a concrete, actionable metric; (d) the cross-generational option bias reversal (p=0.251 → p<0.001) is unique. The paper must cite Galdin & Silbert prominently and position itself as the complementary institutional analysis.

**Paper C (Novelty: MODERATE):** This faces the stiffest competition from multiple directions. **Bini et al. (NBER/NeurIPS 2025)** claim "the most comprehensive set of experiments to date" on LLM behavioral biases with prompting corrections — they are the 800-pound gorilla. **TRIDENT (Hui et al., ACL 2025)** directly tests CFA ethics compliance with 2,600+ adversarial prompts across 19 models. Echterhoff et al. (2024) propose BiasBuster with 13,465 prompts. **Paper C's defenses are:** (1) CFA-level financial scenarios (not abstract gambles) — domain specificity matters for finance journals; (2) first test of the **disposition effect** in LLMs — this is a finance-specific bias completely absent from all competitor work; (3) the **three-tier debiasing hierarchy** (surface/weak/deep) with quantified deltas; (4) the **dual-threat framing** combining bias + ethics is unique — no prior paper examines both failure modes jointly; (5) the rationalization taxonomy is a qualitative contribution competitors lack.

**Is the novelty sufficient for the target venues?**
- Paper A → IRFA: **Yes.** The three-dimensional framework and Memorization Paradox are well-suited for IRFA's applied finance focus. Frame as unified assessment, not individual components.
- Paper B → FAJ: **Yes, with careful Galdin & Silbert differentiation.** The theoretical model is strong, the CFA-specific framing is ideal for FAJ's audience, and the selective collapse result goes beyond Galdin & Silbert. This is the best venue match of the three.
- Paper C → FRL: **Borderline but viable.** FRL values clean experimental design and crisp findings, which Paper C delivers. The dual-threat framing and disposition effect testing are the key differentiators. Must cite and differentiate from Bini et al. and TRIDENT explicitly.

#### Missing Citations (Recommended Additions)

Each paper should cite and differentiate from these works not currently in their reference lists:

**Paper A should add:** MATH-Perturb (ICML 2025), KalshiBench (Liu et al., 2025), "Mind the Confidence Gap" (2025), FinAuditing (Xie et al., 2025)
**Paper B should add:** Galdin & Silbert (2025) — **critical**, Katsaros (2024), Zheng et al. (2024, ICLR) on MCQ selection bias
**Paper C should add:** Bini et al. (NBER/NeurIPS 2025) — **critical**, TRIDENT (Hui et al., ACL 2025) — **critical**, Leng (2024) "Folk Economics", Knipper et al. (2025), CNFinBench (2025)

---

## 3. Experimental Validity

| Check | Assessment | Severity |
|-------|-----------|----------|
| **Data leakage** | CFA-Easy (N=1,032) sourced from SchweserNotes via FinEval (HuggingFace). These questions are almost certainly in GPT-4o-mini and GPT-5-mini training data. The paper acknowledges this and uses perturbation as a mitigation — the memorization gap IS the finding. However, this means "standard accuracy" is an upper bound, not a true measure. | Medium — addressed by design |
| **Baseline fairness** | Cross-model comparisons (GPT-4o-mini vs GPT-5-mini) are not apples-to-apples: different model sizes, architectures, training data, and API behaviors. GPT-5-mini reasoning tokens exhaust budgets differently. The papers acknowledge this but the comparisons still carry weight in the narrative. | Medium |
| **Statistical significance** | Paper A: N=1,032 for stress testing is solid. McNemar's test with Yates' correction used appropriately. Paper B: N=1,032, McNemar's test correctly applied. Paper C: N=60 for bias study is small (10 per bias type), and N=47 for ethics is very small per attack subgroup (~9). Synthetic replications (N=80, N=141) partially address this. Wilcoxon signed-rank test used appropriately for paired data. | **High for Paper C primary; Medium after synthetic replication** |
| **Metric gaming** | Bias scoring via LLM-as-judge (GPT-4o-mini scoring GPT-4o-mini) creates a same-model evaluation circularity. The bias_scorer.py had a bug (markdown code block wrapping) that defaulted all scores to 0.5 — this was fixed, but raises the question: how many other silent failures exist? The confidence fallback to 0.5 when parsing fails in D1 calibration also inflates the "moderate confidence" bucket. | **High** |
| **Reproducibility** | Excellent. All experiments have CLI commands, configs, and result JSON files. Seeds are not explicitly set for LLM API calls (inherent non-determinism), but this is standard. Model versions (gpt-4o-mini, gpt-5-mini) are specified. Hyperparameters documented. | Low |
| **Ablation completeness** | Paper A: No ablation of perturbation methods (why numerical perturbation only? What about structural perturbation?). Paper B: The ability taxonomy weights are author-assigned without empirical calibration. Paper C: No ablation showing which components of adversarial prompts drive flips. | Medium |

### Data Verification Results

I independently verified all key numerical claims against raw experiment JSON files:

**Paper A:** All verified. Memorization gap 18.6pp/36.4pp matches. NSI values match. ECE 0.315 (CFA-Challenge) and 0.073 (CFA-Easy) match. Error taxonomy percentages match. GCI recovery rates match.

**Paper B:** All verified. Option bias percentages, McNemar p-values, and cross-model results all match corrected data. The A5 GPT-5-mini correction (83.2% not 86.3%) is properly reflected.

**Paper C:** 99% verified. One minor discrepancy: D6 GPT-5-mini per-attack accuracy values in the paper (93.6%, 95.7%, 97.9%) show slight variation that doesn't exactly match the raw JSON (~98% uniform across attacks). The core finding (zero flips, ERS > 1.0) is correct and the discrepancy is cosmetic, not substantive.

---

## 4. Methodology Critique

### Fundamental Concerns

**4.1 LLM-as-Judge Circularity (All Papers)**

Using GPT-4o-mini as both test subject and judge/scorer is the single biggest methodological weakness. In Paper C, GPT-4o-mini evaluates GPT-4o-mini's bias responses. In Paper A, the LLM judge evaluates open-ended responses. The known issue — LLM judges score empty responses as correct ~55% of the time — was discovered and partially corrected (A5 GPT-5-mini empty responses, I2 GPT-5-mini data removed), but this raises a systemic question: **what other silent evaluation failures exist that haven't been caught?**

The confidence fallback-to-0.5 issue in D1 calibration is concerning: when the model can't parse a confidence value, it defaults to 0.5 rather than flagging the response. This silently contaminates the calibration data.

**Recommendation:** Acknowledge the LLM-as-judge limitation more prominently. Report the percentage of responses where the judge's parsing failed or fell back to defaults. Consider a human annotation sample (even 50 responses) as a calibration check.

**4.2 Perturbation Validity (Paper A)**

Counterfactual perturbation uses GPT-4o-mini as the perturbation generator. This creates a dependency chain: GPT-4o-mini generates perturbations → GPT-4o-mini answers them → GPT-4o-mini judges the answers. If the perturbation generator creates subtle logical inconsistencies, the test subject might "correctly" reject the perturbed question, and this would be scored as an error.

The paper does not report the **perturbation validity rate** — what fraction of generated perturbations are logically sound? Without this, the memorization gap could partially reflect perturbation quality rather than model fragility.

**Recommendation:** Report perturbation validity. Ideally, have a domain expert validate a random sample of perturbations.

**4.3 Ability Taxonomy Weights (Paper B)**

The six-dimensional ability taxonomy and its mapping to the CFA curriculum (Table 3) are the empirical backbone of the signaling model, yet the weight assignments appear to be author judgments, not empirically derived. The R = 0.288 signaling retention ratio is entirely determined by these weights. If the weights shift even modestly (e.g., ethical reasoning from 0.10 to 0.20), R changes substantially.

**Recommendation:** Conduct a sensitivity analysis on the weight assignments. Report the range of R across plausible weight vectors. Better yet, derive weights from CFA curriculum topic area weightings published by CFA Institute.

**4.4 Synthetic Scenario Design (Paper C)**

The synthetic experiments (N=80 biases, N=141 ethics) are a valuable addition but introduce a confound: the synthetic scenarios are generated by an LLM (presumably GPT-4o-mini), then tested on GPT-4o-mini. If the scenarios reflect GPT-4o-mini's "worldview," they may be systematically easier or harder in ways that don't reflect genuine financial reasoning difficulty. The near-perfect 100% standard accuracy on synthetic ethics (vs. 85.1% on CFA-Easy) and zero recency bias on synthetic scenarios (vs. 0.500 on original) suggest the synthetic scenarios may have a different difficulty distribution.

**Recommendation:** Discuss the synthetic-original difficulty gap explicitly. Acknowledge that synthetic scenarios are not equivalent to human-authored professional exam questions.

**4.5 Confounding: Memorization vs. Understanding (Paper A)**

The Memorization Paradox (GPT-5-mini higher accuracy but larger gap) is the paper's headline finding, but there's a confounding explanation: GPT-5-mini may have stronger memorization AND stronger reasoning, but the reasoning doesn't compensate fully when memorized patterns break. The paper presents this as "memorization dependence" but an alternative interpretation is "ceiling effect" — with 91.8% standard accuracy, there's simply more room to fall.

**Recommendation:** Control for baseline accuracy when comparing memorization gaps across models. Normalize the gap as a fraction of standard accuracy.

**4.6 Single-Turn Adversarial Design (Paper C)**

The adversarial ethics testing uses single-turn attacks. Chen et al. (2025) FITD demonstrates that multi-turn "foot-in-the-door" attacks are significantly more effective. The paper acknowledges this in limitations but the implication is substantial: the 5.96% flip rate may dramatically understate true vulnerability.

---

## 5. Writing & Presentation

### Paper A
- **Strengths:** Clear three-dimensional narrative structure. Tables are informative and well-formatted. The "Illusion of Financial Competence" framing is compelling. The integrated assessment table (Table 6) effectively synthesizes all dimensions.
- **Weaknesses:** At 3,675 words, may be tight for IRFA's expectations for a multi-dimensional framework paper. The Discussion section could benefit from more engagement with opposing interpretations. Some figures (reliability diagrams) are clear but the memorization paradox figure could use a clearer annotation of the key finding.

### Paper B
- **Strengths:** The formal mathematical presentation (Propositions, Corollary, Definitions) is appropriate for FAJ. The theoretical model is elegantly structured. The writing quality is the highest of the three papers.
- **Weaknesses:** The connection between the theoretical model and the empirical option bias experiment is somewhat indirect. The paper proves that signaling collapses when AI replication cost approaches zero, then shows that format doesn't matter (option bias), and infers that "format reform can't save certification." This logical chain has a gap: option bias measures format invariance, not knowledge replication per se. The Limitations section's discussion of weight sensitivity is appropriately candid.

### Paper C
- **Strengths:** Clean experimental design. The "double jeopardy" framing (behavioral risk + ethical risk) is marketable. The debiasing hierarchy (surface/weak/deep) is a practical contribution. The rationalization taxonomy is qualitatively rich.
- **Weaknesses:** At 2,532 words, this is quite short even for FRL. The Related Work section is compressed. The cross-model section on GPT-5-mini zero flips is interesting but thin — it's presented as a strong finding but could be an artifact of GPT-5-mini's tendency to produce empty/default responses (as seen in I2 and A5). The paper acknowledges memorization vs. robustness but doesn't fully engage with this alternative.

### Cross-Paper Issues
- Claims are generally well-supported by data.
- Figures across all three papers are professional quality.
- Limitations are honestly discussed in all papers (a strength).
- The papers reference each other appropriately without excessive cross-citation.

---

## 6. Scoring

| Dimension | Score (0-100) | Weight | Justification |
|-----------|:---:|:---:|---|
| Novelty | 70 | 25% | Paper B is theoretically original but Galdin & Silbert (2025) is a close parallel. Paper A's integration is novel but components are derivative. Paper C faces major competition from Bini et al. (NBER) and TRIDENT (ACL). Key unique elements: memorization paradox, selective signaling collapse, disposition effect, dual-threat framework. |
| Experimental rigor | 65 | 25% | Strong data verification (99%+ match). Key weaknesses: LLM-as-judge circularity, small N in Paper C primary experiments, perturbation validity unreported, weight assignments unvalidated. Synthetic replications help but don't fully resolve. |
| Technical correctness | 78 | 20% | Math in Paper B is clean and well-structured. Statistical tests appropriately chosen. Data integrity corrections (A5, I2) were properly handled. One minor D6 discrepancy. Confidence fallback-to-0.5 is a technical concern. |
| Writing quality | 80 | 15% | Professional, clear, well-structured across all three papers. Paper B is strongest. Appropriate use of Elsevier template. Figures are high quality. Abstracts are comprehensive (perhaps too long for some venues). |
| Impact potential | 75 | 15% | Paper B has highest impact potential (Nobel Prize-level theoretical framework applied to a timely question). Paper A's CaR metric could influence regulatory discourse. Paper C's debiasing hierarchy is practical. CFA Institute audience for Paper B is perfectly targeted. |

| **Weighted Total** | **72.2** | **100%** |
|---|---|---|

**Calibration:** 72.2 falls in the **60–74 range (weak reject / major revision required)**. However, this is an aggregate across three papers. Individually:
- Paper B alone would score ~77 (weak accept — loses points due to Galdin & Silbert proximity)
- Paper A alone would score ~73 (borderline — integration is strong but needs must-fix items)
- Paper C alone would score ~66 (weak reject without revisions — must differentiate from Bini et al. and TRIDENT)

---

## 7. Actionable Recommendations

### Must Fix (Blocking Issues)

1. **Critical Missing Citations (All Papers):** Several major competitor papers are not cited and must be addressed before submission:
   - **Paper B** must cite **Galdin & Silbert (2025, Princeton)** — they apply the same Spence + AI replication cost framework to freelance markets with real labor market data. Not citing this would be an immediate desk reject signal to reviewers familiar with the literature. The paper must explicitly differentiate (formal certification vs. informal portfolios; multi-dimensional ability decomposition; selective collapse).
   - **Paper C** must cite **Bini et al. (NBER/NeurIPS 2025)** — the most comprehensive LLM behavioral bias study to date. Also must cite **TRIDENT (Hui et al., ACL 2025)** — directly tests CFA ethics with 2,600+ adversarial prompts across 19 models.
   - **Paper A** should add MATH-Perturb (ICML 2025) and KalshiBench (2025) to its related work.

2. **LLM-as-Judge Transparency (All Papers):** Report the fraction of judge evaluations that fell back to defaults, produced parsing errors, or evaluated empty responses. This is a known systematic bias that affects the integrity of all results. Add a subsection or appendix on evaluation methodology reliability.

3. **Perturbation Validity Rate (Paper A):** Report what fraction of GPT-4o-mini-generated perturbations are logically valid. Without this number, the memorization gap is uninterpretable — it could reflect perturbation quality, not model fragility. Even reporting the human-validated accuracy on a random sample of 50 perturbations would suffice.

4. **Weight Sensitivity Analysis (Paper B):** The R = 0.288 figure (and the entire policy recommendation) depends on author-assigned ability taxonomy weights. Provide a sensitivity analysis showing R across at least 3 alternative weight vectors (e.g., CFA Institute topic weights, equal weights, extreme scenarios). If R ranges from 0.15 to 0.45, the conclusion is robust; if it ranges from 0.10 to 0.80, the model is not empirically grounded.

### Should Fix (Significant Improvements)

5. **Normalize Memorization Gap (Paper A):** When comparing memorization gaps across models (18.6pp vs. 36.4pp), normalize by baseline accuracy. GPT-5-mini's 36.4pp gap from a 91.8% baseline is 39.6% relative decline; GPT-4o-mini's 18.6pp from 82.4% is 22.6% relative decline. The paradox holds but the magnitude changes.

6. **Discuss Synthetic-Original Difficulty Gap (Paper C):** The synthetic ethics questions show 100% standard accuracy vs. 85.1% on originals, and 0.85% flip rate vs. 5.96%. This systematic difference should be explicitly discussed. Are synthetic questions easier? Does the LLM generate questions it "knows" how to answer?

7. **Strengthen the Theory-Empirics Bridge (Paper B):** The option bias experiment shows format invariance, but the model predicts signaling collapse through ability replication. The paper would be stronger if it directly tested one formalizable ability dimension (e.g., s1 or s2) vs. one tacit dimension (e.g., s5 or s6), showing the differential that the model predicts.

8. **Address GPT-5-mini Zero Flips Skeptically (Paper C):** Given that GPT-5-mini produced 80% empty responses in I2 and 58 empty responses in A5, its "zero adversarial flips" in D6 deserves a harder look. The paper mentions memorization vs. robustness but should quantify: how many D6 responses were empty? Were there parsing anomalies? The 合成 (synthetic) data from D6 GPT-5-mini would strengthen this.

9. **Add Human Baseline (Paper C, ideally Paper A):** Even a small human comparison (e.g., 5 CFA charterholders answering 20 bias scenarios and 20 adversarial ethics questions) would dramatically strengthen the "inherited irrationality" claim. Without a human baseline, we don't know if the LLM's bias level is higher, lower, or similar to human professionals.

### Nice to Have (Minor Suggestions)

10. **Paper A word count:** At 3,675 words, consider whether IRFA expects more depth for a three-dimensional framework paper. IRFA typical papers run 6,000–8,000 words. The paper may benefit from expanding the Discussion section.

11. **Paper C abstract length:** The abstract is 269 words. FRL prefers abstracts under 200 words. Consider trimming.

12. **Cross-paper reference consistency:** Ensure that when Paper B cites Paper A's memorization gap (36.4pp for GPT-5-mini), and Paper C cites Paper B's signaling erosion, the cross-references use consistent numbers after corrections.

13. **Consider testing a third model family:** All results are from OpenAI models. One open-source model (e.g., Llama 3, Qwen3) would strengthen the claim that findings are model-general rather than OpenAI-specific. Paper A mentions Qwen3-32B for calibration but doesn't use it systematically.

---

## 8. Potential Research Directions

### Gaps Exposed by Current Work

1. **Multi-Turn Adversarial Ethics:** Paper C's single-turn design likely underestimates vulnerability (acknowledged). A multi-turn FITD-style attack on CFA ethics questions would be a natural follow-up with potential for a standalone paper. The Chen et al. (2025) methodology can be directly applied.

2. **Dynamic Calibration:** Paper A shows the "fixed confidence register" (ECE=0.073 on easy, 0.315 on hard, with stable ~85% confidence). This suggests the model lacks metacognitive awareness. Can calibration be improved through chain-of-thought self-reflection or post-hoc recalibration? This is a direct research question with practical value.

3. **Empirical Validation of Ability Taxonomy Weights:** Paper B's six-dimensional framework could be validated by mapping actual CFA exam questions to ability dimensions (using human annotators or a structured classification scheme), then measuring AI replication success per dimension. This would transform the model from theoretical to empirically grounded.

### Natural Extensions

4. **Longitudinal Signaling Erosion:** Track how R changes across model generations (GPT-4 → GPT-4o → GPT-4o-mini → GPT-5-mini → future models). Paper B's framework predicts monotonic decline; a time-series analysis would be highly citable.

5. **Debiasing Interventions at Scale:** Paper C identifies surface biases as prompt-debiasable. A systematic intervention study (prompt engineering, CoT, explicit debiasing instructions) across all six bias types with N > 200 per type would be a strong contribution to the AI safety literature.

6. **Cross-Domain Transfer:** Apply the three-dimensional framework (Paper A) to other professional exams (bar exam, medical board, actuarial) to test whether the memorization paradox and calibration failures are domain-specific or universal.

### Unexplored Angles

7. **Regulatory Impact Analysis:** Paper A proposes CaR as a regulatory metric but doesn't model the regulatory implications. A policy paper mapping CaR/Robust Accuracy to existing financial regulation (e.g., MiFID II suitability requirements, SEC fiduciary standards) could be submitted to a law/regulation journal.

8. **Training Data Archaeology:** The memorization paradox raises a question: what specific training data is GPT-5-mini memorizing? A targeted data extraction/membership inference attack could identify which CFA questions are likely in the training set, providing the first concrete evidence of financial exam contamination.

9. **Signaling Theory for Other Professions:** Paper B's framework is immediately applicable to bar exams (law), USMLE (medicine), Series 7/66 (securities). Each application could be a separate paper, and the comparison across professions would reveal how domain-specific the signaling erosion pattern is.

---

## Appendix: Verification Notes

- All raw experiment JSON files were cross-checked against paper claims.
- The `bias_scorer.py` markdown code block fix was confirmed in the codebase.
- The confidence fallback-to-0.5 behavior was identified in `experiments/shared/` infrastructure.
- GPT-5-mini empty response issues were properly addressed (A5 corrected, I2 removed).
- One minor discrepancy found: D6 GPT-5-mini per-attack accuracy values in Paper C show slight inconsistency with raw JSON. Cosmetic, not substantive.
