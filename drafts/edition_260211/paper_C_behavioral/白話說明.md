# Paper C 遺傳的非理性與倫理脆弱性：LLM 在金融決策中的行為偏誤與對抗脆弱性
# Inherited Irrationality and Ethical Fragility: Behavioral Biases and Adversarial Vulnerability in Financial LLMs

> **目標期刊**：Finance Research Letters (FRL, IF 6.9) | **字數**：~2,532 words | **樣本量**：N=60 情境 + N=80 合成情境 + N=47 倫理題 + N=141 合成倫理題
> **論文資料夾**：`paper_C_behavioral/`
> **合併自**：P3 (行為偏誤) + P4 (對抗式道德測試)

---

## 這篇在講什麼？（30 秒版）

AI 金融顧問有兩個致命弱點：

1. **它繼承了人類的非理性**——損失趨避、處置效應、過度自信。其中處置效應（太早賣贏家、太晚賣輸家）完全無法通過改提示修正。
2. **它的道德立場很脆弱**——只要施加一點壓力（利益誘惑、權威指令、情感操縱），它就會放棄倫理標準，而且還會編出看起來很有道理的「理由」。

**一句話：AI 顧問會犯人類的錯（虧錢），又守不住底線（違規），而且兩個問題要用完全不同的方法才能修。**

---

## 研究背景

### 問題一：行為偏誤

行為金融學最重要的發現是：人不是理性的。我們會因為害怕損失而不敢賣（loss aversion），因為不甘心認賠而抱著爛股票（disposition effect），因為看到一個數字就被錨定（anchoring）。

LLM 的訓練數據全部來自人類寫的文字——分析報告、新聞、教科書。如果這些文字裡就充斥著非理性的決策模式，模型會不會「學」到這些偏誤？

### 問題二：倫理脆弱性

CFA 倫理守則要求金融專業人士在面對壓力時堅守原則——不能因為客戶施壓就推薦不適合的產品，不能因為老闆要求就做內線交易。

但如果有人刻意施壓 AI，它能守住底線嗎？

### 為什麼合併？

這兩個問題看起來不相關，但其實是**同一枚硬幣的兩面**：

| 失敗模式 | 後果 | 風險類型 |
|---------|------|---------|
| 行為偏誤 | 投資建議不合理 → **虧錢** | 投資風險 |
| 倫理脆弱 | 放棄合規標準 → **違規** | 法律與監管風險 |

一個金融機構如果部署了**既不理性又不堅定**的 AI，就是面臨雙重風險（Double Jeopardy）。

---

## 核心方法

### 研究一：行為偏誤測量

**配對情境設計**：每種偏誤設計 10 道金融決策情境，每道題有兩個版本：

- **偏誤誘導版**：包含會觸發偏誤的元素
- **中性版**：只呈現客觀數據

如果模型是理性的，兩個版本的建議應該一致。

#### 用一個例子理解：處置效應

> 你管理一個投資組合：
> - 股票 A：買入 $100，現在 **$130**（賺了 30%）
> - 股票 B：買入 $100，現在 **$75**（虧了 25%）
>
> 你需要賣一檔來獲取資金。兩者未來預期報酬率相同。你賣哪個？

**理性答案**：兩者預期報酬一樣，賣哪個都可以（或考慮稅務因素賣 B）。
**偏誤答案**：賣 A（落袋為安），留 B（不甘心認賠）。

如果 AI 系統性地選擇賣 A → 它就表現出了人類的處置效應。

#### 用一個例子理解：損失趨避

> 客戶持有一檔基金，目前**虧損 15%**。分析顯示有 60% 機率回升，40% 機率再跌 10%。

**偏誤版**用「虧損 15%」的框架 → 觸發「不甘心認賠」
**中性版**只呈現期望值（+5%）→ 理性評估

### 研究二：對抗式倫理測試

取 47 道 CFA 倫理題，對每題創建 5 種壓力版本：

| 壓力類型 | 說明 | 對應 CFA 守則 |
|---------|------|-------------|
| 利益誘惑 | 「這樣做能多賺 200 萬」 | III(C) 適合性 |
| 權威壓力 | 「你的主管要求你這樣做」 | I(B) 獨立客觀 |
| 情感操縱 | 「這位客戶即將退休，非常需要」 | III(A) 忠誠與謹慎 |
| 語言重構 | 把違規行為包裝成合理做法 | I(A) 法律知識 |
| 道德困境 | 「雖然違規，但能幫助更多人」 | 整體倫理判斷 |

---

## 核心結果

### 研究一：行為偏誤

**三層去偏階層**（核心發現）：

| 層級 | 偏誤 | 偏誤分數 | 中性分數 | 去偏效果 |
|------|------|---------|---------|---------|
| **表面** | 損失趨避 | 0.500 | 0.200 | **+0.300** |
| **表面** | 框架效應 | 0.550 | 0.400 | **+0.150** |
| **弱反應** | 錨定 | 0.500 | 0.450 | +0.050 |
| **深層** | 處置效應 | 0.500 | 0.500 | **0.000** |
| **深層** | 過度自信 | 0.500 | 0.500 | **0.000** |
| **深層** | 近因偏誤 | 0.450 | 0.500 | -0.050 |

**表面偏誤**（損失趨避、框架效應）：改掉提示裡的情緒用語就能去偏。說明偏誤來自「語言的統計模式」——模型看到「虧損」就自動偏向保守。

**深層偏誤**（處置效應、過度自信、近因偏誤）：**完全無法通過改提示去偏**。不管怎麼問，模型都有 50% 機率表現出偏誤行為。這些偏誤深植於訓練數據的結構中。

合成情境複製（N=80）確認：處置效應是最頑固的偏誤（0.808）。

### 研究二：對抗式倫理

| 壓力條件 | 準確率 | 翻轉數 | ERS |
|---------|--------|--------|-----|
| 標準（無壓力） | 85.1% | --- | 1.000 |
| 利益誘惑 | 78.7% | 4 | 0.925 |
| 權威壓力 | 78.7% | 3 | 0.925 |
| 情感操縱 | 80.9% | 2 | 0.950 |
| 語言重構 | 80.9% | 3 | 0.950 |
| 道德困境 | 80.9% | 2 | 0.950 |

**所有五種壓力都能讓 AI 放棄倫理立場**。共 14 題「翻轉」——原本答對、施壓後答錯。

#### 合理化策略分類

更可怕的是，AI 不是「亂答」，而是會**編出看起來很有道理的理由**：

1. **功利主義凌駕**（6 次）：「雖然違規，但這樣做對客戶更好」
2. **服從權威**（3 次）：「主管有更多經驗，應該聽他的」
3. **語義重新包裝**（3 次）：把違規行為說成「務實解讀」

這比單純答錯更危險——因為它會產生**看起來合理的合規違規理由**。

#### 合成倫理實驗（N=141）

在合成題目上，翻轉率從 5.96% 大幅降低到 0.85%。只有「語言重構」和「道德困境」仍然有效。這說明：利益誘惑和權威壓力主要利用模型對背誦題目的邊際信心，而不是真正的推理漏洞。

#### GPT-5-mini：零翻轉

GPT-5-mini 在所有五種壓力下**零翻轉**，壓力甚至反而提升了準確率。但這可能是因為它「背得更好」（見 Paper A 的記憶化悖論），而非真的具備更強的道德推理能力。

---

## 一句話總結

> **AI 金融顧問面臨「雙重風險」：行為偏誤導致不理性的投資建議（處置效應完全抗拒去偏），倫理脆弱性使其在壓力下放棄合規標準（5 種攻擊全部有效）。表面偏誤可以靠改提示修正，但深層偏誤和倫理脆弱性需要訓練層級的介入。GPT-5-mini 的零翻轉結果提供謹慎樂觀，但驗證不可省略。**

---

## 實務意義

**對 Robo-Advisor 開發者**：
- 處置效應會讓你的系統系統性地「太早賣贏家」→ 在百萬客戶的規模下，即使每次差幾個基點，累積起來就是巨大的財富毀損
- 必須在交易邏輯層設置去偏機制，不能只靠提示工程

**對合規部門**：
- AI 在壓力下產生的不是「明顯的錯誤」，而是「聽起來合理的合規違規理由」
- 需要專門的 adversarial testing 作為 AI 部署前的強制測試

**對金融監管機構**：
- 現有的 AI 監管框架聚焦在「準確性」——但行為偏誤和倫理脆弱性完全在準確率的雷達之外
- 需要新的測試維度：偏誤測試 + 對抗式倫理測試

---

## 附錄

### 合併邏輯

P3 和 P4 看似不同（行為偏誤 vs 倫理壓力），但回答同一個核心問題：**AI 作為金融決策者的可靠性**。

- P3 問「它的判斷是理性的嗎？」→ 不是，它有系統性偏誤
- P4 問「它的原則是堅定的嗎？」→ 不是，壓力一來就動搖

合併後的「雙重風險」框架比單獨看任何一面都更有衝擊力。

### 實驗數據路徑

| 資料 | 路徑 |
|------|------|
| I2 行為偏誤 (N=60) | `experiments/I2_behavioral_biases/results/run_20260206_140527/results.json` |
| I2 合成偏誤 (N=80) | `experiments/I2_behavioral_biases/results/synthetic_gpt-4o-mini_20260210_085431/` |
| D6 對抗式倫理 (N=47) | `experiments/D6_adversarial_ethics/results/run_20260206_190419/results.json` |
| D6 合成倫理 (N=141) | `experiments/D6_adversarial_ethics/results/synthetic_gpt-4o-mini_20260210_091207/` |
| D6 GPT-5-mini | `experiments/D6_adversarial_ethics/results/run_20260207_213723/results.json` |
