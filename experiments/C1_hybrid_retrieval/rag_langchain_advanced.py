"""
RAG LangChain Advanced - å¼ºåŒ–ç‰ˆæ£€ç´¢ï¼ˆrewrite + subquery + hybrid + rerankï¼‰
é€‚é… thelma2 æ•°æ®æ ¼å¼
"""
import os
import json
from tqdm import tqdm
from typing import List, Dict, Any, Sequence

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from pydantic import ConfigDict
import numpy as np

from data_loader import load_thelma2_dataset

print("--- ğŸš€ RAG LangChain Advanced (Rewrite + Subquery + Hybrid + Rerank) ---")

# =========================
# é…ç½®
# =========================
OUTPUT_FILE = "./rag_langchain_advanced_results.json"
DB_PATH = "./rag_langchain_advanced_milvus.db"
COLLECTION_NAME = "rag_langchain_advanced_collection"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®š OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=OPENAI_API_KEY)

print(f"LLM: gpt-4o-mini")
print(f"Embedding: text-embedding-3-large\n")


# =========================
# å·¥å…·ï¼šQuery Rewrite
# =========================
rewrite_prompt = ChatPromptTemplate.from_template(
    "å°†ä»¥ä¸‹é—®é¢˜æ”¹å†™ä¸ºæ›´æ¸…æ™°çš„æœç´¢å‹å¥½æŸ¥è¯¢ï¼š\n{question}"
)
query_rewriter = (
    {"question": RunnablePassthrough()}
    | rewrite_prompt
    | llm
    | StrOutputParser()
)

# =========================
# å·¥å…·ï¼šSub-query decomposition
# =========================
subq_prompt = PromptTemplate.from_template(
    "å°†ä»¥ä¸‹é—®é¢˜åˆ†è§£ä¸º 1-3 ä¸ªå­é—®é¢˜ï¼š\n\n{question}"
)
subq_chain = (
    {"question": RunnablePassthrough()}
    | subq_prompt
    | llm
    | StrOutputParser()
)


# =========================
# å·¥å…·ï¼šReranker
# =========================
class OpenAIReranker(BaseDocumentCompressor):
    embed: OpenAIEmbeddings
    top_n: int = 8
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks=None
    ) -> Sequence[Document]:
        if not documents:
            return []
        
        doc_texts = [doc.page_content for doc in documents]
        doc_vecs = self.embed.embed_documents(doc_texts)
        q_vec = self.embed.embed_query(query)
        
        q = np.asarray(q_vec, dtype=np.float32)
        q_norm = np.linalg.norm(q) or 1.0
        
        scored = []
        for v, doc in zip(doc_vecs, documents):
            dv = np.asarray(v, dtype=np.float32)
            denom = (np.linalg.norm(dv) * q_norm) or 1.0
            score = float(np.dot(dv, q) / denom)
            md = dict(doc.metadata or {})
            md["rerank_score"] = score
            doc.metadata = md
            scored.append((score, doc))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored[:self.top_n]]


reranker = OpenAIReranker(embed=embeddings, top_n=8)


# =========================
# é«˜çº§æ£€ç´¢å‡½æ•°
# =========================
def advanced_retrieve(question: str, ensemble: EnsembleRetriever, reranker: OpenAIReranker) -> List[Document]:
    """æ‰§è¡Œé«˜çº§æ£€ç´¢æµç¨‹ï¼šrewrite -> subquery -> hybrid -> rerank"""
    # Step 1: rewrite
    rewritten = query_rewriter.invoke({"question": question}).strip()
    
    # Step 2: sub-queriesï¼ˆæœ€å¤š 2 æ¡ï¼‰
    sub_queries_str = subq_chain.invoke({"question": rewritten})
    sub_queries = [q.strip() for q in sub_queries_str.split("\n") if q.strip()]
    sub_queries = sub_queries[:2] or [rewritten]
    
    # Step 3: hybrid æ£€ç´¢
    lists = ensemble.batch(sub_queries)
    pool = [doc for lst in lists for doc in lst]
    
    # å»é‡ï¼ˆä»¥ doc_idï¼‰
    uniq = {}
    for d in pool:
        did = d.metadata.get("doc_id")
        if did and did not in uniq:
            uniq[did] = d
    pooled_docs = list(uniq.values())
    
    # Step 4: rerank
    final_docs = reranker.compress_documents(pooled_docs, rewritten)
    return final_docs


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    # 1) åŠ è½½æ•°æ®
    questions, docs = load_thelma2_dataset()
    
    # 2) æ–‡æœ¬åˆ‡ç‰‡
    print("ğŸ”§ å‡†å¤‡çŸ¥è¯†åº“...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(docs)
    print(f"   - åŸå§‹æ–‡æ¡£: {len(docs)}, åˆ‡ç‰‡å: {len(chunks)}")
    
    # 3) å»ºç«‹ BM25 æ£€ç´¢å™¨
    bm25_retriever = BM25Retriever.from_documents(chunks)
    bm25_retriever.k = 20
    
    # 4) å»ºç«‹å‘é‡æ£€ç´¢å™¨
    vectorstore = Milvus(
        embedding_function=embeddings,
        connection_args={"uri": DB_PATH},
        collection_name=COLLECTION_NAME,
        drop_old=True,
        auto_id=True,
    )
    vectorstore.add_documents(chunks)
    milvus_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})
    
    # 5) ç»„åˆ Ensemble Retriever
    ensemble = EnsembleRetriever(
        retrievers=[bm25_retriever, milvus_retriever],
        weights=[0.4, 0.6]  # åå‘å‘é‡æ£€ç´¢
    )
    
    print("âœ… æ£€ç´¢å™¨å‡†å¤‡å®Œæˆ\n")
    
    # 6) æ‰§è¡Œæ£€ç´¢
    print("--- å¼€å§‹å¤„ç†é—®é¢˜ ---")
    results_map: Dict[str, List[Dict[str, Any]]] = {}
    
    for qa in tqdm(questions, desc="å¤„ç†è¿›åº¦"):
        qid = qa.get("question_id")
        q = qa.get("question")
        if not qid or not q:
            continue
        
        try:
            retrieved_docs = advanced_retrieve(q, ensemble, reranker)
            bundle = []
            for doc in retrieved_docs:
                bundle.append({
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                })
            results_map[qid] = bundle
        except Exception as e:
            print(f"âš ï¸ é—®é¢˜ {qid} å‘ç”Ÿé”™è¯¯ï¼š{e}")
            results_map[qid] = []
    
    # 7) è¾“å‡ºç»“æœ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results_map, f, ensure_ascii=False, indent=4)
    
    print(f"\nğŸ‰ å®Œæˆï¼æ£€ç´¢ç»“æœå·²è¾“å‡ºè‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

