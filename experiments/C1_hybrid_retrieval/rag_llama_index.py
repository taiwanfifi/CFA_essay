"""
RAG LlamaIndex - æ ‡å‡†ç‰ˆæœ¬
é€‚é… thelma2 æ•°æ®æ ¼å¼
"""
import os
import json
from tqdm import tqdm
from typing import List, Dict, Any

from dotenv import load_dotenv
load_dotenv()

from llama_index.core import (
    VectorStoreIndex,
    Settings,
    Document,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

from data_loader import load_thelma2_dataset

print("--- ğŸš€ RAG LlamaIndex (Standard Version) ---")

# =========================
# é…ç½®
# =========================
OUTPUT_FILE = "./rag_llama_index_results.json"
KB_PERSIST_DIR = "./rag_llama_index_storage"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")

# è®¾ç½® LlamaIndex
Settings.llm = OpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY, request_timeout=120.0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-large", api_key=OPENAI_API_KEY)

print("âœ… LlamaIndex è®¾ç½®å®Œæˆ")
print(f"   - LLM: gpt-4o-mini")
print(f"   - Embedding: text-embedding-3-large\n")


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    # 1) åŠ è½½æ•°æ®
    questions, docs = load_thelma2_dataset()
    
    # 2) è½¬æ¢ä¸º LlamaIndex Document æ ¼å¼
    llama_docs = [
        Document(text=doc.page_content, metadata=doc.metadata)
        for doc in docs
    ]
    
    # 3) å»ºç«‹æˆ–åŠ è½½ç´¢å¼•
    if not os.path.exists(KB_PERSIST_DIR):
        print("ğŸ“‚ å»ºç«‹æ–°ç´¢å¼•...")
        print(f"   - å…± {len(llama_docs)} ä»½æ–‡ä»¶")
        index = VectorStoreIndex.from_documents(llama_docs, show_progress=True)
        os.makedirs(KB_PERSIST_DIR, exist_ok=True)
        index.storage_context.persist(persist_dir=KB_PERSIST_DIR)
        print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆ\n")
    else:
        print("âœ… ä»æ—¢æœ‰ç´¢å¼•è½½å…¥")
        storage_context = StorageContext.from_defaults(persist_dir=KB_PERSIST_DIR)
        index = load_index_from_storage(storage_context)
        print()
    
    # 4) å»ºç«‹æŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(similarity_top_k=5)
    
    # 5) æ‰§è¡Œæ£€ç´¢
    print("--- å¼€å§‹å¤„ç†é—®é¢˜ ---")
    results_map: Dict[str, List[Dict[str, Any]]] = {}
    
    for qa_pair in tqdm(questions, desc="å¤„ç†è¿›åº¦"):
        q_id = qa_pair.get("question_id")
        question = qa_pair.get("question")
        if not q_id or not question:
            continue
        
        try:
            response = query_engine.query(question)
            retrieved_nodes = response.source_nodes or []
            
            bundle: List[Dict[str, Any]] = []
            for sn in retrieved_nodes:
                node = sn.node
                did = node.metadata.get("doc_id")
                if did:
                    bundle.append({
                        "page_content": node.get_content(),
                        "metadata": {"doc_id": did},
                    })
            results_map[q_id] = bundle
        
        except Exception as e:
            results_map[q_id] = []
            print(f"\nâš ï¸ é—®é¢˜ {q_id} å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    # 6) è¾“å‡ºç»“æœ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results_map, f, ensure_ascii=False, indent=4)
    
    print(f"\nğŸ‰ å®Œæˆï¼æ£€ç´¢ç»“æœå·²è¾“å‡ºè‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

