# run_agent_v43_pragmatist_evaluation.py  (OpenAI ç‰ˆæœ¬)
# ------------------------------------------------------------
# V4.3 Pragmatistï¼ˆOpenAI + Milvusï¼‰é›¢ç·šæª¢ç´¢é è·‘å™¨
# - è®€ ultimate_rag_challenge_questions.json çš„ gold evidence
# - å»º Milvus(Lite) å‘é‡åº«ï¼ˆtext-embedding-3-large, 3072dï¼‰
# - åŸ·è¡Œï¼šè¦åŠƒ â†’ æŸ¥è©¢é‡å¯« â†’ æª¢ç´¢ â†’ æŠ½å– â†’ å……åˆ†æ€§è©•ä¼° â†’ æ‘˜è¦
# - å°æ¯é¡Œè¼¸å‡ºå‰ TOP_K çš„ docï¼ˆpage_content + {"doc_id": ...}ï¼‰
#
# è¼¸å‡ºï¼š./agent_v43_retrieval_results.json
# ------------------------------------------------------------

import os
import json
import itertools
from typing import List, Dict, Any, Set, TypedDict

from dotenv import load_dotenv
load_dotenv()

from tqdm import tqdm

# LangChain / LangGraph
from langchain_core.documents import Document
# è‹¥æƒ³å†åˆ‡ç‰‡å¯é–‹å•Ÿ
# from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_milvus import Milvus

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END


# =========================
# 0) åŸºæœ¬è¨­å®š
# =========================
DATA_FILE = "./data/ultimate_rag_challenge_questions.json"
RESULTS_FILE = "./agent_v43_retrieval_results.json"

# å‘é‡åº«ï¼ˆMilvus Lite æœ¬åœ°æª”æ¡ˆ; ç”¨æ–°åç¨±é¿å…ç¶­åº¦è¡çªï¼‰
COLLECTION_NAME = "agent_v43_collection_openai_te3l"
DB_URI = "./agent_v43_milvus_openai.db"

# æ¨¡å‹ï¼ˆOpenAIï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")

LLM_MODEL = os.getenv("OPENAI_LLM", "gpt-4o-mini")             # çµ±ä¸€ç”¨ gpt-4o-mini
EMBED_MODEL = os.getenv("OPENAI_EMBEDDING", "text-embedding-3-large")  # large v

# RETRIEVER_K = 6         # æ¯æ¬¡æª¢ç´¢ top-k
# AGENT_MAX_TURNS = 8     # æœ€å¤§å¾ªç’°å›åˆ
# TOP_K_RETURN = 5        # æ¯é¡Œè¼¸å‡ºå‰ K ä»½è­‰æ“š

RETRIEVER_K = 8       # â† 3 -> 8
AGENT_MAX_TURNS = 8
TOP_K_RETURN = 5

MIN_TURNS = 3         # â† è‡³å°‘è·‘ 3 è¼ª
MIN_UNIQUE_DOCS = 2   # â† è‡³å°‘å‘½ä¸­ 2 å€‹ä¸åŒ doc_id æ‰èƒ½åœ



print("--- Summary Builder Agent (V4.3 - Pragmatist / OpenAI) : Offline Retrieval Runner ---")
print(f"LLM(JSON/Text): {LLM_MODEL} (OpenAI)")
print(f"Embedding: {EMBED_MODEL}")
print(f"Milvus DB: {DB_URI} / {COLLECTION_NAME}\n")


# =========================
# 1) è¼‰å…¥è³‡æ–™é›†ï¼Œæ”¶é›† gold evidence
# =========================
def load_eval_evidence() -> List[Document]:
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        eval_data = json.load(f)
    unique: Dict[str, str] = {}
    for item in eval_data:
        for ev in item.get("gold_evidence", []):
            did = ev.get("doc_id")
            txt = ev.get("text_snippet")
            if did and isinstance(txt, str):
                unique[did] = txt
    docs = [Document(page_content=txt, metadata={"doc_id": did}) for did, txt in unique.items()]
    return docs

def load_questions() -> List[Dict[str, Any]]:
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


# =========================
# 2) å»ºå‘é‡åº«ï¼ˆMilvus Liteï¼‰
# =========================
def ensure_vectorstore(docs: List[Document]) -> Milvus:
    # è‹¥è¦åˆ‡ç‰‡å¯é–‹å•Ÿ
    # splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    # docs = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(model=EMBED_MODEL, api_key=OPENAI_API_KEY)

    # çµ±ä¸€é‡å»ºï¼Œé¿å…èˆŠ collection ç¶­åº¦ä¸ç¬¦
    print("ğŸ”§ æº–å‚™ Milvus(Lite) collection ...")
    vs = Milvus(
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
        connection_args={"uri": DB_URI},
        drop_old=True,          # ç¢ºä¿ç”¨æ–° embedding ç¶­åº¦é‡å»º
        auto_id=True,
    )
    print("   - å¯«å…¥ gold evidence ...")
    vs.add_documents(docs)
    print("   - âœ… å‘é‡åº«å»ºç«‹å®Œæˆ")
    return vs


# =========================
# 3) V4.3 Agentï¼ˆç²¾ç°¡ä½†å¯æª¢ç´¢ï¼‰
# =========================
class GraphState(TypedDict):
    original_question: str
    question: str
    question_history: List[str]
    rewritten_queries: List[str]
    hypotheses: List[str]
    current_hypothesis: str
    assessment: str
    cumulative_facts: List[str]
    final_summary: str
    newly_retrieved_docs: List[Document]
    cumulative_retrieved_docs: List[Document]
    all_retrieved_doc_ids: Set[str]
    has_new_info: bool
    turn_count: int
    plan: List[str]

# Pydantic models
class SimplePlan(BaseModel):
    plan: List[str] = Field(description="2-3 concrete sub-steps.")

class SimpleHypotheses(BaseModel):
    hypotheses: List[str] = Field(description="2-3 distinct hypotheses.")

class GradedAnswer(BaseModel):
    is_sufficient: str = Field(description="yes or no")

class ExtractedFacts(BaseModel):
    facts: List[str] = Field(description="key facts")

class RewrittenQueries(BaseModel):
    rewritten_queries: List[str] = Field(description="1-3 search queries")

# LLMsï¼ˆå…©å€‹éƒ½ç”¨ gpt-4o-miniï¼›ä¸€å€‹èµ° JSON è¼¸å‡ºä»»å‹™ï¼Œä¸€å€‹è‡ªç”±ç”Ÿæˆï¼‰
llm_json = ChatOpenAI(model=LLM_MODEL, temperature=0, api_key=OPENAI_API_KEY)
llm_text = ChatOpenAI(model=LLM_MODEL, temperature=0, api_key=OPENAI_API_KEY)

# é€™è£¡çš„ retriever æœƒåœ¨ main ä¸­æ³¨å…¥
RETRIEVER = None  # type: ignore

def hypothesize_node(state: GraphState):
    parser = JsonOutputParser(pydantic_object=SimpleHypotheses)
    prompt = PromptTemplate.from_template(
        "Analyze the user's question and produce 2-3 distinct hypotheses.\n"
        "Question: {original_question}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"original_question": state["original_question"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)  # ChatOpenAI -> AIMessage
        out = SimpleHypotheses.model_validate(data)
        state["hypotheses"] = out.hypotheses
        state["current_hypothesis"] = out.hypotheses[0]
    except Exception:
        state["hypotheses"] = [state["original_question"]]
        state["current_hypothesis"] = state["original_question"]
    return state

def plan_node(state: GraphState):
    parser = JsonOutputParser(pydantic_object=SimplePlan)
    prompt = PromptTemplate.from_template(
        "Create 2-3 concrete sub-steps to answer the question.\n"
        "Hypothesis: {current_hypothesis}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"current_hypothesis": state["current_hypothesis"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = SimplePlan.model_validate(data)
        state["plan"] = out.plan
    except Exception:
        state["plan"] = [state["current_hypothesis"]]
    return state

def execute_plan_node(state: GraphState):
    if not state["plan"]:
        state["question"] = ""
        return state
    nxt = state["plan"].pop(0)
    state["question"] = nxt
    return state

def rewrite_query_node(state: GraphState):
    if not state["question"]:
        state["rewritten_queries"] = []
        return state
    parser = JsonOutputParser(pydantic_object=RewrittenQueries)
    prompt = PromptTemplate.from_template(
        "Rewrite the following into 1-3 diverse retrieval queries.\n"
        "Original: {q}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"q": state["question"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = RewrittenQueries.model_validate(data)
        qs = [s for s in out.rewritten_queries if isinstance(s, str) and s.strip()]
        state["rewritten_queries"] = qs or [state["question"]]
    except Exception:
        state["rewritten_queries"] = [state["question"]]
    return state

def retrieve_node(state: GraphState):
    state["turn_count"] += 1
    queries = state.get("rewritten_queries", [])
    if not queries:
        state["newly_retrieved_docs"] = []
        state["has_new_info"] = False
        return state

    # æª¢ç´¢
    batch_lists = RETRIEVER.batch(queries)
    merged = list(itertools.chain.from_iterable(batch_lists))

    # ä»¥ doc_id å»é‡ã€è¨˜éŒ„æ–°å–å›çš„èˆ‡ç¸½ç´¯ç©
    seen = state["all_retrieved_doc_ids"]
    truly_new = []
    for d in merged:
        did = d.metadata.get("doc_id")
        if did and did not in seen:
            truly_new.append(d)
            seen.add(did)

    state["newly_retrieved_docs"] = truly_new
    state["cumulative_retrieved_docs"].extend(truly_new)
    state["has_new_info"] = len(truly_new) > 0
    return state

def extract_facts_node(state: GraphState):
    if not state["newly_retrieved_docs"]:
        return state
    parser = JsonOutputParser(pydantic_object=ExtractedFacts)
    prompt = PromptTemplate.from_template(
        "Extract salient facts that help answer the original question.\n"
        "Original Question: {oq}\n"
        "Sub-question: {sq}\n"
        "Documents:\n{docs}\n"
        "{format_instructions}"
    )
    docs_txt = "\n\n".join(
        f"[doc_id={d.metadata.get('doc_id')}] {d.page_content}" for d in state["newly_retrieved_docs"]
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"oq": state["original_question"], "sq": state["question"], "docs": docs_txt})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = ExtractedFacts.model_validate(data)
        if out.facts:
            state["cumulative_facts"].extend(out.facts)
            state["cumulative_facts"] = list(dict.fromkeys(state["cumulative_facts"]))  # å»é‡
    except Exception:
        pass
    return state

# def grade_facts_node(state: GraphState):
#     decision = "yes" if state["cumulative_facts"] else "no"
#     if state["turn_count"] >= AGENT_MAX_TURNS or not state.get("has_new_info", True):
#         decision = "yes"
#     state["assessment"] = decision
#     return state

# def grade_facts_node(state: GraphState):
#     uniq_docs = {
#         d.metadata.get("doc_id")
#         for d in state.get("cumulative_retrieved_docs", [])
#         if d.metadata.get("doc_id")
#     }
#     enough_docs = len(uniq_docs) >= MIN_UNIQUE_DOCS
#     enough_turns = state["turn_count"] >= MIN_TURNS
#     hit_max = state["turn_count"] >= AGENT_MAX_TURNS
#     no_new_info = not state.get("has_new_info", True)

#     state["assessment"] = "yes" if ((enough_docs and enough_turns) or hit_max or no_new_info) else "no"
#     return state

def grade_facts_node(state: GraphState):
    distinct_doc_ids = {d.metadata.get("doc_id") for d in state.get("cumulative_retrieved_docs", []) if d.metadata.get("doc_id")}
    has_two_docs = len(distinct_doc_ids) >= 2
    # è‹¥è‡³å°‘è·‘äº† 2 å›åˆå†è©•ä¼°åœæ­¢ï¼Œé¿å…ç¬¬ä¸€è¼ªå°±æ”¶æ‰‹
    min_turns_done = state.get("turn_count", 0) >= 2
    stop_now = (has_two_docs and min_turns_done) or (state.get("turn_count", 0) >= AGENT_MAX_TURNS) or (not state.get("has_new_info", True))
    state["assessment"] = "yes" if stop_now else "no"
    return state



def decide_to_continue(state: GraphState):
    return "generate_summary" if state.get("assessment") == "yes" else "continue"

def generate_summary_node(state: GraphState):
    if not state["cumulative_facts"]:
        state["final_summary"] = "No sufficient facts were found."
        return state
    prompt = PromptTemplate.from_template(
        "Write a concise answer using only these facts.\n"
        "Question: {q}\nFacts:\n- {facts}\n\nAnswer:"
    )
    chain = prompt | llm_text | StrOutputParser()
    facts_str = "\n- ".join(state["cumulative_facts"])
    state["final_summary"] = chain.invoke({"q": state["original_question"], "facts": facts_str})
    return state

# åœ–
workflow = StateGraph(GraphState)
workflow.add_node("hypothesize", hypothesize_node)
workflow.add_node("plan", plan_node)
workflow.add_node("execute_plan", execute_plan_node)
workflow.add_node("rewrite_query", rewrite_query_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("extract_facts", extract_facts_node)
workflow.add_node("grade_facts", grade_facts_node)
workflow.add_node("generate_summary", generate_summary_node)

workflow.set_entry_point("hypothesize")
workflow.add_edge("hypothesize", "plan")
workflow.add_edge("plan", "execute_plan")
workflow.add_edge("execute_plan", "rewrite_query")
workflow.add_edge("rewrite_query", "retrieve")
workflow.add_edge("retrieve", "extract_facts")
workflow.add_edge("extract_facts", "grade_facts")
workflow.add_conditional_edges("grade_facts", decide_to_continue, {
    "generate_summary": "generate_summary",
    "continue": "plan"
})
workflow.add_edge("generate_summary", END)
agent = workflow.compile()


# =========================
# 4) åŸ·è¡Œæ¯é¡Œï¼Œè¼¸å‡ºæª¢ç´¢çµæœ JSON
# =========================
def run_agent_once(question: str) -> Dict[str, Any]:
    """å›å‚³ final_state èˆ‡ top-k æª¢ç´¢æ–‡ä»¶ï¼ˆdoc_id + page_contentï¼‰"""
    init = {
        "original_question": question,
        "question": "",
        "question_history": [],
        "rewritten_queries": [],
        "hypotheses": [],
        "current_hypothesis": "",
        "assessment": "",
        "cumulative_facts": [],
        "final_summary": "",
        "newly_retrieved_docs": [],
        "cumulative_retrieved_docs": [],
        "all_retrieved_doc_ids": set(),
        "has_new_info": True,
        "turn_count": 0,
        "plan": [],
    }
    final_state = agent.invoke(init, {"recursion_limit": 50})
    # ä¾ã€Œé¦–æ¬¡è¢«æª¢ç´¢åˆ°ã€çš„é †åºå–å‰ K
    uniq: Dict[str, Document] = {}
    for d in final_state.get("cumulative_retrieved_docs", []):
        did = d.metadata.get("doc_id")
        if did and did not in uniq:
            uniq[did] = d
    top_docs = list(uniq.values())[:TOP_K_RETURN]
    out_bundle = [
        {"page_content": d.page_content, "metadata": {"doc_id": d.metadata.get("doc_id")}}
        for d in top_docs
    ]
    return {"state": final_state, "docs": out_bundle}


def main():
    # 1) å»ºçŸ¥è­˜åº« & æª¢ç´¢å™¨
    gold_docs = load_eval_evidence()
    vectorstore = ensure_vectorstore(gold_docs)
    global RETRIEVER
    RETRIEVER = vectorstore.as_retriever(search_kwargs={"k": RETRIEVER_K})
    print("âœ… Retriever ready.\n")

    # 2) é€é¡ŒåŸ·è¡Œ
    eval_data = load_questions()
    results_map: Dict[str, List[Dict[str, Any]]] = {}

    print("--- é–‹å§‹é›¢ç·šæª¢ç´¢ (V4.3 Pragmatist / OpenAI) ---")
    for item in tqdm(eval_data, desc="Evaluating"):
        qid = item.get("question_id")
        q = item.get("question")
        if not qid or not q:
            continue
        try:
            r = run_agent_once(q)
            results_map[qid] = r["docs"]
        except Exception as e:
            results_map[qid] = []
            print(f"\nâš ï¸ å•é¡Œ {qid} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    # 3) è¼¸å‡º
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        json.dump(results_map, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å®Œæˆï¼æª¢ç´¢çµæœå·²è¼¸å‡ºåˆ° {RESULTS_FILE}")
    print("æ¥è‘—åˆ° rag_systems_to_test.py ç”¨ JsonFileRetriever è¼‰å…¥ï¼Œæ–¼ evaluate.py ä¸¦åˆ—è©•æ¸¬ã€‚")


if __name__ == "__main__":
    main()
