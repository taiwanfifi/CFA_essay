# run_llama_index_vector_only.py
# å…¬å¹³æ¯”è¼ƒç‰ˆï¼šåªè·‘ç´” Vector æª¢ç´¢ï¼Œä¸å•Ÿç”¨ rewrite/sub-query/hybrid
import os
import json
from tqdm import tqdm
from typing import List, Dict, Any

from dotenv import load_dotenv
from llama_index.core import (
    Document,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

print("--- ğŸš€ LlamaIndex ç´” Vector æª¢ç´¢ è©•ä¼°å™¨ ---")

# =========================
# åŸºæœ¬è¨­å®š
# =========================
DATA_FILE_PATH = "./data/ultimate_rag_challenge_questions.json"
OUTPUT_FILE_PATH = "./llama_index_vector_only_results.json"
KB_PERSIST_DIR = "./llama_index_vector_only_storage"

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")

# ç”¨ OpenAI GPT-4o-mini ç•¶ dummy LLMï¼ˆä¸æœƒçœŸçš„ç”¨åˆ°ï¼‰
Settings.llm = OpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)

# åµŒå…¥ = text-embedding-3-large (3072 ç¶­)
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-large", api_key=OPENAI_API_KEY
)

print("âœ… LlamaIndex è¨­å®šå®Œæˆï¼šVector-only mode")
print("   - Embedding: text-embedding-3-large")
print("   - LLM: gpt-4o-mini (not used in query pipeline)\n")

# =========================
# å»ºç«‹çŸ¥è­˜åº«
# =========================
if not os.path.exists(KB_PERSIST_DIR):
    print("--- å»ºç«‹æ–°ç´¢å¼• ---")
    with open(DATA_FILE_PATH, "r", encoding="utf-8") as f:
        eval_data = json.load(f)

    unique_evidence: Dict[str, str] = {
        ev["doc_id"]: ev["text_snippet"]
        for item in eval_data
        for ev in item.get("gold_evidence", [])
        if ev.get("doc_id") and isinstance(ev.get("text_snippet"), str)
    }

    docs = [Document(text=txt, metadata={"doc_id": did}) for did, txt in unique_evidence.items()]
    print(f"   - å…± {len(docs)} ä»½æ–‡ä»¶åŠ å…¥ç´¢å¼•")

    index = VectorStoreIndex.from_documents(docs, show_progress=True)
    index.storage_context.persist(persist_dir=KB_PERSIST_DIR)
else:
    print("--- è¼‰å…¥æ—¢æœ‰ç´¢å¼• ---")
    storage_context = StorageContext.from_defaults(persist_dir=KB_PERSIST_DIR)
    index = load_index_from_storage(storage_context)

# å»ºç«‹ Retrieverï¼ˆç´” vector search, top_k=5ï¼‰
retriever = index.as_retriever(similarity_top_k=5)

# =========================
# åŸ·è¡Œæª¢ç´¢
# =========================
print("\n--- é–‹å§‹è™•ç†è©•ä¼°å•é¡Œ ---")
with open(DATA_FILE_PATH, "r", encoding="utf-8") as f:
    eval_data = json.load(f)

results_map: Dict[str, List[Dict[str, Any]]] = {}

for qa in tqdm(eval_data, desc="è©•ä¼°é€²åº¦"):
    qid = qa.get("question_id")
    q = qa.get("question")
    if not qid or not q:
        continue

    try:
        retrieved_nodes = retriever.retrieve(q)
        bundle: List[Dict[str, Any]] = []
        for node in retrieved_nodes:
            did = node.metadata.get("doc_id")
            if did:
                bundle.append({
                    "page_content": node.get_content(),
                    "metadata": {"doc_id": did},
                })
        results_map[qid] = bundle
    except Exception as e:
        print(f"âš ï¸ å•é¡Œ {qid} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        results_map[qid] = []

# =========================
# è¼¸å‡ºçµæœ
# =========================
with open(OUTPUT_FILE_PATH, "w", encoding="utf-8") as f:
    json.dump(results_map, f, ensure_ascii=False, indent=4)

print(f"\nğŸ‰ å®Œæˆï¼æª¢ç´¢çµæœå·²è¼¸å‡ºè‡³ {OUTPUT_FILE_PATH}")
print("ğŸ‘‰ ç¾åœ¨ä½ å¯ä»¥åŸ·è¡Œ `python evaluate.py` åŠ å…¥é€™å€‹ç³»çµ±åšå…¬å¹³æ¯”è¼ƒã€‚")
