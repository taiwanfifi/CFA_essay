"""
RAG Agent Pragmatist - åŸºäº LangGraph çš„å¤šè½®æ£€ç´¢ Agent
é€‚é… thelma2 æ•°æ®æ ¼å¼
"""
import os
import json
import itertools
from typing import List, Dict, Any, Set, TypedDict

from dotenv import load_dotenv
load_dotenv()

from tqdm import tqdm

# LangChain / LangGraph
from langchain_core.documents import Document
from langchain_milvus import Milvus
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END

from data_loader import load_thelma2_dataset

print("--- RAG Agent Pragmatist (LangGraph Multi-turn Retrieval) ---")

# =========================
# é…ç½®
# =========================
OUTPUT_FILE = "./rag_agent_pragmatist_results.json"
COLLECTION_NAME = "rag_agent_pragmatist_collection"
DB_URI = "./rag_agent_pragmatist_milvus.db"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")

LLM_MODEL = os.getenv("OPENAI_LLM", "gpt-4o-mini")
EMBED_MODEL = os.getenv("OPENAI_EMBEDDING", "text-embedding-3-large")

RETRIEVER_K = 8
AGENT_MAX_TURNS = 8
TOP_K_RETURN = 5

print(f"LLM: {LLM_MODEL}")
print(f"Embedding: {EMBED_MODEL}")
print(f"Milvus DB: {DB_URI} / {COLLECTION_NAME}\n")


# =========================
# Agent çŠ¶æ€å’ŒèŠ‚ç‚¹
# =========================
class GraphState(TypedDict):
    original_question: str
    question: str
    question_history: List[str]
    rewritten_queries: List[str]
    hypotheses: List[str]
    current_hypothesis: str
    assessment: str
    cumulative_facts: List[str]
    final_summary: str
    newly_retrieved_docs: List[Document]
    cumulative_retrieved_docs: List[Document]
    all_retrieved_doc_ids: Set[str]
    has_new_info: bool
    turn_count: int
    plan: List[str]


class SimplePlan(BaseModel):
    plan: List[str] = Field(description="2-3 concrete sub-steps.")


class SimpleHypotheses(BaseModel):
    hypotheses: List[str] = Field(description="2-3 distinct hypotheses.")


class ExtractedFacts(BaseModel):
    facts: List[str] = Field(description="key facts")


class RewrittenQueries(BaseModel):
    rewritten_queries: List[str] = Field(description="1-3 search queries")


# LLMs
llm_json = ChatOpenAI(model=LLM_MODEL, temperature=0, api_key=OPENAI_API_KEY)
llm_text = ChatOpenAI(model=LLM_MODEL, temperature=0, api_key=OPENAI_API_KEY)

RETRIEVER = None  # type: ignore


def hypothesize_node(state: GraphState):
    parser = JsonOutputParser(pydantic_object=SimpleHypotheses)
    prompt = PromptTemplate.from_template(
        "åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆ 2-3 ä¸ªä¸åŒçš„å‡è®¾ã€‚\n"
        "é—®é¢˜: {original_question}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"original_question": state["original_question"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = SimpleHypotheses.model_validate(data)
        state["hypotheses"] = out.hypotheses
        state["current_hypothesis"] = out.hypotheses[0]
    except Exception:
        state["hypotheses"] = [state["original_question"]]
        state["current_hypothesis"] = state["original_question"]
    return state


def plan_node(state: GraphState):
    parser = JsonOutputParser(pydantic_object=SimplePlan)
    prompt = PromptTemplate.from_template(
        "åˆ›å»º 2-3 ä¸ªå…·ä½“çš„å­æ­¥éª¤æ¥å›ç­”é—®é¢˜ã€‚\n"
        "å‡è®¾: {current_hypothesis}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"current_hypothesis": state["current_hypothesis"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = SimplePlan.model_validate(data)
        state["plan"] = out.plan
    except Exception:
        state["plan"] = [state["current_hypothesis"]]
    return state


def execute_plan_node(state: GraphState):
    if not state["plan"]:
        state["question"] = ""
        return state
    nxt = state["plan"].pop(0)
    state["question"] = nxt
    return state


def rewrite_query_node(state: GraphState):
    if not state["question"]:
        state["rewritten_queries"] = []
        return state
    parser = JsonOutputParser(pydantic_object=RewrittenQueries)
    prompt = PromptTemplate.from_template(
        "å°†ä»¥ä¸‹å†…å®¹æ”¹å†™ä¸º 1-3 ä¸ªä¸åŒçš„æ£€ç´¢æŸ¥è¯¢ã€‚\n"
        "åŸå§‹: {q}\n"
        "{format_instructions}"
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"q": state["question"]})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = RewrittenQueries.model_validate(data)
        qs = [s for s in out.rewritten_queries if isinstance(s, str) and s.strip()]
        state["rewritten_queries"] = qs or [state["question"]]
    except Exception:
        state["rewritten_queries"] = [state["question"]]
    return state


def retrieve_node(state: GraphState):
    state["turn_count"] += 1
    queries = state.get("rewritten_queries", [])
    if not queries:
        state["newly_retrieved_docs"] = []
        state["has_new_info"] = False
        return state

    batch_lists = RETRIEVER.batch(queries)
    merged = list(itertools.chain.from_iterable(batch_lists))

    seen = state["all_retrieved_doc_ids"]
    truly_new = []
    for d in merged:
        did = d.metadata.get("doc_id")
        if did and did not in seen:
            truly_new.append(d)
            seen.add(did)

    state["newly_retrieved_docs"] = truly_new
    state["cumulative_retrieved_docs"].extend(truly_new)
    state["has_new_info"] = len(truly_new) > 0
    return state


def extract_facts_node(state: GraphState):
    if not state["newly_retrieved_docs"]:
        return state
    parser = JsonOutputParser(pydantic_object=ExtractedFacts)
    prompt = PromptTemplate.from_template(
        "ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–æœ‰åŠ©äºå›ç­”åŸå§‹é—®é¢˜çš„å…³é”®äº‹å®ã€‚\n"
        "åŸå§‹é—®é¢˜: {oq}\n"
        "å­é—®é¢˜: {sq}\n"
        "æ–‡æ¡£:\n{docs}\n"
        "{format_instructions}"
    )
    docs_txt = "\n\n".join(
        f"[doc_id={d.metadata.get('doc_id')}] {d.page_content}" for d in state["newly_retrieved_docs"]
    )
    chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm_json
    try:
        raw = chain.invoke({"oq": state["original_question"], "sq": state["question"], "docs": docs_txt})
        data = raw if isinstance(raw, dict) else json.loads(raw.content)
        out = ExtractedFacts.model_validate(data)
        if out.facts:
            state["cumulative_facts"].extend(out.facts)
            state["cumulative_facts"] = list(dict.fromkeys(state["cumulative_facts"]))
    except Exception:
        pass
    return state


def grade_facts_node(state: GraphState):
    distinct_doc_ids = {d.metadata.get("doc_id") for d in state.get("cumulative_retrieved_docs", []) if d.metadata.get("doc_id")}
    has_two_docs = len(distinct_doc_ids) >= 2
    min_turns_done = state.get("turn_count", 0) >= 2
    stop_now = (has_two_docs and min_turns_done) or (state.get("turn_count", 0) >= AGENT_MAX_TURNS) or (not state.get("has_new_info", True))
    state["assessment"] = "yes" if stop_now else "no"
    return state


def decide_to_continue(state: GraphState):
    return "generate_summary" if state.get("assessment") == "yes" else "continue"


def generate_summary_node(state: GraphState):
    if not state["cumulative_facts"]:
        state["final_summary"] = "æœªæ‰¾åˆ°è¶³å¤Ÿçš„äº‹å®ã€‚"
        return state
    prompt = PromptTemplate.from_template(
        "ä½¿ç”¨ä»¥ä¸‹äº‹å®æ’°å†™ç®€æ´çš„ç­”æ¡ˆã€‚\n"
        "é—®é¢˜: {q}\näº‹å®:\n- {facts}\n\nç­”æ¡ˆ:"
    )
    chain = prompt | llm_text | StrOutputParser()
    facts_str = "\n- ".join(state["cumulative_facts"])
    state["final_summary"] = chain.invoke({"q": state["original_question"], "facts": facts_str})
    return state


# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(GraphState)
workflow.add_node("hypothesize", hypothesize_node)
workflow.add_node("plan", plan_node)
workflow.add_node("execute_plan", execute_plan_node)
workflow.add_node("rewrite_query", rewrite_query_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("extract_facts", extract_facts_node)
workflow.add_node("grade_facts", grade_facts_node)
workflow.add_node("generate_summary", generate_summary_node)

workflow.set_entry_point("hypothesize")
workflow.add_edge("hypothesize", "plan")
workflow.add_edge("plan", "execute_plan")
workflow.add_edge("execute_plan", "rewrite_query")
workflow.add_edge("rewrite_query", "retrieve")
workflow.add_edge("retrieve", "extract_facts")
workflow.add_edge("extract_facts", "grade_facts")
workflow.add_conditional_edges("grade_facts", decide_to_continue, {
    "generate_summary": "generate_summary",
    "continue": "plan"
})
workflow.add_edge("generate_summary", END)
agent = workflow.compile()


# =========================
# æ‰§è¡Œå‡½æ•°
# =========================
def run_agent_once(question: str) -> Dict[str, Any]:
    """è¿è¡Œä¸€æ¬¡ Agentï¼Œè¿”å›æœ€ç»ˆçŠ¶æ€å’Œ top-k æ£€ç´¢æ–‡æ¡£"""
    init = {
        "original_question": question,
        "question": "",
        "question_history": [],
        "rewritten_queries": [],
        "hypotheses": [],
        "current_hypothesis": "",
        "assessment": "",
        "cumulative_facts": [],
        "final_summary": "",
        "newly_retrieved_docs": [],
        "cumulative_retrieved_docs": [],
        "all_retrieved_doc_ids": set(),
        "has_new_info": True,
        "turn_count": 0,
        "plan": [],
    }
    final_state = agent.invoke(init, {"recursion_limit": 50})
    
    # æŒ‰é¦–æ¬¡æ£€ç´¢é¡ºåºå–å‰ K
    uniq: Dict[str, Document] = {}
    for d in final_state.get("cumulative_retrieved_docs", []):
        did = d.metadata.get("doc_id")
        if did and did not in uniq:
            uniq[did] = d
    top_docs = list(uniq.values())[:TOP_K_RETURN]
    
    out_bundle = [
        {"page_content": d.page_content, "metadata": {"doc_id": d.metadata.get("doc_id")}}
        for d in top_docs
    ]
    return {"state": final_state, "docs": out_bundle}


def main():
    # 1) åŠ è½½æ•°æ®
    questions, docs = load_thelma2_dataset()
    
    # 2) å»ºç«‹å‘é‡åº“
    print("ğŸ”§ å»ºç«‹ Milvus å‘é‡åº“...")
    embeddings = OpenAIEmbeddings(model=EMBED_MODEL, api_key=OPENAI_API_KEY)
    vectorstore = Milvus(
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
        connection_args={"uri": DB_URI},
        drop_old=True,
        auto_id=True,
    )
    vectorstore.add_documents(docs)
    print("âœ… å‘é‡åº“å»ºç«‹å®Œæˆ\n")
    
    # 3) è®¾ç½®æ£€ç´¢å™¨
    global RETRIEVER
    RETRIEVER = vectorstore.as_retriever(search_kwargs={"k": RETRIEVER_K})
    
    # 4) æ‰§è¡Œæ£€ç´¢
    print("--- å¼€å§‹æ‰§è¡Œ Agent æ£€ç´¢ ---")
    results_map: Dict[str, List[Dict[str, Any]]] = {}
    
    for item in tqdm(questions, desc="å¤„ç†é—®é¢˜"):
        qid = item.get("question_id")
        q = item.get("question")
        if not qid or not q:
            continue
        try:
            r = run_agent_once(q)
            results_map[qid] = r["docs"]
        except Exception as e:
            results_map[qid] = []
            print(f"\nâš ï¸ é—®é¢˜ {qid} å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    # 5) è¾“å‡ºç»“æœ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results_map, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ å®Œæˆï¼æ£€ç´¢ç»“æœå·²è¾“å‡ºåˆ° {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

