"""
RAG LlamaIndex Vector Only - çº¯å‘é‡æ£€ç´¢ç‰ˆæœ¬
é€‚é… thelma2 æ•°æ®æ ¼å¼
"""
import os
import json
from tqdm import tqdm
from typing import List, Dict, Any

from dotenv import load_dotenv
load_dotenv()

from llama_index.core import (
    Document,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

from data_loader import load_thelma2_dataset

print("--- ğŸš€ RAG LlamaIndex Vector Only (Pure Vector Retrieval) ---")

# =========================
# é…ç½®
# =========================
OUTPUT_FILE = "./rag_llama_index_vector_results.json"
KB_PERSIST_DIR = "./rag_llama_index_vector_storage"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")

# è®¾ç½® LlamaIndexï¼ˆLLM ä»…ä½œä¸ºå ä½ç¬¦ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰
Settings.llm = OpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-large", api_key=OPENAI_API_KEY
)

print("âœ… LlamaIndex è®¾ç½®å®Œæˆï¼šVector-only mode")
print("   - Embedding: text-embedding-3-large")
print("   - LLM: gpt-4o-mini (not used in query pipeline)\n")


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    # 1) åŠ è½½æ•°æ®
    questions, docs = load_thelma2_dataset()
    
    # 2) è½¬æ¢ä¸º LlamaIndex Document æ ¼å¼
    llama_docs = [
        Document(text=doc.page_content, metadata=doc.metadata)
        for doc in docs
    ]
    
    # 3) å»ºç«‹æˆ–åŠ è½½ç´¢å¼•
    if not os.path.exists(KB_PERSIST_DIR):
        print("ğŸ“‚ å»ºç«‹æ–°ç´¢å¼•...")
        print(f"   - å…± {len(llama_docs)} ä»½æ–‡ä»¶")
        index = VectorStoreIndex.from_documents(llama_docs, show_progress=True)
        index.storage_context.persist(persist_dir=KB_PERSIST_DIR)
        print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆ\n")
    else:
        print("âœ… ä»æ—¢æœ‰ç´¢å¼•è½½å…¥")
        storage_context = StorageContext.from_defaults(persist_dir=KB_PERSIST_DIR)
        index = load_index_from_storage(storage_context)
        print()
    
    # 4) å»ºç«‹çº¯å‘é‡æ£€ç´¢å™¨
    retriever = index.as_retriever(similarity_top_k=5)
    
    # 5) æ‰§è¡Œæ£€ç´¢
    print("--- å¼€å§‹å¤„ç†é—®é¢˜ ---")
    results_map: Dict[str, List[Dict[str, Any]]] = {}
    
    for qa in tqdm(questions, desc="å¤„ç†è¿›åº¦"):
        qid = qa.get("question_id")
        q = qa.get("question")
        if not qid or not q:
            continue
        
        try:
            retrieved_nodes = retriever.retrieve(q)
            bundle: List[Dict[str, Any]] = []
            for node in retrieved_nodes:
                did = node.metadata.get("doc_id")
                if did:
                    bundle.append({
                        "page_content": node.get_content(),
                        "metadata": {"doc_id": did},
                    })
            results_map[qid] = bundle
        except Exception as e:
            print(f"âš ï¸ é—®é¢˜ {qid} å‘ç”Ÿé”™è¯¯ï¼š{e}")
            results_map[qid] = []
    
    # 6) è¾“å‡ºç»“æœ
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results_map, f, ensure_ascii=False, indent=4)
    
    print(f"\nğŸ‰ å®Œæˆï¼æ£€ç´¢ç»“æœå·²è¾“å‡ºè‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

