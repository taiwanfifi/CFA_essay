"""
æ•°æ®åŠ è½½å·¥å…· - é€‚é… thelma2 æ•°æ®æ ¼å¼
æ”¯æŒä»Ž qa_dataset.json åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸º RAG ç³»ç»Ÿå¯ç”¨çš„æ ¼å¼
"""
import json
import os
from typing import List, Dict, Any
from langchain_core.documents import Document


def load_thelma2_dataset(filepath: str = None) -> tuple[List[Dict[str, Any]], List[Document]]:
    """
    åŠ è½½ thelma2 æ ¼å¼çš„æ•°æ®é›†
    
    Args:
        filepath: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æžœä¸º None åˆ™è‡ªåŠ¨æŸ¥æ‰¾
    
    Returns:
        questions: List[Dict] - é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« id, query, source_text
        documents: List[Document] - çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« page_content å’Œ metadata
    """
    if filepath is None:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            "../thelma2/qa_dataset.json",
            "./thelma2/qa_dataset.json",
            "thelma2/qa_dataset.json",
            "../qa_dataset.json",
            "./qa_dataset.json",
        ]
        for path in possible_paths:
            if os.path.exists(path):
                filepath = path
                break
        if filepath is None:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œå·²å°è¯•ä»¥ä¸‹è·¯å¾„: {possible_paths}")
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {filepath}")
    
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # æå–é—®é¢˜å’ŒçŸ¥è¯†åº“
    questions = []
    unique_docs: Dict[str, str] = {}  # doc_id -> text
    
    for item in data:
        qid = item.get("id", "")
        query = item.get("query", "")
        source_text = item.get("source_text", "") or item.get("æ¨™æº–è§£ç­”", "")
        
        if query:
            questions.append({
                "question_id": str(qid),
                "question": query,
                "source_text": source_text
            })
        
        # å°† source_text ä½œä¸ºçŸ¥è¯†åº“æ–‡æ¡£ï¼ˆä½¿ç”¨ id ä½œä¸º doc_idï¼‰
        if source_text and qid:
            doc_id = f"doc_{qid}"
            # å¦‚æžœåŒä¸€ä¸ª doc_id å·²å­˜åœ¨ï¼Œåˆå¹¶æ–‡æœ¬ï¼ˆç”¨æ¢è¡Œåˆ†éš”ï¼‰
            if doc_id in unique_docs:
                unique_docs[doc_id] += f"\n\n{source_text}"
            else:
                unique_docs[doc_id] = source_text
    
    # è½¬æ¢ä¸º Document å¯¹è±¡
    documents = [
        Document(
            page_content=text,
            metadata={"doc_id": doc_id}
        )
        for doc_id, text in unique_docs.items()
    ]
    
    print(f"ðŸ“Š åŠ è½½æ•°æ®: {len(questions)} ä¸ªé—®é¢˜, {len(documents)} ä¸ªæ–‡æ¡£")
    return questions, documents


def load_questions_only(filepath: str = None) -> List[Dict[str, Any]]:
    """ä»…åŠ è½½é—®é¢˜åˆ—è¡¨"""
    questions, _ = load_thelma2_dataset(filepath)
    return questions

