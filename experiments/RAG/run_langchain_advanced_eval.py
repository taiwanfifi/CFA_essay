# run_langchain_advanced_eval.py
# LangChain å¼·åŒ–ç‰ˆï¼šæ¨¡ä»¿ LlamaIndex pipeline (rewrite + subquery + hybrid + rerank)

import os
import json
from tqdm import tqdm
from typing import List, Dict, Any

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import numpy as np

print("--- ğŸš€ LangChain å¼·åŒ–ç‰ˆ è©•ä¼°å™¨ ---")

# =========================
# åŸºæœ¬è¨­å®š
# =========================
DATA_FILE_PATH = "./data/ultimate_rag_challenge_questions.json"
OUTPUT_FILE_PATH = "./langchain_advanced_results.json"
DB_PATH = "./langchain_eval_milvus.db"
COLLECTION_NAME = "langchain_advanced_eval_v1"

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("ğŸ›‘ è«‹å…ˆè¨­å®š OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=OPENAI_API_KEY)

# =========================
# KB æº–å‚™
# =========================
with open(DATA_FILE_PATH, "r", encoding="utf-8") as f:
    eval_data = json.load(f)

unique_evidence: Dict[str, str] = {
    ev["doc_id"]: ev["text_snippet"]
    for item in eval_data
    for ev in item.get("gold_evidence", [])
    if ev.get("doc_id") and isinstance(ev.get("text_snippet"), str)
}

docs = [Document(page_content=txt, metadata={"doc_id": did}) for did, txt in unique_evidence.items()]
print(f"ğŸ“„ Evidence docs: {len(docs)}")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

bm25_retriever = BM25Retriever.from_documents(chunks)
# bm25_retriever.k = 10
bm25_retriever.k = 20  # â† 10 -> 20


vectorstore = Milvus(
    embedding_function=embeddings,
    connection_args={"uri": DB_PATH},
    collection_name=COLLECTION_NAME,
    drop_old=True,  # always rebuild fresh
    auto_id=True,
)
vectorstore.add_documents(chunks)
# milvus_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
milvus_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})  # â† 10 -> 20

# =========================
# å·¥å…·ï¼šQuery Rewrite
# =========================
rewrite_prompt = ChatPromptTemplate.from_template(
    "Rewrite the following question into a clearer search-friendly query:\n{question}"
)
query_rewriter = (
    {"question": RunnablePassthrough()}
    | rewrite_prompt
    | llm
    | StrOutputParser()
)

# =========================
# å·¥å…·ï¼šSub-query decomposition
# =========================
subq_prompt = PromptTemplate.from_template(
    "Decompose the following question into 1-3 sub-questions:\n\n{question}"
)
subq_chain = (
    {"question": RunnablePassthrough()}
    | subq_prompt
    | llm
    | StrOutputParser()
)

def subquery_retrieve(question: str) -> List[Document]:
    sub_queries_str = subq_chain.invoke({"question": question})
    sub_queries = [q.strip() for q in sub_queries_str.split("\n") if q.strip()]
    if not sub_queries:
        return milvus_retriever.invoke(question)

    all_docs = []
    for sq in sub_queries:
        all_docs.extend(milvus_retriever.invoke(sq))
    # unique by doc_id
    uniq = {doc.metadata.get("doc_id"): doc for doc in all_docs}
    return list(uniq.values())


# è¿½åŠ ï¼šPydantic v2 ConfigDict
from pydantic import ConfigDict
from typing import Sequence

# =========================
# å·¥å…·ï¼šReranker (cosine on embeddings)  â€” ä¿®æ­£ç‰ˆ
# =========================
class OpenAIReranker(BaseDocumentCompressor):
    # å®£å‘Šç‚º Pydantic æ¬„ä½ï¼ˆä¸è¦è¦†å¯« __init__ï¼‰
    embed: OpenAIEmbeddings
    top_n: int = 5

    # å…è¨±ä»»æ„å‹åˆ¥ï¼ˆOpenAIEmbeddings ä¸æ˜¯ pydantic modelï¼‰
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks=None
    ) -> Sequence[Document]:
        if not documents:
            return []

        # å‘é‡åŒ–
        doc_texts = [doc.page_content for doc in documents]
        doc_vecs = self.embed.embed_documents(doc_texts)
        q_vec = self.embed.embed_query(query)

        q = np.asarray(q_vec, dtype=np.float32)
        q_norm = np.linalg.norm(q) or 1.0

        scored = []
        for v, doc in zip(doc_vecs, documents):
            dv = np.asarray(v, dtype=np.float32)
            denom = (np.linalg.norm(dv) * q_norm) or 1.0
            score = float(np.dot(dv, q) / denom)
            # è¨˜åˆ° metadata æ–¹ä¾¿å¾ŒçºŒæª¢è¦–
            md = dict(doc.metadata or {})
            md["rerank_score"] = score
            doc.metadata = md
            scored.append((score, doc))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored[: self.top_n]]


# reranker = OpenAIReranker(embed=embeddings, top_n=5)
reranker = OpenAIReranker(embed=embeddings, top_n=8)

# =========================
# çµ„åˆï¼šHybrid Ensemble + Rewrite + Subquery + Rerank
# =========================
ensemble = EnsembleRetriever(
    retrievers=[bm25_retriever, milvus_retriever],
    weights=[0.4, 0.6]  # â† åŸ 0.5/0.5 æ”¹æˆåå‘å‘é‡
    # weights=[0.5, 0.5]
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=ensemble
)


def advanced_retrieve(question: str) -> List[Document]:
    # Step 1: rewrite
    rewritten = query_rewriter.invoke({"question": question}).strip()

    # Step 2: sub-queriesï¼ˆæœ€å¤š 2 æ¢ï¼‰
    sub_queries_str = subq_chain.invoke({"question": rewritten})
    sub_queries = [q.strip() for q in sub_queries_str.split("\n") if q.strip()]
    sub_queries = sub_queries[:2] or [rewritten]

    # Step 3: hybrid æª¢ç´¢ï¼ˆä¸€æ¬¡ batchï¼‰
    lists = ensemble.batch(sub_queries)       # List[List[Document]]
    pool = [doc for lst in lists for doc in lst]

    # å»é‡ï¼ˆä»¥ doc_idï¼‰
    uniq = {}
    for d in pool:
        did = d.metadata.get("doc_id")
        if did and did not in uniq:
            uniq[did] = d
    pooled_docs = list(uniq.values())

    # Step 4: rerank on rewritten
    final_docs = reranker.compress_documents(pooled_docs, rewritten)  # top_n=8 å·²åœ¨å¯¦ä¾‹ä¸Šæ§åˆ¶
    return final_docs


# =========================
# ä¸»æµç¨‹ï¼šè·‘é¡Œç›®
# =========================
print("\n--- é–‹å§‹è™•ç†è©•ä¼°å•é¡Œ ---")
results_map: Dict[str, List[Dict[str, Any]]] = {}

for qa in tqdm(eval_data, desc="è©•ä¼°é€²åº¦"):
    qid = qa.get("question_id")
    q = qa.get("question")
    if not qid or not q:
        continue

    try:
        retrieved_docs = advanced_retrieve(q)
        bundle = []
        for doc in retrieved_docs:
            bundle.append({
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            })
        results_map[qid] = bundle
    except Exception as e:
        print(f"âš ï¸ å•é¡Œ {qid} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        results_map[qid] = []

with open(OUTPUT_FILE_PATH, "w", encoding="utf-8") as f:
    json.dump(results_map, f, ensure_ascii=False, indent=4)

print(f"\nğŸ‰ å®Œæˆï¼æª¢ç´¢çµæœå·²è¼¸å‡ºè‡³ {OUTPUT_FILE_PATH}")
print("ğŸ‘‰ ç¾åœ¨ä½ å¯ä»¥åŸ·è¡Œ `python evaluate.py` åŠ å…¥é€™å€‹ç³»çµ±æ¯”è¼ƒã€‚")
