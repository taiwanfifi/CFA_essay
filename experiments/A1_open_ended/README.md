# A1: Open-Ended CFA Benchmark

## Research Question

CFA exams include essay/constructed-response sections, but LLM evaluation almost exclusively uses MCQ. We build an **open-ended benchmark** from existing MCQ questions by stripping choices and evaluating responses with a **three-level grading system**.

---

## Three-Level Evaluation

| Level | Name | Criteria | Example |
|-------|------|----------|---------|
| A | Exact | Within ±2% numerical tolerance or exact semantic match | Gold: £400,000 → Student: £400,000 ✓ |
| B | Directional | Correct approach, same magnitude, different assumptions | Gold: £400,000 → Student: £396,000 (continuous compounding) |
| C | Incorrect | Wrong answer or fundamentally flawed reasoning | Gold: £400,000 → Student: £40,000 (order of magnitude error) |

---

## Worked Example

### Original MCQ

```
Q: A sweepstakes winner may select either a perpetuity of £2,000 a month
beginning with the first payment in one month or an immediate lump sum
payment of £350,000. If the annual discount rate is 6% compounded monthly,
the present value of the perpetuity is:
CHOICES: A: less than the lump sum. B: equal to the lump sum.
C: greater than the lump sum.
Answer: C
```

### Open-Ended Version (choices stripped)

```
A sweepstakes winner may select either a perpetuity of £2,000 a month
beginning with the first payment in one month or an immediate lump sum
payment of £350,000. If the annual discount rate is 6% compounded monthly,
what is the present value of the perpetuity?
```

### Gold Answer (generated by judge)

```json
{
    "numerical_answer": 400000,
    "unit": "£",
    "concept": "PV of perpetuity = PMT / r_monthly = 2000 / 0.005",
    "solution_summary": "Monthly rate = 6%/12 = 0.5%. PV = 2000/0.005 = £400,000"
}
```

### Three Model Responses

| Model Response | Level | Reasoning |
|-------|-------|-----------|
| "PV = 2000/0.005 = **£400,000**" | A (Exact) | Within tolerance |
| "Using continuous compounding: PV = **£396,026**" | B (Directional) | Same magnitude, defensible method |
| "PV = 2000/0.06 = **£33,333**" | C (Incorrect) | Used annual rate instead of monthly |

### Error Attribution (Level C only)

```json
{
    "error_category": "assumption_error",
    "explanation": "Used annual rate (6%) instead of monthly rate (0.5%) in perpetuity formula",
    "could_be_fixed_with_tools": true
}
```

---

## Evaluation Pipeline

```
Question (MCQ)
    → Strip choices → Open-ended question
    → Generate gold answer (GPT-4o-mini from MCQ answer key)
    → Get model response (subject model)
    → Three-level evaluation:
        1. Try numerical matching (auto-grade)
        2. Fall back to LLM judge (semantic)
    → If Level C: error attribution
```

---

## Running

```bash
# POC: 5 questions
python -m experiments.A1_open_ended.run_experiment \
    --dataset easy --limit 5 --model gpt-4o-mini

# Full dataset
python -m experiments.A1_open_ended.run_experiment \
    --dataset easy --model gpt-4o-mini

# Analysis
python -m experiments.A1_open_ended.analysis \
    --input experiments/A1_open_ended/results/run_*/results.json
```

---

## Output Format

```json
{
  "metadata": {"model": "gpt-4o-mini", "dataset": "easy", "n_questions": 5},
  "summary": {
    "level_distribution": {"exact": 2, "directional": 1, "incorrect": 2},
    "level_rates": {"exact": 0.4, "directional": 0.2, "incorrect": 0.4},
    "strict_accuracy": 0.4,
    "lenient_accuracy": 0.6,
    "error_categories": {"formula_error": 1, "assumption_error": 1}
  },
  "results": [
    {
      "question_id": "easy_0",
      "level": "exact",
      "gold_answer": {"numerical": 400000, "text": "greater than the lump sum"},
      "evaluation": {"reasoning": "...", "auto_graded": true}
    }
  ]
}
```

---

## Dependencies

- `experiments/shared/` (config, llm_client, prompts, evaluation, data_loader)
- OpenAI API key (`OPENAI_API_KEY` env var)
